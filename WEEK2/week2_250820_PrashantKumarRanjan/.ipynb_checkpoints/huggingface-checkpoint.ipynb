{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PLYeJw9p_31D",
        "outputId": "04d0aec2-4f34-436d-a086-8cb91baf1791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets sentencepiece torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "gWPO-QAv_9Hj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"t5-small\""
      ],
      "metadata": {
        "id": "hdTQF8sHADHF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VtEO8RhOAIG5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"Judge It Well â€” Fine-Tuning LLMs for Legal Intelligence\n",
        "Transforming general AI into specialized legal expertise.\n",
        "\n",
        "Abstract\n",
        "Judge It Well is a project aimed at transforming a general-purpose large language model (LLM) into a specialized legal expert through structured fine-tuning. The goal is to build an AI system capable of interpreting, analyzing, and reasoning through complex legal documents with high accuracy and efficiency.\n",
        "\n",
        "Introduction\n",
        "Artificial Intelligence has advanced rapidly, yet legal text remains one of the most challenging domains for AI. Legal language is:\n",
        "\n",
        "Highly technical\n",
        "Context-dependent\n",
        "Structured but linguistically unique\n",
        "General LLMs cannot fully understand or reason through legal documents without domain-specific training. Judge It Well bridges this gap by fine-tuning LLMs on legal data, enabling them to process statutes, contracts, judgments, and legal queries more effectively.\n",
        "\n",
        "Project Overview\n",
        "Imagine taking a general AI and training it to speak the language of law. This project focuses on:\n",
        "\n",
        "Fine-tuning an LLM for legal interpretation\n",
        "Teaching the model to handle legal reasoning\n",
        "Building tools for preprocessing, training, evaluation, and deployment\n",
        "Objectives\n",
        "Develop a fine-tuned LLM specialized for legal tasks\n",
        "Prepare and preprocess diverse legal datasets\n",
        "Support tasks like summarization, Q&A, clause extraction, classification\n",
        "Evaluate model performance with legal benchmarks\n",
        "Why Legal AI?\n",
        "Legal documents are:\n",
        "\n",
        "Dense and complex\n",
        "Time-consuming to analyze manually\n",
        "Filled with technical language and long reasoning chains\n",
        "A specialized model can assist:\n",
        "\n",
        "Lawyers\n",
        "Students\n",
        "Researchers\n",
        "Legal-tech applications\n",
        "By automating repetitive tasks and improving decision-making efficiency.\n",
        "\n",
        "Methodology\n",
        "1. Dataset Preparation\n",
        "The process includes:\n",
        "\n",
        "Collecting corpora (judgments, statutes, case summaries, contracts)\n",
        "Cleaning and preprocessing text\n",
        "Chunking into model-friendly segments\n",
        "Tokenization and vocabulary optimization\n",
        "2. Model Fine-Tuning\n",
        "Techniques used:\n",
        "\n",
        "LoRA / QLoRA parameter-efficient fine-tuning\n",
        "Supervised Fine-Tuning (SFT)\n",
        "Instruction tuning for legal reasoning tasks\n",
        "3. Evaluation\n",
        "The model is evaluated on:\n",
        "\n",
        "Legal reasoning tasks\n",
        "Summarization quality\n",
        "Text classification\n",
        "Domain-specific metrics\n",
        "Applications\n",
        "A fine-tuned legal LLM can be used for:\n",
        "\n",
        "AI legal assistants\n",
        "Automated contract review\n",
        "Case law summarization\n",
        "Compliance analysis\n",
        "Research support for students\n",
        "Structured Q&A on legal documents\n",
        "Conclusion\n",
        "Judge It Well is a foundational step toward building AI that truly understands legal text. By fine-tuning general-purpose LLMs on legal documents, we create tools that are more:\n",
        "\n",
        "Accurate\n",
        "Efficient\n",
        "Context-aware\n",
        "Explainable\n",
        "This project moves us closer to deploying specialized AI\"\"\"\n",
        "\n",
        "prompt = \"summarize the text: \" + text"
      ],
      "metadata": {
        "id": "yMU7MGzJALet"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WzO60IOUA8kZ",
        "outputId": "32148583-9aa8-4c18-9a15-7e86bf72d2a3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[21603,     8,  1499,    10, 12330,    94,  1548,     3,   318, 11456,\n",
              "            18,   382,   202,    53,   301, 11160,     7,    21, 11281,  5869,\n",
              "          2825,  1433,  4946, 10454,   879,  7833,   139,     3,  8689,  1281,\n",
              "          2980,     5, 20114, 12330,    94,  1548,    19,     3,     9,   516,\n",
              "             3,  8287,    44,     3, 21139,     3,     9,   879,    18, 19681,\n",
              "           508,  1612,   825,    41, 10376,   329,    61,   139,     3,     9,\n",
              "             3,  8689,  1281,  2205,   190, 14039,  1399,    18,    17,   202,\n",
              "            53,     5,    37,  1288,    19,    12,   918,    46,  7833,   358,\n",
              "          3919,    13,     3, 29490,     6,     3, 19175,     6,    11, 20893,\n",
              "           190,  1561,  1281,  2691,    28,   306,  7452,    11,  3949,     5,\n",
              "         18921, 24714,  5869,  2825,  1433,    65,  2496,  7313,     6,   780,\n",
              "          1281,  1499,  3048,    80,    13,     8,   167,  4421,  3303,     7,\n",
              "            21,  7833,     5, 11281,  1612,    19,    10, 20600,  2268,  1193,\n",
              "          6327,    18, 17631, 21627,    26,    68,     3, 24703,  1427,   775,\n",
              "          2146,   301, 11160,     7,  1178,  1540,   734,    42,  1053,   190,\n",
              "          1281,  2691,   406,  3303,    18,  9500,   761,     5, 12330,    94,\n",
              "          1548,  4716,     7,    48,  6813,    57,  1399,    18,    17,   202,\n",
              "            53,   301, 11160,     7,    30,  1281,   331,     6,     3, 10165,\n",
              "           135,    12,   433, 18692,     7,     6,  8201,     6,  7661,     7,\n",
              "             6,    11,  1281, 13154,    72,  3762,     5,  2786, 28479, 11648,\n",
              "           838,     3,     9,   879,  7833,    11,   761,    34,    12,  2516,\n",
              "             8,  1612,    13,   973,     5,   100,   516,     3,  6915,    30,\n",
              "            10, 11456,    18,    17,   202,    53,    46,   301, 11160,    21,\n",
              "          1281,  8868, 16648,     8,   825,    12,  2174,  1281, 20893,  5450,\n",
              "          1339,    21,   554, 15056,    53,     6,   761,     6,  5002,     6,\n",
              "            11, 12001, 27919,     7, 24305,     3,     9,  1399,    18,    17,\n",
              "           444,    26,   301, 11160,     3,  8689,    21,  1281,  4145, 16386,\n",
              "            11,   554, 15056,  2399,  1281, 17953,     7,  4224,  4145,   114,\n",
              "          4505,  1635,  1707,     6,  1593,   184,   188,     6, 14442, 16629,\n",
              "             6, 13774, 17627, 22156,   825,   821,    28,  1281, 15705,     7,\n",
              "          1615, 11281,  7833,    58, 11281,  2691,    33,    10,  3128,     7,\n",
              "            15,    11,  1561,  2900,    18, 10862,    12,  8341, 12616,  6222,\n",
              "          1361,    28,  2268,  1612,    11,   307, 20893, 16534,    71,     3,\n",
              "          8689,   825,    54,  2094,    10, 18768,     7,  4375, 23066, 11281,\n",
              "            18,  3470,  1564,   938,  7791,    53, 24869,  4145,    11,  4863,\n",
              "          1357,    18,  5239,  3949,     5,  7717,  1863,  1300,  2747,  2244,\n",
              "          1266,  1893,   257,    37,   433,   963,    10,  3043,  3437,    53,\n",
              "         11736,   127,     9,    41, 14312,   122,  4128,     6, 18692,     7,\n",
              "             6,   495,  4505,    51,  5414,     6,  8201,    61, 11115,    11,\n",
              "           554, 15056,    53,  1499, 16636,    29,  1765,   139,   825,    18,\n",
              "          4905, 15107,   304,  2217,  1707,    11, 19067, 11295,  1682,  5154,\n",
              "         11456,    18,   382,   202,    53, 22222,     7,   261,    10,  1815,\n",
              "          4763,     3,    87,  1593,   434,    32,  4763, 15577,    18, 16995,\n",
              "          1399,    18,    17,   202,    53,  2011,   208,  3375, 11456,    18,\n",
              "           382,   202,    53,    41,   134,  6245,    61, 21035, 23668,    21,\n",
              "          1281, 20893,  4145,  1877, 22714,    37,   825,    19, 14434,    30,\n",
              "            10, 11281, 20893,  4145, 12198,  1635,  1707,   463,  5027, 13774,\n",
              "         13979,    18,  9500, 15905, 15148,    71,  1399,    18,    17,   444,\n",
              "            26,  1281,   301, 11160,    54,    36,   261,    21,    10,  7833,\n",
              "          1281,  6165,     7, 23672,    15,    26,  1696,  1132,  6320,   973,\n",
              "          4505,  1635,  1707, 24779,  1693,  2200,   380,    21,   481, 21627,\n",
              "            26,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_summary = model.generate(result[\"input_ids\"], max_length=120, min_length=40, length_penalty=2.0, num_beams=4)\n",
        "tokenized_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ldHESm-a18",
        "outputId": "049b6ed9-c5c1-4ff1-b5d8-f2f212456114"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0, 12330,    94,  1548,    19,     3,     9,   516,     3,  8287,\n",
              "            44,     3, 21139,     3,     9,   879,    18, 19681,   508,  1612,\n",
              "           825,   139,     3,     9,     3,  8689,  1281,  2205,   190, 14039,\n",
              "          1399,    18,    17,   202,    53,     3,     5,     8,  1288,    19,\n",
              "            12,   918,    46,  7833,   358,  3919,    13,     3, 29490,     6,\n",
              "             3, 19175,     6,    11, 20893,   190,  1561,  1281,  2691,    28,\n",
              "           306,  7452,    11,  3949,     3,     5,     1]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_summary.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rStg3SsfFTqf",
        "outputId": "f7123492-dcd2-4c90-e741-2105923b8ef3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 67])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = tokenizer.decode(tokenized_summary[0], skip_special_tokens=True)\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "QdE1flSH764i",
        "outputId": "afce95a3-cec2-47fd-f438-47bab46a1ae9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Judge It Well is a project aimed at transforming a general-purpose large language model into a specialized legal expert through structured fine-tuning. the goal is to build an AI system capable of interpreting, analyzing, and reasoning through complex legal documents with high accuracy and efficiency.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}