{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKWgHSdfCb38"
   },
   "source": [
    "# 1. DATA PREPROCESSING STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPmzi1L8zRUy"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3bb2f3a",
    "outputId": "489d0cf6-3e81-4887-f766-428085a0d667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed old 'Churn_Modelling.csv'.\n",
      "Successfully set up 'Churn_Modelling.csv' from '/content/Churn_Modelling_CSV.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Source file path provided by the user (the actual CSV file)\n",
    "source_file = '/content/Churn_Modelling_CSV.csv'\n",
    "\n",
    "# Destination file path expected by the existing code\n",
    "destination_file = 'Churn_Modelling.csv'\n",
    "\n",
    "# Check if the source CSV file exists\n",
    "if os.path.exists(source_file):\n",
    "    # If the destination file already exists from previous attempts, remove it\n",
    "    if os.path.exists(destination_file):\n",
    "        os.remove(destination_file)\n",
    "        print(f\"Removed old '{destination_file}'.\")\n",
    "\n",
    "    # Copy the correct CSV file to the expected filename\n",
    "    shutil.copyfile(source_file, destination_file)\n",
    "    print(f\"Successfully set up '{destination_file}' from '{source_file}'.\")\n",
    "else:\n",
    "    print(f\"Error: Source CSV file '{source_file}' not found. Please ensure the correct CSV file is uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MFdMXo-SCi8W"
   },
   "outputs": [],
   "source": [
    "#STEP 1 INSTALLING LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  LabelEncoder, OneHotEncoder,StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#step 2 load data\n",
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "#STEP 3: SEPARATE FEATURES (X) AND TARGET (y)\n",
    "#we exclude rownumber,customerid,surname(indices 0,1,2)\n",
    "#we take data from index 3 upto the last one as features\n",
    "X=data.iloc[:,3:-1].values\n",
    "#we take the last column as target\n",
    "y=data.iloc[:,-1].values\n",
    "\n",
    "# STEP 4: ENCODING CATEGORICAL DATA\n",
    "#label encoding the gender column\n",
    "#gender is at index 2 in our new x matrix (creditscore =0,geography=1,gender =2)\n",
    "le=LabelEncoder()\n",
    "X[:,2]= le.fit_transform(X[:,2])\n",
    "#now female/male are 0/1\n",
    "#one Hot Encoding the \"geography\" column\n",
    "#\"geography\"is at index 1.it has three categories :french,spain ,germany\n",
    "#we transform column1 into 3 separate binary columnc\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder='passthrough')\n",
    "X=np.array(ct.fit_transform(X))\n",
    "\n",
    "\n",
    "#STEP 5 :SPLITINTO TRAIN AND TEST SET\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "#step 6 :FEATURE SCALLING\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmHGMujEMYxY"
   },
   "source": [
    "#2 ARCHITECTURE OF THE BRAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Hqz57y6MiTZ",
    "outputId": "a27a755f-27ef-4555-ed44-ba58e57cf8a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChurnPredictor(\n",
      "  (layer1): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (layer2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#define the ANN ARCHITECTURE\n",
    "class ChurnPredictor(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ChurnPredictor ,self).__init__()\n",
    "\n",
    "    #hidden layer 1\n",
    "    #input :12 features (from our preprocessed data)\n",
    "    #output:8 neurons (arbitrary)\n",
    "    self.layer1=nn.Linear(in_features =12,out_features=8)\n",
    "    #hidden layer 2\n",
    "    #input =8,output=8\n",
    "    self.layer2=nn.Linear(in_features =8,out_features=8)\n",
    "    #output latyer\n",
    "    #input =8,output =1 as only probability is needed\n",
    "    self.output_layer=nn.Linear(in_features=8,out_features=1)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    # 1. pass data through layer 1 and relu function\n",
    "    x=F.relu(self.layer1(x))\n",
    "\n",
    "    #2. Pass data through layer 2 and relu function\n",
    "    x=F.relu(self.layer2(x))\n",
    "\n",
    "    #3. pass through output layer and apply sigmoid  activation\n",
    "    # sigmoid squashes the result between 0 and 1 (Probability)\n",
    "    x=torch.sigmoid(self.output_layer(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "# instantiate the model\n",
    "model = ChurnPredictor()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEz6vBWYUl7s"
   },
   "source": [
    "# 3.TRAINING THE  BRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMNHcoOtUuZ9",
    "outputId": "9dedec9c-7047-4516-d825-13cbb2395bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/9] ,Loss:0.6494\n",
      "Epoch [20/19] ,Loss:0.5494\n",
      "Epoch [30/29] ,Loss:0.4756\n",
      "Epoch [40/39] ,Loss:0.4392\n",
      "Epoch [50/49] ,Loss:0.4220\n",
      "Epoch [60/59] ,Loss:0.4025\n",
      "Epoch [70/69] ,Loss:0.3838\n",
      "Epoch [80/79] ,Loss:0.3691\n",
      "Epoch [90/89] ,Loss:0.3553\n",
      "Epoch [100/99] ,Loss:0.3482\n"
     ]
    }
   ],
   "source": [
    "#1. SETUP THE DATA OR PYTORCH\n",
    "# convert standard numpy arrays into pytorch tensors\n",
    "#.unsqueeze(1) changes y from [0,1,0] to [[0],[1],[0]]\n",
    "X_train_tensor = torch.tensor(X_train,dtype= torch.float32)\n",
    "y_train_tensor= torch.tensor(y_train,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 2. DEFINE THE TEACHER AND CORRECTOR\n",
    "criterion=nn.BCELoss() #binary cross entropy loss (teacher)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01) #adam optimiser (the corrector)\n",
    "\n",
    "#3. THE TRAINING LOOP\n",
    "epochs=100 # how many times we go through the dataset\n",
    "for epochs in range(epochs):\n",
    "  #A. FORWARD PASS (THE GUESS)\n",
    "  y_pred=model(X_train_tensor)\n",
    "\n",
    "  #B. CALCULATE LOSS(THE GRADE)\n",
    "  loss=criterion(y_pred,y_train_tensor)\n",
    "  #C. BACKWARD PASS(THE Learning )\n",
    "  optimizer.zero_grad() #clear previous calculation\n",
    "  loss.backward() #calculate gradients (how much to adjust each weight)\n",
    "  optimizer.step() #update weights\n",
    "\n",
    "  #monitoring\n",
    "  if(epochs +1)%10 == 0 :\n",
    "    print(f'Epoch [{epochs+1}/{epochs}] ,Loss:{loss.item():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkemSAVgaEd6"
   },
   "source": [
    "# 4 .THE EVALUATION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgj2r4leaTN_",
    "outputId": "c128c56e-c04e-4ebc-8961-f6f27d2cb9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy :0.8545\n"
     ]
    }
   ],
   "source": [
    "# PREPARE THE TEST DATA\n",
    "X_test_tensor=torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor=torch.tensor(y_test,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "#2 evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#the exam\n",
    "with torch.no_grad():\n",
    "  #make predictions\n",
    "  y_pred_prob=model(X_test_tensor)\n",
    "\n",
    "  #convert probabilty to yes or no\n",
    "  #if prob >0.5,it rounds to 1 and if prob<0.5 it rounds to 0\n",
    "  y_pred_cls=y_pred_prob.round()\n",
    "\n",
    "  #grade the exam\n",
    "  #.eq() compares predictions vs truth sum() counts the matches\n",
    "  correct_count=y_pred_cls.eq(y_test_tensor).sum().item()\n",
    "  accuracy = correct_count/y_test.shape[0]\n",
    "  print(f\"test accuracy :{accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndi7BVOgucCu"
   },
   "source": [
    "### WHOLE THEME OR SUMMARY OF WHAT I HAVE DONE HERE IN CODE\n",
    "1] WE HAVE A PROBLEM IN A BANK THAT MANY CUSTOMERS OF DIFFERENT FEATURES AND BACKGROUND ARE LEAVING IT AS MENTIONED IN CHURN CSV FILE ,WE HAVE TO ANALYSE THAT DATA BY OUR ML MODEL,OUR ML MODEL DRAW INFERENCES OR CONNECTION BETWWEN PEOPLE'S DIFFERENT FEATURES LIKE COUNTRY,INCOMEAND OTHER COLUMN MENTIONED IN CSV FILE AND THEIR PROBABILITY OF LEAVING THE COMPANY .THEN IT WILL ADJUST ITS WEIGHT TO GIVE US ACCURATE PROBABILITIES OF WHETHER A PERSON WILL LEAVE OR NOT BASED ON WHAT IT HAS LEARNED FROM PREVIOUS DATA .NOW IF WE WILL GIVE THIS DATA A NEW PERSON WITH CERTAIN FEATURES IT WILL PREDICT THE PROBABILITY OF THAT PERSON LEAVING OR NOT ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3NG3J3JMrjD"
   },
   "source": [
    "LSTM PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "peWQzIEu8Os2"
   },
   "outputs": [],
   "source": [
    "#WIKI TEXT DATA LOADER\n",
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from io import open\n",
    "\n",
    "class Dictionary(object):\n",
    "    \"\"\"\n",
    "    Maintains a bijection between words and unique integer indices.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    \"\"\"\n",
    "    Handles file loading, tokenization, and tensorization of the text.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='./data'):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "        # Auto-download logic to make this script self-contained\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # We check for the train file. If missing, we download WikiText-2.\n",
    "        train_path = os.path.join(path, 'wikitext-2.train.txt')\n",
    "        if not os.path.exists(train_path):\n",
    "            print(\"Downloading WikiText-2 dataset...\")\n",
    "            url = \"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\"\n",
    "            urllib.request.urlretrieve(url, os.path.join(path, 'wikitext-2.train.txt'))\n",
    "            urllib.request.urlretrieve(url.replace('train', 'valid'), os.path.join(path, 'wikitext-2.valid.txt'))\n",
    "            urllib.request.urlretrieve(url.replace('train', 'test'), os.path.join(path, 'wikitext-2.test.txt'))\n",
    "            print(\"Download complete.\")\n",
    "\n",
    "        self.train = self.tokenize(os.path.join(path, 'wikitext-2.train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wikitext-2.valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wikitext-2.test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>'] # Add End of Sentence token\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5Tb_QsMXM45L"
   },
   "outputs": [],
   "source": [
    "#LSTM MODEL ARCHITECTURE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard LSTM Language Model.\n",
    "\n",
    "    Structure:\n",
    "    Embedding Layer -> LSTM Layer(s) -> Linear Decoder -> Output Probabilities\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # 1. Encoder: Turns word IDs (0, 1, 55) into dense vectors\n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # 2. LSTM: The brain. We use 'batch_first=False' (Sequence Length, Batch, Hidden)\n",
    "        # This is the standard shape for NLP in PyTorch.\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, nlayers, dropout=dropout)\n",
    "\n",
    "        # 3. Decoder: Transforms hidden states back to vocabulary size\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # \"Tied Weights\" Optimization\n",
    "        # If true, the embedding weights and decoder weights share memory.\n",
    "        # This forces the model to learn a unified semantic space for input and output.\n",
    "        if tie_weights:\n",
    "            if hidden_size != embed_size:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights to small uniform values for stability.\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        \"\"\"\n",
    "        input_seq: shape [seq_len, batch_size] containing word IDs\n",
    "        hidden_state: tuple (h_0, c_0) from previous batch\n",
    "        \"\"\"\n",
    "        # Get embeddings and apply dropout\n",
    "        emb = self.drop(self.encoder(input_seq))\n",
    "\n",
    "        # Pass through LSTM\n",
    "        # output shape: [seq_len, batch_size, hidden_size]\n",
    "        output, hidden_state = self.rnn(emb, hidden_state)\n",
    "\n",
    "        # Apply dropout to the output of the LSTM\n",
    "        output = self.drop(output)\n",
    "\n",
    "        # Flatten output for the linear layer: [seq_len * batch_size, hidden_size]\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "\n",
    "        # Return decoded logits and the new hidden state\n",
    "        # Reshape decoded to [seq_len, batch_size, vocab_size]\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Creates the initial zero state for the LSTM.\"\"\"\n",
    "        weight = next(self.parameters())\n",
    "        # LSTM needs two hidden states: (h_0, c_0)\n",
    "        return (weight.new_zeros(self.nlayers, batch_size, self.hidden_size),\n",
    "                weight.new_zeros(self.nlayers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "nkyvDC3aM-AR",
    "outputId": "08a23ca9-d168-47ec-c328-74b40ce6dfe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data...\n",
      "Downloading WikiText-2 dataset...\n",
      "Download complete.\n",
      "Vocabulary Size: 33278\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |   200/ 2983 batches | loss  7.61 | ppl  2010.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1675773350.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;31m# Final Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1675773350.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepackage_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1561471673.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, hidden_state)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Flatten output for the linear layer: [seq_len * batch_size, hidden_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Return decoded logits and the new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING ENGINE\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 20      # How many independent sentences we process in parallel\n",
    "EVAL_BATCH_SIZE = 10\n",
    "SEQ_LEN = 35         # BPTT (Backprop Through Time) window size\n",
    "EMBED_SIZE = 200     # Dimension of word vectors\n",
    "HIDDEN_SIZE = 200    # Dimension of LSTM memory\n",
    "LAYERS = 2           # Number of stacked LSTMs\n",
    "DROPOUT = 0.2        # Regularization\n",
    "EPOCHS = 5           # (Keep low for demo)\n",
    "LR = 20.0            # Initial learning rate\n",
    "CLIP = 0.25          # Gradient clipping threshold\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Preparation ---\n",
    "print(\"Loading data...\")\n",
    "corpus = Corpus('./data') # Changed from data_loader.Corpus\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    \"\"\"\n",
    "    Reshapes the 1D data tensor into a grid of size [N // bsz, bsz].\n",
    "    This allows us to process 'bsz' streams of text in parallel.\n",
    "    \"\"\"\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "train_data = batchify(corpus.train, BATCH_SIZE)\n",
    "val_data = batchify(corpus.valid, EVAL_BATCH_SIZE)\n",
    "test_data = batchify(corpus.test, EVAL_BATCH_SIZE)\n",
    "\n",
    "vocab_size = len(corpus.dictionary)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "# --- Model Setup ---\n",
    "model = RNNModel(vocab_size, EMBED_SIZE, HIDDEN_SIZE, LAYERS, DROPOUT, tie_weights=True).to(device) # Changed from model.RNNModel\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "    Without this, backprop would try to go all the way back to the start of the epoch!\n",
    "    \"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    \"\"\"\n",
    "    Gets a slice of data of length SEQ_LEN.\n",
    "    Target is simply the input shifted by 1 word.\n",
    "    \"\"\"\n",
    "    seq_len = min(SEQ_LEN, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "def evaluate(data_source):\n",
    "    \"\"\"Evaluates the model on validation or test data.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(EVAL_BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, SEQ_LEN):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "\n",
    "            # Reshape output for loss: [seq_len * batch_size, vocab_size]\n",
    "            output_flat = output.view(-1, vocab_size)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Main training loop.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, SEQ_LEN)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "\n",
    "        # Detach hidden state from previous batch\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent \"exploding gradient\" problem common in LSTMs\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr) # Manual SGD update\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 200 == 0 and batch > 0:\n",
    "            cur_loss = total_loss / 200\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{len(train_data)//SEQ_LEN:5d} batches | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# --- Execution ---\n",
    "best_val_loss = None\n",
    "lr = LR\n",
    "\n",
    "try:\n",
    "    print(\"-\" * 89)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print(\"-\" * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
    "              f'valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}')\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        # Learning Rate Annealing: If validation loss doesn't improve, lower the learning rate\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr /= 4.0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Final Test\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSJKYCQE73Oy"
   },
   "source": [
    "THIS LSTM MODEL IS FOR PREDICTING NEXT OUTPUT FROM PREVIOUS INPUTS USING THE LSTM ARCHITECTURE AND ADVANTAGE IN USING LSTM IT CAN GENERATE TEXT BY MAINTAINING CELL STATE AND FORGET FUNCTION LSTM CAN EASILY MAINTAIN CONTEXT.Language Modelling by LSTM refers to the specific technical process of training an LSTM network to learn the probability distribution of the next word (or character) in a sequence, given the previous words.In simple terms, it is teaching the LSTM to answer the question: \"Given the history I have seen so far, what is the most likely word to happen next?\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
