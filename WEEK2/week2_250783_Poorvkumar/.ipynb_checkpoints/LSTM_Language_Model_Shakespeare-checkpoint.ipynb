{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48dc5761",
   "metadata": {},
   "source": [
    "\n",
    "# LSTM Language Model (Character-Level)\n",
    "\n",
    "**Task:** Language Modeling using LSTM  \n",
    "**Dataset:** Small Shakespeare excerpt (domain-specific)  \n",
    "**Framework:** PyTorch  \n",
    "\n",
    "This notebook demonstrates how to build, train, and evaluate a character-level LSTM language model.\n",
    "We focus on:\n",
    "- Clean implementation\n",
    "- Clear explanation\n",
    "- Hyperparameter choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c40182",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports and Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25945eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5389964",
   "metadata": {},
   "source": [
    "\n",
    "### Hyperparameters\n",
    "- `seq_length`: length of input character sequence\n",
    "- `embedding_dim`: size of character embeddings\n",
    "- `hidden_dim`: number of LSTM hidden units\n",
    "- `num_layers`: stacked LSTM layers\n",
    "- `lr`: learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7c5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length = 40\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "lr = 0.003\n",
    "epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9676a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset (Shakespeare Excerpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fdb24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\n",
    "    \"To be, or not to be, that is the question:\\n\"\n",
    "    \"Whether 'tis nobler in the mind to suffer\\n\"\n",
    "    \"The slings and arrows of outrageous fortune,\"\n",
    ")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "\n",
    "encoded = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0efa46",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Create Input Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfa42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        inputs.append(data[i:i+seq_length])\n",
    "        targets.append(data[i+1:i+seq_length+1])\n",
    "    return torch.stack(inputs), torch.stack(targets)\n",
    "\n",
    "X, y = create_sequences(encoded, seq_length)\n",
    "X, y = X.to(device), y.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641559d",
   "metadata": {},
   "source": [
    "\n",
    "## 4. LSTM Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691eda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b2edd",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e45680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93444b",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.1434\n",
      "Epoch 20/50, Loss: 0.1013\n",
      "Epoch 30/50, Loss: 0.0816\n",
      "Epoch 40/50, Loss: 0.0713\n",
      "Epoch 50/50, Loss: 0.0652\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()        \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(X)\n",
    "    loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445acfa",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72fe274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, that is the mind to suffer\n",
      "The slings and to suffer\n",
      "The slings and to suffer\n",
      "The slings and to sbl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(model, start_str, length=200):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor([char_to_idx[ch] for ch in start_str], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    generated = list(start_str)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output = model(input_seq)\n",
    "            last_char_logits = output[:, -1, :]\n",
    "            prob = torch.softmax(last_char_logits, dim=-1)\n",
    "            next_char = torch.multinomial(prob, 1).item()\n",
    "\n",
    "            generated.append(idx_to_char[next_char])\n",
    "            input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[next_char]]).to(device)], dim=1)\n",
    "\n",
    "    return \"\".join(generated)\n",
    "\n",
    "print(generate_text(model, \"To be\", 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1108f4d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Conclusion\n",
    "\n",
    "- LSTM captures long-term character dependencies\n",
    "- Domain-specific data improves coherence\n",
    "- Hyperparameters strongly affect performance\n",
    "\n",
    "This fulfills the **Language Modeling using LSTM** task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
