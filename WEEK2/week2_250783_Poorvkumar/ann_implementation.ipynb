{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVRzzmNUwoTwqixK98f2En"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pC_nIL4U_-A","executionInfo":{"status":"ok","timestamp":1767131351146,"user_tz":-330,"elapsed":29801,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"f26a4a02-89f8-4d50-930d-40f4123bbef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/100], Loss: 0.5081\n","Epoch [20/100], Loss: 0.3913\n","Epoch [30/100], Loss: 0.3523\n","Epoch [40/100], Loss: 0.3228\n","Epoch [50/100], Loss: 0.2978\n","Epoch [60/100], Loss: 0.2751\n","Epoch [70/100], Loss: 0.2538\n","Epoch [80/100], Loss: 0.2327\n","Epoch [90/100], Loss: 0.2116\n","Epoch [100/100], Loss: 0.1901\n","Test Accuracy: 0.772\n"]}],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = pd.read_csv(\"Churn_Modelling.csv\")\n","\n","# Separate features and target\n","X = data.drop(columns=[data.columns[-1]])\n","y = data[data.columns[-1]]\n","\n","# Encode categorical columns\n","X = pd.get_dummies(X, drop_first=True)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Feature scaling\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","\n","y_train = torch.tensor(y_train.values, dtype=torch.long)\n","y_test = torch.tensor(y_test.values, dtype=torch.long)\n","\n","# Define ANN model\n","class ANN(nn.Module):\n","    def __init__(self, input_size):\n","        super(ANN, self).__init__()\n","\n","        # Hyperparameters:\n","        # 2 hidden layers for sufficient learning capacity\n","        self.fc1 = nn.Linear(input_size, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.out = nn.Linear(32, len(torch.unique(y_train)))\n","\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        return self.out(x)\n","\n","# Initialize model, loss, optimizer\n","model = ANN(X_train.shape[1])\n","\n","criterion = nn.CrossEntropyLoss()   # taking cross entropy as loss function\n","optimizer = optim.Adam(model.parameters(), lr=0.001) # taking adam as optimizer\n","\n","# 10. Training loop\n","epochs = 100\n","for epoch in range(epochs):\n","    model.train()\n","\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch + 1) % 10 == 0:\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n","\n","# 11. Evaluation\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test)\n","    _, preds = torch.max(outputs, 1)\n","    accuracy = accuracy_score(y_test.numpy(), preds.numpy())\n","\n","print(\"Test Accuracy:\", accuracy)\n"]}]}