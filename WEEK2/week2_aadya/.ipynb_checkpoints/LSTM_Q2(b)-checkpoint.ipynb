{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MwEblt5Dqqvv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Total characters:\", len(text))\n",
        "print(text[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4uu24j3qrHb",
        "outputId": "91682ea1-6964-4601-d5ee-bccbd4e96658"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 976\n",
            "From fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "\n",
            "To be, or not to be, that is the question:\n",
            "Whether 'tis nobler in the mind to suffer\n",
            "The slings and arrows of outrageous fortune,\n",
            "Or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM7k8xtMqxXc",
        "outputId": "edc282cb-ba4a-4c89-b145-a32a453a549f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 40  # hyperparameter\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(text) - seq_length):\n",
        "    inputs.append([char_to_idx[ch] for ch in text[i:i+seq_length]])\n",
        "    targets.append(char_to_idx[text[i+seq_length]])\n",
        "\n",
        "inputs = torch.tensor(inputs)\n",
        "targets = torch.tensor(targets)\n",
        "#seq_length: how much past context the LSTM sees"
      ],
      "metadata": {
        "id": "pkXcZ3-Qq88v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "fpiEvQDsrS_h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 64        # size of character embedding\n",
        "hidden_size = 128      # LSTM memory size\n",
        "learning_rate = 0.003\n",
        "\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "hUjDDccxrY1n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5  # keep small for demo\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDFDjNV0rdyH",
        "outputId": "bdd3fbf5-6239-4954-b416-1c8a224ccea7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 3.8364\n",
            "Epoch [2/5], Loss: 3.7680\n",
            "Epoch [3/5], Loss: 3.6916\n",
            "Epoch [4/5], Loss: 3.5913\n",
            "Epoch [5/5], Loss: 3.4454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(start_text, length=200):\n",
        "    model.eval()\n",
        "    result = start_text\n",
        "\n",
        "    for _ in range(length):\n",
        "        x = torch.tensor([[char_to_idx[ch] for ch in result[-seq_length:]]])\n",
        "        out = model(x)\n",
        "        char_idx = torch.argmax(out, dim=1).item()\n",
        "        result += idx_to_char[char_idx]\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "xeVR19oQrihy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"To be or not\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx0a5qwZrn3O",
        "outputId": "13b8997c-2383-4ac7-c850-50feee0c8393"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not e e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e a e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters are values that are chosen before training a neural network and control how the model learns. In this project, parameters such as sequence length, embedding size, hidden size, learning rate, and number of epochs were selected manually. These values affect the modelâ€™s learning speed, memory capacity, and overall performance."
      ],
      "metadata": {
        "id": "F5TNiHxNsnEM"
      }
    }
  ]
}