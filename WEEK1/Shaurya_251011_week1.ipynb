{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_scratch_text(s):\n",
        "    n = len(s)\n",
        "    for i in range(n):\n",
        "        s[i] = lowercase(s[i])\n",
        "    for i in range(n):\n",
        "        s[i] = clean(s[i])\n",
        "    l = []\n",
        "    for i in range(n):\n",
        "        l += s[i].split()\n",
        "    stop = {'the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'}\n",
        "    words = [w for w in l if w not in stop]\n",
        "    words = helper(words)\n",
        "    return words\n",
        "\n",
        "def lowercase(s):\n",
        "    r = \"\"\n",
        "    for i in s:\n",
        "        if \"A\" <= i <= \"Z\":\n",
        "            r += chr(ord(i) - ord('A') + ord('a'))\n",
        "        else:\n",
        "            r += i\n",
        "    return r\n",
        "\n",
        "def clean(s):\n",
        "    return re.sub(r'[^\\w\\s]', '', s)\n",
        "\n",
        "def helper(words):\n",
        "    suf = (\"ing\", \"ly\", \"ed\", \"s\")\n",
        "    result = []\n",
        "    for w in words:\n",
        "        for suffix in suf:\n",
        "            if w.endswith(suffix) and len(w) > len(suffix):\n",
        "                w = w[:-len(suffix)]\n",
        "                break\n",
        "        result.append(w)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "oR4BKqITy17z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus1 = [(corpus[0])]\n",
        "words = clean_scratch_text(corpus1)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivxKttqcuHJ9",
        "outputId": "d8bea8d2-22ba-43a2-80b1-7fd05f72485b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def rem_stop(s):\n",
        "  l =[w for w in s if w not in stopwords.words(\"english\")]\n",
        "  return l\n",
        "\n",
        "words = []\n",
        "for i in range(len(corpus)):\n",
        "  words += nltk.tokenize.word_tokenize(corpus[i])\n",
        "print(words)\n",
        "\n",
        "stopwords = nltk.corpus.stopwords\n",
        "words1 = rem_stop(words)\n",
        "print(words1)\n",
        "wordl = WordNetLemmatizer()\n",
        "def lem(s):\n",
        "  tokens = [wordl.lemmatize(w) for w in s]\n",
        "  tokens = [wordl.lemmatize(w,pos='v') for w in tokens]\n",
        "  return tokens\n",
        "tokens = lem(words1)\n",
        "print(tokens)\n",
        "\n",
        "m = corpus[1]\n",
        "words2 = word_tokenize(m)\n",
        "words2 = rem_stop(words2)\n",
        "token2 = lem(words2)\n",
        "print(token2)"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1594feb-0a19-4073-8f9e-4d36169f3869"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', 'however', 'ethical', 'concerns', 'remain', 'the', 'pizza', 'was', 'absolutely', 'delicious', 'but', 'the', 'service', 'was', 'terrible', 'i', 'wont', 'go', 'back', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'to', 'be', 'or', 'not', 'to', 'be', 'that', 'is', 'the', 'question', 'whether', 'tis', 'nobler', 'in', 'the', 'mind', 'data', 'science', 'involves', 'statistics', 'linear', 'algebra', 'and', 'machine', 'learning', 'i', 'love', 'machine', 'learning', 'but', 'i', 'hate', 'the', 'math', 'behind', 'it']\n",
            "['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concerns', 'remain', 'pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'question', 'whether', 'tis', 'nobler', 'mind', 'data', 'science', 'involves', 'statistics', 'linear', 'algebra', 'machine', 'learning', 'love', 'machine', 'learning', 'hate', 'math', 'behind']\n",
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain', 'pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'question', 'whether', 'ti', 'nobler', 'mind', 'data', 'science', 'involve', 'statistic', 'linear', 'algebra', 'machine', 'learn', 'love', 'machine', 'learn', 'hate', 'math', 'behind']\n",
            "['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "def clean(s):\n",
        "    return re.sub(r'[^\\w\\s]', '', s)\n",
        "\n",
        "def make_vocab(s):\n",
        "  vocab = []\n",
        "  for i in range(len(s)):\n",
        "    words = word_tokenize(s[i])\n",
        "    for k in range(len(words)):\n",
        "      if(words[k] not in vocab):\n",
        "        vocab.append(words[k])\n",
        "  return vocab\n",
        "\n",
        "def vectorise(s,vocab):\n",
        "  n = len(s)\n",
        "  for i in range(n):\n",
        "    s[i] = lowercase(s[i])\n",
        "  for i in range(n):\n",
        "    s[i] = clean(s[i])\n",
        "  l = []\n",
        "  for i in range(n):\n",
        "    l += s[i].split()\n",
        "  count = [0]*len(vocab)\n",
        "  for i in range(len(vocab)):\n",
        "    for k in l:\n",
        "      if(vocab[i]==k):\n",
        "        count[i] += 1\n",
        "  return count\n",
        "vocab = make_vocab(corpus)\n",
        "vocab.sort()\n",
        "print(vocab)\n",
        "\n",
        "count = vectorise(corpus,vocab)\n",
        "print(count)"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dead377-aad1-45c3-a90e-7eaabdc8ed95"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['absolutely', 'algebra', 'and', 'artificial', 'back', 'be', 'behind', 'brown', 'but', 'concerns', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'i', 'in', 'intelligence', 'involves', 'is', 'it', 'jumps', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'not', 'or', 'over', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistics', 'terrible', 'that', 'the', 'tis', 'to', 'transforming', 'was', 'whether', 'wont', 'world']\n",
            "[1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 2, 1, 2, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "vocab1 = make_vocab(s)\n",
        "bow = vectorise(s,vocab1)\n",
        "print(vocab1)\n",
        "print(bow)\n",
        "\n",
        "#clearly this is not good\n",
        "#our own built function was better than this lets implement that"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfbVRM0i8ESz",
        "outputId": "d97fa9a0-e2c3-4fac-e1c0-d6f8251d2497"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "[0, 1, 1, 1, 1, 1, 2, 1, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_maker(s):\n",
        "      n = len(s)\n",
        "      for i in range(n):\n",
        "        s[i] = lowercase(s[i])\n",
        "      for i in range(n):\n",
        "        s[i] = clean(s[i])\n",
        "      l = []\n",
        "      for i in range(n):\n",
        "        l += s[i].split()\n",
        "      unique = []\n",
        "      for w in l:\n",
        "        if w not in unique:\n",
        "            unique.append(w)\n",
        "      return unique\n",
        "vocab2 = vocab_maker(s)\n",
        "bow1 = vectorise(s,vocab2)\n",
        "print(vocab2)\n",
        "print(bow1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f4C_4BS9PCF",
        "outputId": "24799851-74a0-45ba-d932-788a9ff809c6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog']\n",
            "[2, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer()\n",
        "tok = vect.fit_transform(corpus)\n",
        "print(tok.toarray())\n"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb15f694-76a7-4d8f-ba31-0214d140477e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "s = corpus[5]\n",
        "target = \"machine\"\n",
        "def tf(s,target):\n",
        "  words = s.split(\" \")\n",
        "  return(words.count(target)/len(words))\n",
        "print(tf(s,target))\n",
        "\n",
        "tot = 1\n",
        "d = 1\n",
        "def df(n,m):\n",
        "  return(math.log(n/m))\n",
        "print(df(tot,d))\n",
        "\n",
        "print(tf(s,target)*df(tot,d))"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21df942d-bb83-4c7b-8844-f9b0c3ab3e1c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09090909090909091\n",
            "0.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfv = TfidfVectorizer()\n",
        "tfidf = tfv.fit_transform(corpus)\n",
        "words = (tfv.get_feature_names_out())\n",
        "tfidf_values = (tfidf.toarray()[0])\n",
        "tfidf_dict = dict(zip(words, tfidf_values))\n",
        "print(tfidf_dict)"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81088bbb-7fe0-42c0-8f46-5c95131707f3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'absolutely': np.float64(0.0), 'algebra': np.float64(0.0), 'and': np.float64(0.0), 'artificial': np.float64(0.3345454287016015), 'back': np.float64(0.0), 'be': np.float64(0.0), 'behind': np.float64(0.0), 'brown': np.float64(0.0), 'but': np.float64(0.0), 'concerns': np.float64(0.3345454287016015), 'data': np.float64(0.0), 'delicious': np.float64(0.0), 'dog': np.float64(0.0), 'ethical': np.float64(0.3345454287016015), 'fox': np.float64(0.0), 'go': np.float64(0.0), 'hate': np.float64(0.0), 'however': np.float64(0.3345454287016015), 'in': np.float64(0.0), 'intelligence': np.float64(0.3345454287016015), 'involves': np.float64(0.0), 'is': np.float64(0.27433203727401334), 'it': np.float64(0.0), 'jumps': np.float64(0.0), 'lazy': np.float64(0.0), 'learning': np.float64(0.0), 'linear': np.float64(0.0), 'love': np.float64(0.0), 'machine': np.float64(0.0), 'math': np.float64(0.0), 'mind': np.float64(0.0), 'nobler': np.float64(0.0), 'not': np.float64(0.0), 'or': np.float64(0.0), 'over': np.float64(0.0), 'pizza': np.float64(0.0), 'question': np.float64(0.0), 'quick': np.float64(0.0), 'remain': np.float64(0.3345454287016015), 'science': np.float64(0.0), 'service': np.float64(0.0), 'statistics': np.float64(0.0), 'terrible': np.float64(0.0), 'that': np.float64(0.0), 'the': np.float64(0.17139656473798645), 'tis': np.float64(0.0), 'to': np.float64(0.0), 'transforming': np.float64(0.3345454287016015), 'was': np.float64(0.0), 'whether': np.float64(0.0), 'wont': np.float64(0.0), 'world': np.float64(0.3345454287016015)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "snt = [word_tokenize(s.lower()) for s in corpus]\n",
        "\n",
        "vect = Word2Vec(snt, vector_size=10, min_count=1)\n",
        "\n",
        "print(vect.wv[\"learning\"])"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c703387-e9a6-4b89-b1d2-048e942f30ba"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.07309777  0.05074635  0.06763472  0.00753128  0.06360789 -0.03414256\n",
            " -0.00940346  0.05780506 -0.07521617 -0.03939066]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "predict = glove_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(predict[0][0])"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb6c37c-5b2f-4331-bfb1-fa1874a53fed"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "#nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "print(sia.polarity_scores(corpus[1]))\n",
        "print(sia.polarity_scores(corpus[5]))"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67096f4-0297-480f-94c9-b35e60a4cc38"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.235, 'neu': 0.623, 'pos': 0.141, 'compound': -0.3926}\n",
            "{'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"
          ]
        }
      ]
    }
  ]
}