{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#write rest of the code here\n",
        "\n",
        "##LOWERCASING\n",
        "t=\"Artificial Intelligence is transforming the world; however, ethical concerns remain!\"\n",
        "t=t.lower()\n",
        "print(t)\n",
        "print(\" \")##adds blank line after printing\n",
        "\n",
        "##TOKENIZATION\n",
        "doc=t.split()\n",
        "print(doc)\n",
        "print(\" \")##adds blank line after printing\n",
        "\n",
        "##REMOVE THE SPECIAL CHARACTERS\n",
        "clean_text=\" \"\n",
        "clean_text1=\" \"\n",
        "char_to_remove=\"!.,:;'...\"\n",
        "for char in t:\n",
        "  if char not in char_to_remove:\n",
        "    clean_text+=char\n",
        "  else:\n",
        "    clean_text1+=char\n",
        "print(\"Sentence after removing special characters:\")\n",
        "print(clean_text)\n",
        "print(\" \")##adds blank line after printing\n",
        "\n",
        "##STOPWORD REMOVAL\n",
        "!pip install -u spacy==3\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "stop_words=[\"the\",\"is\",\"in\",\"to\",\"of\",\"and\",\"a\",\"it\",\"was\",\"but\",\"or\"]\n",
        "print(\"Sentence after removing stopwords:\")\n",
        "for word in doc:\n",
        "  if word not in stop_words:\n",
        "    print(word)\n",
        "\n",
        "\n",
        "##Stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def stemmed_st(t):\n",
        " ps=PorterStemmer()\n",
        " print(\"\\n Stemming a sentence :\")\n",
        " tk_words=word_tokenize(t)\n",
        " st_tk_words=[ps.stem(words) for word in tk_words]\n",
        " stemmed_st=\" \".join(st_tk_words)\n",
        " print(stemmed_st)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2279c4a-3408-4e18-bcc0-7f0e93000eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial intelligence is transforming the world; however, ethical concerns remain!\n",
            " \n",
            "['artificial', 'intelligence', 'is', 'transforming', 'the', 'world;', 'however,', 'ethical', 'concerns', 'remain!']\n",
            " \n",
            "Sentence after removing special characters:\n",
            " artificial intelligence is transforming the world however ethical concerns remain\n",
            " \n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -u\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.8.11                        \n",
            "Location         /usr/local/lib/python3.12/dist-packages/spacy\n",
            "Platform         Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Python version   3.12.12                       \n",
            "Pipelines        en_core_web_sm (3.8.0)        \n",
            "\n",
            "Sentence after removing stopwords:\n",
            "artificial\n",
            "intelligence\n",
            "transforming\n",
            "world;\n",
            "however,\n",
            "ethical\n",
            "concerns\n",
            "remain!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#write rest of the code here\n",
        "##TOKENIZATION\n",
        "x=\"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\"\n",
        "words=word_tokenize(x)\n",
        "print(words)\n",
        "print(\" \")##adds blank line after printing\n",
        "\n",
        "##STOPWORDS\n",
        "stop_words=set(stopwords.words('english'))\n",
        "tokens=word_tokenize(x.lower())\n",
        "\n",
        "filtered_tokens=[word for word in tokens if word not in stop_words]\n",
        "print(\"Original:\",tokens)\n",
        "print(\"Filtered Tokens:\",filtered_tokens)\n",
        "print(\" \")##adds blank line after printing\n",
        "\n",
        "##LEMMATIZATION\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "tokens1=word_tokenize(x)\n",
        "lemmitized_words=[lemmatizer.lemmatize(word) for word in tokens1]\n",
        "print(f\"Original Text:{x}\")\n",
        "print(f\"Lemmatized Text:{lemmitized_words}\")\n"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8baf08d-5474-491c-887a-a206d6c1fc2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'pizza', 'was', 'absolutely', 'delicious', ',', 'but', 'the', 'service', 'was', 'terrible', '...', 'I', 'wo', \"n't\", 'go', 'back', '.']\n",
            " \n",
            "Original: ['the', 'pizza', 'was', 'absolutely', 'delicious', ',', 'but', 'the', 'service', 'was', 'terrible', '...', 'i', 'wo', \"n't\", 'go', 'back', '.']\n",
            "Filtered Tokens: ['pizza', 'absolutely', 'delicious', ',', 'service', 'terrible', '...', 'wo', \"n't\", 'go', 'back', '.']\n",
            " \n",
            "Original Text:The pizza was absolutely delicious, but the service was terrible ... I won't go back.\n",
            "Lemmatized Text:['The', 'pizza', 'wa', 'absolutely', 'delicious', ',', 'but', 'the', 'service', 'wa', 'terrible', '...', 'I', 'wo', \"n't\", 'go', 'back', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OTlyWNyiPob"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# rest of the code here\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "corpus=[\"\"]\n",
        "def build_vocabulary(corpus):\n",
        "\n",
        "  for sentence in corpus:\n",
        "    words=[]\n",
        "    cleaned_sentence = re.sub(r'[^\\w\\s]','',sentence).lower()\n",
        "    words=cleaned_sentence.split()\n",
        "    words= words.extend(words)\n",
        "    vocabulary=sorted(list(set(words)))\n",
        "    return vocabulary\n",
        "\n",
        "def vectorize_sentence(sentence,vocabulary):\n",
        " cleaned_sentence = re.sub(r'[^\\w\\]','',sentence).lower()\n",
        " words=cleaned_sentence.split()\n",
        " word_counts=Counter(words)\n",
        " bow_vector=[word_counts.get(word,0) for word in vocabulary]\n",
        " return bow_vector\n",
        "\n",
        " vocabulary=build_vocab(corpus)\n",
        " sentence=\"The quick brown fox jumps over the lazy dog.\"\n",
        " bow_vector=vectorize_sentence(sentence,vocabulary)\n",
        "\n",
        " print(\"--BAG OF WORDS IMPLEMENTATION--\")\n",
        " print(\"Vocabulary:\",vocabulary)\n",
        " print(\"BOW Vector for sentence:\",sentence)\n",
        " print(bow_vector)\n"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3ffa69-a49b-41b2-a238-c219796b9d9f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsVLdz0ZyTP2"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBZrTqKWVyS2"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#rest of the code here\n",
        "corpus=[\"Artificial Intelligence is transforming the world; howver , ehical concerns remain!\",\n",
        "\"The pizza was absolutely delicious , but the service was terrible ... I won't go back\",\n",
        "\"The quick brown fox jumps over the lazy dog\",\n",
        "\"To be, or not to be , that is the question ; Whether 'tis nobler in the mind\",\n",
        "\"Data science involves statistics , linear algebra, and machine learning.\",\n",
        "\"I love machine learning , but I hate the math behind it\"]\n",
        "tr_idf_model=TfidfVectorizer() # Corrected TfidVectorizer to TfidfVectorizer\n",
        "tf_idf_vector=tr_idf_model.fit_transform(corpus)\n",
        "print(tf_idf_vector.shape)\n",
        "print(tf_idf_vector.toarray())"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0959dd3-c9e1-47a3-c35a-5b58b3425533"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 52)\n",
            "[[0.         0.         0.         0.33454543 0.         0.\n",
            "  0.         0.         0.         0.33454543 0.         0.\n",
            "  0.         0.33454543 0.         0.         0.         0.33454543\n",
            "  0.         0.33454543 0.         0.27433204 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.33454543 0.         0.         0.\n",
            "  0.         0.         0.17139656 0.         0.         0.33454543\n",
            "  0.         0.         0.         0.33454543]\n",
            " [0.26995162 0.         0.         0.         0.26995162 0.\n",
            "  0.         0.         0.22136419 0.         0.         0.26995162\n",
            "  0.         0.         0.         0.26995162 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.26995162\n",
            "  0.         0.         0.         0.         0.26995162 0.\n",
            "  0.26995162 0.         0.27660686 0.         0.         0.\n",
            "  0.53990324 0.         0.26995162 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.35245556 0.         0.         0.         0.\n",
            "  0.35245556 0.         0.35245556 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.35245556\n",
            "  0.35245556 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.35245556 0.\n",
            "  0.         0.35245556 0.         0.         0.         0.\n",
            "  0.         0.         0.3611448  0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.4622213\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.23111065 0.         0.         0.18951404 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.23111065 0.23111065 0.23111065 0.23111065 0.         0.\n",
            "  0.23111065 0.         0.         0.         0.         0.\n",
            "  0.         0.23111065 0.23680833 0.23111065 0.4622213  0.\n",
            "  0.         0.23111065 0.         0.        ]\n",
            " [0.         0.3461711  0.3461711  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.3461711  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.3461711  0.         0.         0.\n",
            "  0.         0.28386526 0.3461711  0.         0.28386526 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.3461711  0.         0.3461711\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.37063105 0.         0.30392276 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.37063105 0.\n",
            "  0.         0.         0.         0.         0.37063105 0.\n",
            "  0.         0.30392276 0.         0.37063105 0.30392276 0.37063105\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.18988419 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "\n",
        "# The full corpus from the problem description\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "target_word = \"machine\"\n",
        "last_sentence_index\n",
        "last_sentence = corpus[last_sentence_index]\n",
        "\n",
        "\n",
        "cleaned_last_sentence = re.sub(r'[^\\w\\s]', '', last_sentence).lower()\n",
        "words_in_last_sentence = cleaned_last_sentence.split()\n",
        "\n",
        "\n",
        "count_machine_in_sentence = words_in_last_sentence.count(target_word)\n",
        "\n",
        "\n",
        "total_words_in_sentence = len(words_in_last_sentence)\n",
        "\n",
        "\n",
        "if total_words_in_sentence > 0:\n",
        "    tf = count_machine_in_sentence / total_words_in_sentence\n",
        "else:\n",
        "    tf = 0.0\n",
        "\n",
        "print(f\"Term Frequency (TF) for '{target_word}' in the last sentence: {tf:.4f}\")\n",
        "\n",
        "\n",
        "total_documents = len(corpus)\n",
        "documents_containing_machine = 0\n",
        "\n",
        "for doc in corpus:\n",
        "\n",
        "    cleaned_doc = re.sub(r'[^ȦȦȦȦ\\s]', '', doc).lower()\n",
        "    if target_word in cleaned_doc.split():\n",
        "        documents_containing_machine += 1\n",
        "\n",
        "\n",
        "if documents_containing_machine > 0:\n",
        "    idf = math.log(total_documents / documents_containing_machine)\n",
        "else:\n",
        "    idf = 0.0\n",
        "\n",
        "print(f\"Inverse Document Frequency (IDF) for '{target_word}' across the corpus: {idf:.4f}\")\n",
        "\n",
        "tf_idf_score = tf * idf\n",
        "print(f\"Manual TF-IDF score for '{target_word}' in the last sentence: {tf_idf_score:.4f}\")"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a41700d-a53a-4612-8b1e-a58bec7fe48e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term Frequency (TF) for 'machine' in the last sentence: 0.0909\n",
            "Inverse Document Frequency (IDF) for 'machine' across the corpus: 0.0000\n",
            "Manual TF-IDF score for 'machine' in the last sentence: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# rest of the code here\n",
        "tr_idf_model=TfidfVectorizer()\n",
        "corpus1=[\"Artificial Intelligence is transforming the world;however,ethical concerns remain!.\",\n",
        "         \"The pizza was absolutely delicious , but the service was terrible ...I won,t go back.\",\n",
        "         \"The quick brown fox jumps over the lazy dog.\",\n",
        "         \"To be, or not to be , that is the question ; Whether 'tis nobler in the mind.\",\n",
        "         \"Data science involves statistics , linear algebra, and machine learning.\",\n",
        "         \"I love machine learning , but I hate the math behind it\"]\n",
        "tf_idf_vector=tr_idf_model.fit_transform(corpus1)\n",
        "tf_array=tf_idf_vector.toarray()\n",
        "\n",
        "\n",
        "df_tf_idf=pd.DataFrame(tf_array,columns=tr_idf_model.get_feature_names_out())\n",
        "print(df_tf_idf)\n",
        "\n",
        "tf_idf_vector=tr_idf_model.fit_transform(corpus) [:1]\n",
        "print(tf_idf_vector)"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb98ad8b-7939-4fc8-a072-e875f3ebdb8a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   absolutely   algebra       and  artificial      back        be    behind  \\\n",
            "0    0.000000  0.000000  0.000000    0.334545  0.000000  0.000000  0.000000   \n",
            "1    0.269952  0.000000  0.000000    0.000000  0.269952  0.000000  0.000000   \n",
            "2    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000   \n",
            "3    0.000000  0.000000  0.000000    0.000000  0.000000  0.462221  0.000000   \n",
            "4    0.000000  0.346171  0.346171    0.000000  0.000000  0.000000  0.000000   \n",
            "5    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.370631   \n",
            "\n",
            "      brown       but  concerns  ...  terrible      that       the       tis  \\\n",
            "0  0.000000  0.000000  0.334545  ...  0.000000  0.000000  0.171397  0.000000   \n",
            "1  0.000000  0.221364  0.000000  ...  0.269952  0.000000  0.276607  0.000000   \n",
            "2  0.352456  0.000000  0.000000  ...  0.000000  0.000000  0.361145  0.000000   \n",
            "3  0.000000  0.000000  0.000000  ...  0.000000  0.231111  0.236808  0.231111   \n",
            "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "5  0.000000  0.303923  0.000000  ...  0.000000  0.000000  0.189884  0.000000   \n",
            "\n",
            "         to  transforming       was   whether       won     world  \n",
            "0  0.000000      0.334545  0.000000  0.000000  0.000000  0.334545  \n",
            "1  0.000000      0.000000  0.539903  0.000000  0.269952  0.000000  \n",
            "2  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "3  0.462221      0.000000  0.000000  0.231111  0.000000  0.000000  \n",
            "4  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "5  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[6 rows x 52 columns]\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 10 stored elements and shape (1, 10)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t0.31622776601683794\n",
            "  (0, 4)\t0.31622776601683794\n",
            "  (0, 5)\t0.31622776601683794\n",
            "  (0, 8)\t0.31622776601683794\n",
            "  (0, 7)\t0.31622776601683794\n",
            "  (0, 9)\t0.31622776601683794\n",
            "  (0, 3)\t0.31622776601683794\n",
            "  (0, 2)\t0.31622776601683794\n",
            "  (0, 1)\t0.31622776601683794\n",
            "  (0, 6)\t0.31622776601683794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "#rest of the code here\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go again\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "cleaned_corpus=[]\n",
        "for sentence in corpus:\n",
        "  # Corrected the regular expression to remove non-word characters and keep whitespace\n",
        "  cleaned_sentence=re.sub(r'[^\\w\\s]','',sentence).lower()\n",
        "  words=word_tokenize(cleaned_sentence)\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  filtered_words=[word for word in words if word not in stop_words]\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  lemmatized_words=[lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "  cleaned_corpus.append(lemmatized_words)\n",
        "\n",
        "# Train the model after the entire corpus has been cleaned and tokenized\n",
        "model=Word2Vec(cleaned_corpus,min_count=1,vector_size=10)\n",
        "\n",
        "# Now print the vector for 'learning'\n",
        "print(\"Vector for 'learning':\")\n",
        "print(model.wv['learning'])"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31111760-7218-46ed-9e96-1be4e06bfe27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Vector for 'learning':\n",
            "[-0.00537893  0.00240059  0.0510531   0.09015553 -0.09299233 -0.07115052\n",
            "  0.0646439   0.08971389 -0.05020833 -0.03767695]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "# --- Analogy Task ---\n",
        "# Compute the analogy: King - Man + Woman = ?\n",
        "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "\n",
        "print(f\"King - Man + Woman = {result[0][0]}\")\n",
        "\n",
        "# Question: Does the model correctly guess \"Queen\"?\n",
        "# Check if 'queen' is the top result\n",
        "is_queen_correct = (result[0][0].lower() == 'queen')\n",
        "print(f\"Does the model correctly guess \\\"Queen\\\"? {is_queen_correct}\")"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3cdd887-385d-4ab4-9089-5d75d6879bc6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "King - Man + Woman = queen\n",
            "Does the model correctly guess \"Queen\"? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Install vaderSentiment if it's not already there (this is the external library)\n",
        "!pip install vaderSentiment\n",
        "\n",
        "# Using NLTK's SentimentIntensityAnalyzer\n",
        "def sentiment_scores(sentence):\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "\n",
        "    print(f\"Sentiment Scores: {sentiment_dict}\")\n",
        "    print(f\"Negative Sentiment: {sentiment_dict['neg']*100:.2f}%\")\n",
        "    print(f\"Neutral Sentiment: {sentiment_dict['neu']*100:.2f}%\")\n",
        "    print(f\"Positive Sentiment: {sentiment_dict['pos']*100:.2f}%\")\n",
        "\n",
        "    if sentiment_dict['compound'] >= 0.05:\n",
        "        print(\"Overall Sentiment: Positive\")\n",
        "    elif sentiment_dict['compound'] <= -0.05:\n",
        "        print(\"Overall Sentiment: Negative\")\n",
        "    else:\n",
        "        print(\"Overall Sentiment: Neutral\")\n",
        "\n",
        "# Define the corpus for use in the task\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Task 2: Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "print(\"\\n--- Analyzing Pizza Review (corpus[1]) ---\")\n",
        "pizza_review = corpus[1]\n",
        "print(f\"Statement: {pizza_review}\")\n",
        "sentiment_scores(pizza_review)\n",
        "\n",
        "# Task 3: Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "print(\"\\n--- Analyzing Math Complaint (corpus[5]) ---\")\n",
        "math_complaint = corpus[5]\n",
        "print(f\"Statement: {math_complaint}\")\n",
        "sentiment_scores(math_complaint)\n"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5840ff2-d397-483b-c0fb-de8d52da2a92"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "\n",
            "--- Analyzing Pizza Review (corpus[1]) ---\n",
            "Statement: The pizza was absolutely delicious, but the service was terrible ... I won't go back.\n",
            "Sentiment Scores: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "Negative Sentiment: 22.30%\n",
            "Neutral Sentiment: 64.40%\n",
            "Positive Sentiment: 13.40%\n",
            "Overall Sentiment: Negative\n",
            "\n",
            "--- Analyzing Math Complaint (corpus[5]) ---\n",
            "Statement: I love machine learning, but I hate the math behind it.\n",
            "Sentiment Scores: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n",
            "Negative Sentiment: 34.50%\n",
            "Neutral Sentiment: 47.80%\n",
            "Positive Sentiment: 17.70%\n",
            "Overall Sentiment: Negative\n"
          ]
        }
      ]
    }
  ]
}