{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghoshankur102/Judge_It_Well/blob/main/AnkurGhosh_250153_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_scratch(text):\n",
        "  corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "  text=text.lower()            #1\n",
        "  for ch in '!,.,,,:,;,...,''':\n",
        "    text=text.replace(ch,\"\")   #2\n",
        "  words=text.split()           #3\n",
        "  stopwords = [\"is\", \"a\", \"the\", \"and\", \"of\", \"to\", \"in\", \"for\", \"on\", \"with\"]     #4\n",
        "  filteredwords=[word for word in words if word not in stopwords]\n",
        "  def simple_stem(word):\n",
        "    if word.endswith(\"ing\") and len(word) > 4:\n",
        "        return word[:-3]\n",
        "    if word.endswith(\"ly\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "    if word.endswith(\"ed\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "    if word.endswith(\"s\") and len(word) > 2:\n",
        "        return word[:-1]\n",
        "    return word\n",
        "  filteredwords=[simple_stem(word) for word in filteredwords]\n",
        "\n",
        "  print(filteredwords)"
      ],
      "metadata": {
        "id": "YMFQKJ6K5Nsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "clean_text_scratch(corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjdOFi_TDKvl",
        "outputId": "6285bbba-11c9-424f-da16-1a294b252c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_corpus(corpus):\n",
        "    cleaned_corpus = []\n",
        "\n",
        "    for document in corpus:\n",
        "        # Lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(document)\n",
        "\n",
        "        # Remove punctuation, stopwords, and non-alphabetic tokens\n",
        "        tokens = [\n",
        "            word for word in tokens\n",
        "            if word.isalpha() and word not in stop_words\n",
        "        ]\n",
        "\n",
        "        # Lemmatize\n",
        "        lemmatized_tokens = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in tokens\n",
        "        ]\n",
        "\n",
        "        cleaned_corpus.append(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56300777-898b-4a64-fc20-b01a3db98069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "clean_text_scratch(corpus[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHtzkAUdKb-G",
        "outputId": "6aff2341-c69f-41ed-823d-a7add5e34db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pizza', 'wa', 'absolute', 'deliciou', 'but', 'service', 'wa', 'terrible', 'i', \"won't\", 'go', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_corpus(corpus):\n",
        "    cleaned = []\n",
        "    for doc in corpus:\n",
        "        tokens = word_tokenize(doc.lower())\n",
        "        tokens = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in tokens\n",
        "            if word.isalpha() and word not in stop_words\n",
        "        ]\n",
        "        cleaned.append(tokens)\n",
        "    return cleaned\n"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7cb408-b650-4bca-da48-a39afd6981a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(cleaned_corpus):\n",
        "    vocab = set()\n",
        "    for doc in cleaned_corpus:\n",
        "        vocab.update(doc)\n",
        "    return sorted(vocab)"
      ],
      "metadata": {
        "id": "7zPOOv6OMylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sentence(sentence, vocabulary):\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in tokens\n",
        "        if word.isalpha() and word not in stop_words\n",
        "    ]\n",
        "\n",
        "    vector = [tokens.count(word) for word in vocabulary]\n",
        "    return vector"
      ],
      "metadata": {
        "id": "9FA8CV8TM15-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Transforming data is essential for building models.\",\n",
        "    \"Students are learning NLP techniques efficiently.\"\n",
        "]\n",
        "\n",
        "cleaned_corpus = clean_corpus(corpus)\n",
        "\n",
        "vocabulary = build_vocabulary(cleaned_corpus)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(vocabulary)\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "bow_vector = vectorize_sentence(sentence, vocabulary)\n",
        "\n",
        "print(\"\\nBoW Vector for sentence:\")\n",
        "print(bow_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blxdNKsvM6qq",
        "outputId": "1f44b309-5cc0-4ebb-c3fe-dac8eeb08e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['brown', 'building', 'data', 'dog', 'efficiently', 'essential', 'fox', 'jump', 'lazy', 'learning', 'model', 'nlp', 'quick', 'student', 'technique', 'transforming']\n",
            "\n",
            "BoW Vector for sentence:\n",
            "[1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Transforming data is essential for building models.\",\n",
        "    \"Students are learning NLP techniques efficiently.\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X_array = X.toarray()\n",
        "\n",
        "print(\"BoW Matrix:\")\n",
        "print(X_array)\n",
        "print(\"\\nVocabulary:\")\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c8dcd6-fcbf-43dd-830d-83321e931371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Matrix:\n",
            "[[0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 2 0]\n",
            " [0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0]]\n",
            "\n",
            "Vocabulary:\n",
            "['are' 'brown' 'building' 'data' 'dog' 'efficiently' 'essential' 'for'\n",
            " 'fox' 'is' 'jumps' 'lazy' 'learning' 'models' 'nlp' 'over' 'quick'\n",
            " 'students' 'techniques' 'the' 'transforming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "tf = 1 / 10\n",
        "idf = math.log(3 / 1)\n",
        "\n",
        "tf_idf = tf * idf\n",
        "print(tf_idf)"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873cb604-9ace-4e3d-f0ac-eb1ecbd002fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10986122886681099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world\",\n",
        "    \"Intelligence is the ability to learn\",\n",
        "    \"Machine learning is a part of artificial intelligence\"\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "# Convert to array\n",
        "X_array = X.toarray()\n",
        "\n",
        "# Vocabulary for reference\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(vocab)\n",
        "\n",
        "print(\"\\nTF-IDF Vector for first sentence:\")\n",
        "print(X_array[0])     #unique words have higher tf-idf scores"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b43f3d9-b348-40fc-8a0b-734e56170a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['ability' 'artificial' 'intelligence' 'is' 'learn' 'learning' 'machine'\n",
            " 'of' 'part' 'the' 'to' 'transforming' 'world']\n",
            "\n",
            "TF-IDF Vector for first sentence:\n",
            "[0.         0.38737583 0.30083189 0.30083189 0.         0.\n",
            " 0.         0.         0.         0.38737583 0.         0.50935267\n",
            " 0.50935267]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "cleaned_corpus = [\n",
        "    [\"artificial\", \"intelligence\", \"transform\", \"world\"],\n",
        "    [\"intelligence\", \"ability\", \"learn\"],\n",
        "    [\"machine\", \"learning\", \"part\", \"artificial\", \"intelligence\"]\n",
        "]\n",
        "model = Word2Vec(\n",
        "    sentences=cleaned_corpus,\n",
        "    vector_size=10,   # small vector size\n",
        "    min_count=1,      # keep all words\n",
        "    window=3,\n",
        "    workers=1,\n",
        "    seed=42           # for reproducibility\n",
        ")\n",
        "learning_vector = model.wv[\"learning\"]\n",
        "print(\"Vector for 'learning':\")\n",
        "print(learning_vector)"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bedd015-472d-41ac-9b72-7adcf7c39a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Vector for 'learning':\n",
            "[-0.00990809 -0.05455226 -0.08157282  0.01091695  0.07757796 -0.08723656\n",
            "  0.07165825  0.06552622 -0.04464806  0.02633288]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "\n",
        "\n",
        "result = model.most_similar(positive=[\"woman\", \"king\"],negative=[\"man\"],topn=5)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048a33e7-520a-44a0-efab-64b95bb55107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "# rest of the code here\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "corpus = [\n",
        "    \"I love machine learning.\",\n",
        "    \"The pizza was delicious, but the service was terrible.\",\n",
        "    \"Artificial intelligence is fascinating.\",\n",
        "    \"This course is interesting.\",\n",
        "    \"The lecture was okay.\",\n",
        "    \"I hate the math behind machine learning.\"\n",
        "]\n",
        "pizza_review = corpus[1]\n",
        "math_complaint = corpus[5]\n",
        "\n",
        "pizza_scores = sia.polarity_scores(pizza_review)\n",
        "math_scores = sia.polarity_scores(math_complaint)\n",
        "\n",
        "print(\"Pizza Review Scores:\")\n",
        "print(pizza_scores)\n",
        "\n",
        "print(\"\\nMath Complaint Scores:\")\n",
        "print(math_scores)"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06197fcc-377c-49be-aece-e8417150a612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pizza Review Scores:\n",
            "{'neg': 0.307, 'neu': 0.519, 'pos': 0.174, 'compound': -0.4215}\n",
            "\n",
            "Math Complaint Scores:\n",
            "{'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'compound': -0.5719}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}