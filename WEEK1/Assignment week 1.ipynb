{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEaoaCxf0E0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5h2_AzYpzsW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import re\n",
        "#write rest of the code here\n",
        "\n",
        "import re\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "stopwords = {'the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'}\n",
        "\n",
        "# Naive stemmer: remove one of the specified suffixes if present at the end\n",
        "suffixes = ['ing', 'ly', 'ed', 's']\n",
        "\n",
        "def naive_stem(word: str) -> str:\n",
        "    for suf in suffixes:\n",
        "        if word.endswith(suf) and len(word) > len(suf):\n",
        "            return word[:-len(suf)]\n",
        "    return word\n",
        "\n",
        "def clean_text_scratch(text: str):\n",
        "    # 1) Lowercase\n",
        "    text = text.lower()\n",
        "    # 2) Remove punctuation/special chars (keep letters/digits/space)\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # 3) Tokenize\n",
        "    tokens = text.split()\n",
        "    # 4) Stopword removal\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    # 5) Naive stemming\n",
        "    stemmed = [naive_stem(t) for t in tokens]\n",
        "    return stemmed\n",
        "\n",
        "# Run on the first sentence of the corpus and print the result\n",
        "first_cleaned = clean_text_scratch(corpus[0])\n",
        "print(first_cleaned)\n"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d64869-da0d-4432-da8f-234f2751bc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Define a function to clean text using NLTK tools\n",
        "def clean_text_nltk(text):\n",
        "    # Lowercasing and tokenization\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    return lemmas\n",
        "\n",
        "# Apply the function to the second sentence of the corpus\n",
        "# Ensure corpus is defined. Assuming it's in a previous cell.\n",
        "# corpus = [\n",
        "#     \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "#     \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "#     \"The quick brown fox jumps over the lazy dog.\",\n",
        "#     \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "#     \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "#     \"I love machine learning, but I hate the math behind it.\"\n",
        "# ]\n",
        "\n",
        "# Get the second sentence (index 1)\n",
        "second_sentence = corpus[1]\n",
        "\n",
        "# Clean and print the result\n",
        "cleaned_second_sentence = clean_text_nltk(second_sentence)\n",
        "print(cleaned_second_sentence)"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc40b207-759d-4a0e-c0b2-c47f61787352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKr6CUNx0xdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# rest of the code here"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b994ade-5e1a-47fd-84b7-4fe439fd4789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "stopwords = {'the','is','in','to','of','and','a','it','was','but','or'}\n",
        "suffixes = ['ing','ly','ed','s']\n",
        "\n",
        "def naive_stem(word: str) -> str:\n",
        "    for suf in suffixes:\n",
        "        if word.endswith(suf) and len(word) > len(suf):\n",
        "            return word[:-len(suf)]\n",
        "    return word\n",
        "\n",
        "def clean_text_scratch(text: str):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    stemmed = [naive_stem(t) for t in tokens]\n",
        "    return stemmed\n",
        "\n",
        "# 1) Clean corpus\n",
        "cleaned_corpus = [clean_text_scratch(s) for s in corpus]\n",
        "\n",
        "# 2) Build vocabulary (sorted unique tokens)\n",
        "vocab = sorted({tok for doc in cleaned_corpus for tok in doc})\n",
        "\n",
        "# 3) BoW vectorizer\n",
        "from collections import Counter\n",
        "\n",
        "def bow_vectorize(sentence: str, vocab_list):\n",
        "    tokens = clean_text_scratch(sentence)\n",
        "    counts = Counter(tokens)\n",
        "    return [counts[word] for word in vocab_list]\n",
        "\n",
        "# Target sentence\n",
        "target = \"The quick brown fox jumps over the lazy dog.\"\n",
        "bow_vec = bow_vectorize(target, vocab)\n"
      ],
      "metadata": {
        "id": "o-NH82CQ0yky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "#rest of the code here\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Raw corpus\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# 1. Instantiate the vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# 2. Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 3. Convert to array\n",
        "bow_array = X.toarray()\n",
        "\n",
        "# 4. Print vocabulary and BoW matrix\n",
        "print(\"Feature Names (Vocabulary):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_array)\n"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfcf8ad-6d6c-4185-b6cf-868ecef3865a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Vocabulary):\n",
            "['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n",
            " 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n",
            " 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n",
            " 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n",
            " 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n",
            " 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n",
            " 'whether' 'won' 'world']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write code here\n",
        "\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Corpus\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# Basic cleaner: lowercase + remove punctuation, then tokenize by whitespace\n",
        "def tokenize(text: str):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.split()\n",
        "\n",
        "# Target setup\n",
        "target_word = \"machine\"\n",
        "last_sentence = corpus[-1]\n",
        "\n",
        "# ---- TF (Term Frequency) ----\n",
        "tokens_last = tokenize(last_sentence)\n",
        "count_target = sum(1 for t in tokens_last if t == target_word)\n",
        "total_words = len(tokens_last)\n",
        "TF = count_target / total_words if total_words > 0 else 0.0\n",
        "\n",
        "# ---- IDF (Inverse Document Frequency) ----\n",
        "N = len(corpus)\n",
        "docs_with_word = sum(1 for doc in corpus if target_word in tokenize(doc))\n",
        "IDF = math.log(N / docs_with_word) if docs_with_word > 0 else 0.0\n",
        "\n",
        "# ---- TF-IDF ----\n",
        "TF_IDF = TF * IDF\n",
        "\n",
        "# Print manual calculation\n",
        "print(\"Tokens in last sentence:\", tokens_last)\n",
        "print(f\"TF('{target_word}') = {count_target} / {total_words} = {TF}\")\n",
        "print(f\"IDF('{target_word}') = log({N} / {docs_with_word}) = {IDF}\")\n",
        "print(f\"TF-IDF('{target_word}') = {TF} * {IDF} = {TF_IDF}\")\n"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7ce67d-b10c-4c9f-869c-eb4c848c6e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in last sentence: ['i', 'love', 'machine', 'learning', 'but', 'i', 'hate', 'the', 'math', 'behind', 'it']\n",
            "TF('machine') = 1 / 11 = 0.09090909090909091\n",
            "IDF('machine') = log(6 / 2) = 1.0986122886681098\n",
            "TF-IDF('machine') = 0.09090909090909091 * 1.0986122886681098 = 0.09987384442437362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# rest of the code here\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# 1) Instantiate the TF-IDF vectorizer and fit on the corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 2) Extract the vector for the first sentence\n",
        "first_vec = X[0].toarray().flatten()\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 3) Print the non-zero terms and their TF-IDF scores for readability\n",
        "nonzero_indices = np.where(first_vec > 0)[0]\n",
        "terms_scores = sorted(\n",
        "    [(feature_names[i], first_vec[i]) for i in nonzero_indices],\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "print(\"TF-IDF vector (non-zero terms) for the first sentence:\")\n",
        "for term, score in terms_scores:\n",
        "    print(f\"{term}: {score:.4f}\")\n",
        "\n",
        "# Optional: direct comparison for 'intelligence' vs 'is'\n",
        "def score_of(term):\n",
        "    try:\n",
        "        idx = list(feature_names).index(term)\n",
        "        return first_vec[idx]\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "print(\"\\nScores in the first sentence:\")\n",
        "print(\"intelligence:\", score_of(\"intelligence\"))\n",
        "print(\"is:\", score_of(\"is\"))\n"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d1d193-03ee-418e-9755-fec8432e0e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vector (non-zero terms) for the first sentence:\n",
            "artificial: 0.3345\n",
            "concerns: 0.3345\n",
            "ethical: 0.3345\n",
            "however: 0.3345\n",
            "intelligence: 0.3345\n",
            "remain: 0.3345\n",
            "transforming: 0.3345\n",
            "world: 0.3345\n",
            "is: 0.2743\n",
            "the: 0.1714\n",
            "\n",
            "Scores in the first sentence:\n",
            "intelligence: 0.3345454287016015\n",
            "is: 0.27433203727401334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#rest of the code here\n",
        "\n",
        "# If needed:\n",
        "# !pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "# ---------- Part 1.2: Cleaner (reused) ----------\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "stopwords = {'the','is','in','to','of','and','a','it','was','but','or'}\n",
        "suffixes = ['ing','ly','ed','s']  # naive stemmer suffixes\n",
        "\n",
        "def naive_stem(word: str) -> str:\n",
        "    for suf in suffixes:\n",
        "        if word.endswith(suf) and len(word) > len(suf):\n",
        "            return word[:-len(suf)]\n",
        "    return word\n",
        "\n",
        "def clean_text_scratch(text: str):\n",
        "    # 1) Lowercase\n",
        "    text = text.lower()\n",
        "    # 2) Remove punctuation / special characters\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # 3) Tokenize\n",
        "    tokens = text.split()\n",
        "    # 4) Stopword removal\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    # 5) Naive stemming\n",
        "    tokens = [naive_stem(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Cleaned, tokenized corpus (list of lists of tokens)\n",
        "cleaned_corpus_tokens = [clean_text_scratch(doc) for doc in corpus]\n",
        "\n",
        "# ---------- Train Word2Vec ----------\n",
        "SEED = 42  # for reproducibility\n",
        "model = Word2Vec(\n",
        "    sentences=cleaned_corpus_tokens,\n",
        "    vector_size=10,   # small vector size for easy viewing\n",
        "    window=5,\n",
        "    min_count=1,      # keep all words\n",
        "    sg=0,             # CBOW (use sg=1 for Skip-gram)\n",
        "    workers=1,\n",
        "    epochs=50,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# ---------- Experiment: print vector for \"learning\" ----------\n",
        "target = \"learning\"\n",
        "fallback = \"learn\"  # due to naive stemmer\n",
        "\n",
        "if target in model.wv:\n",
        "    vec = model.wv[target]\n",
        "    print(f\"Vector for '{target}' (length {len(vec)}):\\n{vec}\")\n",
        "elif fallback in model.wv:\n",
        "    vec = model.wv[fallback]\n",
        "    print(f\"'learning' not found due to stemming; showing vector for '{fallback}' (length {len(vec)}):\\n{vec}\")\n",
        "else:\n",
        "    print(\"Neither 'learning' nor 'learn' found in the model vocabulary. Try retraining without stemming.\")\n"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8461cf2d-a4f9-4fc9-9525-8f1913586703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "'learning' not found due to stemming; showing vector for 'learn' (length 10):\n",
            "[ 0.00579272  0.09525505  0.04697919  0.05290301  0.04492708  0.05687171\n",
            "  0.0019812  -0.07445481  0.0668371  -0.01057429]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "#glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "#rest of the code here\n",
        "\n",
        "# If needed:\n",
        "# !pip install gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "# 1) Load pre-trained GloVe model (50-dimensional)\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# 2) Analogy: King - Man + Woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "\n",
        "# 3) Print top results\n",
        "print(\"Analogy: King - Man + Woman = ?\")\n",
        "for word, score in result:\n",
        "    print(f\"{word}: {score:.4f}\")\n",
        "\n",
        "# Check if 'queen' is the top guess\n",
        "print(\"\\nTop guess:\", result[0][0])\n"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8669921d-9353-4b38-9c7c-3cd560dc04a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Analogy: King - Man + Woman = ?\n",
            "queen: 0.8524\n",
            "throne: 0.7664\n",
            "prince: 0.7592\n",
            "daughter: 0.7474\n",
            "elizabeth: 0.7460\n",
            "princess: 0.7425\n",
            "kingdom: 0.7337\n",
            "monarch: 0.7214\n",
            "eldest: 0.7185\n",
            "widow: 0.7099\n",
            "\n",
            "Top guess: queen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "# rest of the code here\n",
        "\n",
        "# If needed (first time only):\n",
        "# import nltk\n",
        "# nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# 1) Initialize analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 2) Inputs\n",
        "pizza_review = corpus[1]\n",
        "math_complaint = corpus[5]\n",
        "\n",
        "# 3) Get polarity scores\n",
        "pizza_scores = sia.polarity_scores(pizza_review)\n",
        "math_scores = sia.polarity_scores(math_complaint)\n",
        "\n",
        "# 4) Print detailed results and compound scores\n",
        "print(\"Pizza Review:\", pizza_review)\n",
        "print(\"Pizza Scores:\", pizza_scores)\n",
        "print(\"Pizza Compound:\", pizza_scores['compound'])\n",
        "\n",
        "print(\"\\nMath Complaint:\", math_complaint)\n",
        "print(\"Math Scores:\", math_scores)\n",
        "print(\"Math Compound:\", math_scores['compound'])\n"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02574d8e-e31b-431f-8c50-1961bcaf9a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pizza Review: The pizza was absolutely delicious, but the service was terrible ... I won't go back.\n",
            "Pizza Scores: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "Pizza Compound: -0.3926\n",
            "\n",
            "Math Complaint: I love machine learning, but I hate the math behind it.\n",
            "Math Scores: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n",
            "Math Compound: -0.5346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5264239"
      },
      "source": [
        "# Task\n",
        "Define a simple stemming helper function `simple_stem(word)` to remove 'ing', 'ly', 'ed', and 's' suffixes. Implement `clean_text_scratch(text)` to perform lowercasing, punctuation removal using `re`, tokenize by whitespace, remove specified stopwords ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'], and apply the `simple_stem` helper to each token. Apply `clean_text_scratch` to the first sentence of the `corpus` and print the resulting cleaned text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ff2d372"
      },
      "source": [
        "## Define Simple Stemming Helper\n",
        "\n",
        "### Subtask:\n",
        "Create a helper function, `simple_stem(word)`, that removes 'ing', 'ly', 'ed', and 's' suffixes from a word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a792cb2"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `simple_stem` function according to the instructions, which involves checking for specific suffixes ('ing', 'ly', 'ed', 's') and removing them in a specific order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f809f5d2"
      },
      "source": [
        "import re\n",
        "#write rest of the code here\n",
        "\n",
        "def simple_stem(word):\n",
        "    if word.endswith('ing'):\n",
        "        return word[:-3]\n",
        "    elif word.endswith('ly'):\n",
        "        return word[:-2]\n",
        "    elif word.endswith('ed'):\n",
        "        return word[:-2]\n",
        "    elif word.endswith('s'):\n",
        "        return word[:-1]\n",
        "    return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "# Given corpus\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# Stopwords per instructions\n",
        "STOPWORDS = {'the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'}\n",
        "\n",
        "# --- Simple Stemming Helper ---\n",
        "def simple_stem(word: str) -> str:\n",
        "    \"\"\"\n",
        "    Naively remove one of the suffixes 'ing', 'ly', 'ed', or 's' (in this order)\n",
        "    if the word ends with it and the remaining stem is non-empty.\n",
        "    \"\"\"\n",
        "    for suf in ('ing', 'ly', 'ed', 's'):\n",
        "        if word.endswith(suf) and len(word) > len(suf):\n",
        "            return word[:-len(suf)]\n",
        "    return word\n",
        "\n",
        "# --- Cleaning Function ---\n",
        "def clean_text_scratch(text: str):\n",
        "    \"\"\"\n",
        "    1) Lowercase\n",
        "    2) Remove punctuation/special characters using regex (keep a-z, 0-9, whitespace)\n",
        "    3) Tokenize by whitespace\n",
        "    4) Remove specified stopwords\n",
        "    5) Apply naive simple_stem to each token\n",
        "    \"\"\"\n",
        "    # 1) Lowercase\n",
        "    text = text.lower()\n",
        "    # 2) Remove punctuation/special chars\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # 3) Tokenize\n",
        "    tokens = text.split()\n",
        "    # 4) Stopword removal\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "    # 5) Simple stemming\n",
        "    tokens = [simple_stem(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "# --- Apply to the first sentence and print ---\n",
        "first_cleaned = clean_text_scratch(corpus[0])\n",
        "print(first_cleaned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzDFno2piKEF",
        "outputId": "858a2bf1-34eb-4722-d9ba-b5d705ee0468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49c35795"
      },
      "source": [
        "## Define clean_text_scratch Function\n",
        "\n",
        "### Subtask:\n",
        "Write the `clean_text_scratch(text)` function. This function will perform lowercasing, punctuation removal using `re`, tokenization by splitting on whitespace, stopword removal using the provided list ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'], and apply the `simple_stem` helper function to each word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a51cb2b"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `clean_text_scratch` function that performs lowercasing, punctuation removal, tokenization, stopword removal, and applies the `simple_stem` function as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c17fddd"
      },
      "source": [
        "def clean_text_scratch(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Punctuation Removal\n",
        "    text = re.sub(r'[!.,:;...?\\\\]', '', text)\n",
        "\n",
        "    # 3. Tokenization\n",
        "    words = text.split()\n",
        "\n",
        "    # 4. Stopword Removal\n",
        "    stopwords = ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "    # 5. Simple Stemming\n",
        "    stemmed_words = [simple_stem(word) for word in filtered_words]\n",
        "\n",
        "    return stemmed_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "769d551e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `clean_text_scratch` function has been defined, so the next logical step is to apply it to the first sentence of the corpus and print the result as requested by the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "035db4d4",
        "outputId": "ada59eba-0a63-4543-fe58-300b636996c1"
      },
      "source": [
        "cleaned_first_sentence = clean_text_scratch(corpus[0])\n",
        "print(cleaned_first_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "297f6c5d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Review the output of the `clean_text_scratch` function for the first sentence of the corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "753428a9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The output of the `clean_text_scratch` function for the first sentence of the corpus is `['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']`.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A helper function `simple_stem` was successfully created to remove 'ing', 'ly', 'ed', and 's' suffixes, returning the original word if no specified suffix was found.\n",
        "*   The `clean_text_scratch` function was implemented to perform several text preprocessing steps: lowercasing, punctuation removal, tokenization by whitespace, removal of specified stopwords (e.g., 'the', 'is', 'in'), and application of the `simple_stem` helper to each token.\n",
        "*   Applying `clean_text_scratch` to the first sentence of the corpus resulted in the cleaned and stemmed token list: `['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The established `clean_text_scratch` function provides a foundational text preprocessing pipeline that can be directly applied to larger text datasets for various natural language processing tasks.\n",
        "*   For more sophisticated text analysis, consider expanding the stemming logic to include a broader set of suffixes or integrating a more robust stemming or lemmatization library (e.g., NLTK's Porter Stemmer or WordNet Lemmatizer) for enhanced linguistic accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0571c1f7"
      },
      "source": [
        "# Task\n",
        "Clean the entire `corpus` using the `clean_text_nltk` function to create a `cleaned_corpus`. From this `cleaned_corpus`, extract all unique words, sort them alphabetically, and store them in a list called `vocabulary`. Define a function `get_bow_vector(sentence, vocabulary)` that cleans the input `sentence` using `clean_text_nltk` and then generates a Bag of Words vector by counting the occurrences of each word from the `vocabulary` in the cleaned sentence. Finally, print the `vocabulary` and the Bag of Words vector for the sentence \"The quick brown fox jumps over the lazy dog.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bd85f0c"
      },
      "source": [
        "## Clean Corpus for Vocabulary\n",
        "\n",
        "### Subtask:\n",
        "Apply the `clean_text_nltk` function (defined in Part 1.2) to each sentence in the `corpus` to create a cleaned and lemmatized version of the entire corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aee7b1c"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to apply the `clean_text_nltk` function to each sentence in the `corpus` and store the results in a new list named `cleaned_corpus`, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e18a261b",
        "outputId": "6dacec95-89bb-42a6-e25a-f4a73401d6eb"
      },
      "source": [
        "cleaned_corpus = []\n",
        "for sentence in corpus:\n",
        "    cleaned_corpus.append(clean_text_nltk(sentence))\n",
        "\n",
        "print(cleaned_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concern', 'remain'], ['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back'], ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog'], ['question', 'whether', 'nobler', 'mind'], ['data', 'science', 'involves', 'statistic', 'linear', 'algebra', 'machine', 'learning'], ['love', 'machine', 'learning', 'hate', 'math', 'behind']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83b3d04b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the corpus is cleaned, I need to extract all unique words from `cleaned_corpus`, sort them alphabetically, and store them in a list called `vocabulary` as per the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74b2e28f"
      },
      "source": [
        "# Task\n",
        "Instantiate `CountVectorizer` from `sklearn.feature_extraction.text`, fit and transform the raw `corpus` using this vectorizer, and then print the resulting Bag of Words matrix as a dense array, along with the feature names (vocabulary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c9f2d4f"
      },
      "source": [
        "## Instantiate CountVectorizer\n",
        "\n",
        "### Subtask:\n",
        "Import and instantiate `CountVectorizer` from `sklearn.feature_extraction.text`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532d5bc3"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires instantiating `CountVectorizer`. I will write a code block to import `CountVectorizer` and create an instance named `vectorizer`.\n",
        "\n"
      ]
    }
  ]
}