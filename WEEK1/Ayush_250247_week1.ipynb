{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def simple_stem(word):\n",
        "    # Remove suffixes 'ing', 'ly', 'ed', 's'\n",
        "    if word.endswith('ing'):\n",
        "        word = word[:-3]\n",
        "    elif word.endswith('ly'):\n",
        "        word = word[:-2]\n",
        "    elif word.endswith('ed'):\n",
        "        word = word[:-2]\n",
        "    elif word.endswith('s'):\n",
        "        word = word[:-1]\n",
        "    return word\n",
        "\n",
        "def clean_text_scratch(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "    # 2. Punctuation Removal\n",
        "    # First, handle the literal '...' sequence\n",
        "    text = text.replace('...', '')\n",
        "    # Then, remove other single character punctuation (!, ., ,, :, ;, ', [, ])\n",
        "    # Note: Apostrophe is also handled here, and square brackets are correctly escaped for regex.\n",
        "    text = re.sub(r'[!.,:;\\'\\['']', '', text)\n",
        "    # 3. Tokenization\n",
        "    words = text.split()\n",
        "    # 4. Stopword Removal\n",
        "    stopwords = ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    # 5. Simple Stemming\n",
        "    words = [simple_stem(word) for word in words]\n",
        "    return words\n",
        "\n",
        "# Task: Run this function on the first sentence of the corpus and print the result.\n",
        "cleaned_first_sentence_scratch = clean_text_scratch(corpus[0])\n",
        "print(cleaned_first_sentence_scratch)"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a03f5d2-fd70-483d-bf32-ff2d3bcfd250"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text_nltk(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "    # 2. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # 3. Punctuation Removal and Stopword Removal and Lemmatization\n",
        "    cleaned_tokens = []\n",
        "    for word in tokens:\n",
        "        # Remove punctuation by checking if the word is alphabetic\n",
        "        if word.isalpha():\n",
        "            if word not in stop_words:\n",
        "                cleaned_tokens.append(lemmatizer.lemmatize(word))\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Apply the cleaning function to the entire corpus\n",
        "cleaned_corpus_nltk = [clean_text_nltk(sentence) for sentence in corpus]\n",
        "\n",
        "# Print the cleaned, lemmatized tokens for the second sentence (corpus[1])\n",
        "print(cleaned_corpus_nltk[1])"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b448f01-2c84-4de7-fa90-1a9d1bc9a733"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# rest of the code here\n",
        "# 1. Build Vocabulary\n",
        "all_words = []\n",
        "for sentence_tokens in cleaned_corpus_nltk:\n",
        "    all_words.extend(sentence_tokens)\n",
        "\n",
        "vocabulary = sorted(list(set(all_words)))\n",
        "\n",
        "# Print the unique Vocabulary list\n",
        "print(\"Unique Vocabulary:\", vocabulary)\n",
        "\n",
        "# 2. Vectorize function\n",
        "def vectorize_bow(sentence, vocab, cleaning_function):\n",
        "    # Clean the input sentence using the NLTK cleaning function\n",
        "    cleaned_sentence_tokens = cleaning_function(sentence)\n",
        "\n",
        "    # Initialize vector with zeros\n",
        "    vector = [0] * len(vocab)\n",
        "\n",
        "    # Populate vector with word counts\n",
        "    for word in cleaned_sentence_tokens:\n",
        "        if word in vocab:\n",
        "            vector[vocab.index(word)] += 1\n",
        "    return vector\n",
        "\n",
        "# Task: Print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence_to_vectorize = \"The quick brown fox jumps over the lazy dog.\"\n",
        "bow_vector_for_sentence = vectorize_bow(sentence_to_vectorize, vocabulary, clean_text_nltk)\n",
        "\n",
        "print(f\"\\nBoW Vector for '{sentence_to_vectorize}':\", bow_vector_for_sentence)"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae6716f-4d9a-4def-f602-bc4f860d7812"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Vocabulary: ['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jump', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'transforming', 'whether', 'wo', 'world']\n",
            "\n",
            "BoW Vector for 'The quick brown fox jumps over the lazy dog.': [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. Instantiate the vectorizer.\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# 2. fit_transform the raw corpus.\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 3. Convert the result to an array (.toarray()) and print it.\n",
        "print(\"BoW Matrix (CountVectorizer):\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e096cd0-0b4d-416e-dabb-d973bc03f53c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Matrix (CountVectorizer):\n",
            "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# The last sentence (corpus[5]) after NLTK cleaning and lemmatization\n",
        "last_sentence_cleaned = cleaned_corpus_nltk[5]\n",
        "\n",
        "# 1. Calculate TF (Term Frequency) for 'machine' in the last sentence\n",
        "word_to_analyze = 'machine'\n",
        "\n",
        "count_machine_in_sentence = last_sentence_cleaned.count(word_to_analyze)\n",
        "total_words_in_last_sentence = len(last_sentence_cleaned)\n",
        "\n",
        "tf = count_machine_in_sentence / total_words_in_last_sentence\n",
        "\n",
        "print(f\"TF for '{word_to_analyze}' in the last sentence: {tf:.4f}\")\n",
        "\n",
        "# 2. Calculate IDF (Inverse Document Frequency) for 'machine' across the entire corpus\n",
        "total_documents = len(cleaned_corpus_nltk)\n",
        "documents_containing_machine = 0\n",
        "\n",
        "for doc in cleaned_corpus_nltk:\n",
        "    if word_to_analyze in doc:\n",
        "        documents_containing_machine += 1\n",
        "\n",
        "# Add a small epsilon to avoid division by zero if word not found in any document\n",
        "# (though in this case 'machine' is present)\n",
        "idf = math.log(total_documents / (documents_containing_machine + 1e-6))\n",
        "\n",
        "print(f\"IDF for '{word_to_analyze}' across the corpus: {idf:.4f}\")\n",
        "\n",
        "# 3. Calculate TF-IDF\n",
        "tfidf_score = tf * idf\n",
        "\n",
        "print(f\"\\nManual TF-IDF score for '{word_to_analyze}' in the last sentence: {tfidf_score:.4f}\")"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d7e4e2-1c52-42b6-a32d-5fc25c5fee7f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF for 'machine' in the last sentence: 0.1667\n",
            "IDF for 'machine' across the corpus: 1.0986\n",
            "\n",
            "Manual TF-IDF score for 'machine' in the last sentence: 0.1831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 2. Fit and transform the raw corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 3. Print the TF-IDF vector for the first sentence\n",
        "print(\"TF-IDF Vector for the first sentence (corpus[0]):\")\n",
        "print(tfidf_matrix[0].toarray())\n",
        "\n",
        "# Observation: Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\").\n",
        "# Get feature names (vocabulary)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the TF-IDF vector for the first sentence as a dense array\n",
        "first_sentence_vector = tfidf_matrix[0].toarray()[0]\n",
        "\n",
        "# Find the index and score for 'intelligence'\n",
        "try:\n",
        "    idx_intelligence = list(feature_names).index('intelligence')\n",
        "    score_intelligence = first_sentence_vector[idx_intelligence]\n",
        "    print(f\"\\nTF-IDF score for 'intelligence': {score_intelligence:.4f}\")\n",
        "except ValueError:\n",
        "    print(\"\\n'intelligence' not found in vocabulary for TF-IDF.\")\n",
        "\n",
        "# Find the index and score for 'is'\n",
        "try:\n",
        "    idx_is = list(feature_names).index('is')\n",
        "    score_is = first_sentence_vector[idx_is]\n",
        "    print(f\"TF-IDF score for 'is': {score_is:.4f}\")\n",
        "except ValueError:\n",
        "    print(\"\\n'is' not found in vocabulary for TF-IDF.\")"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e210680d-7828-4ab8-c59c-947df0e99a11"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vector for the first sentence (corpus[0]):\n",
            "[[0.         0.         0.         0.33454543 0.         0.\n",
            "  0.         0.         0.         0.33454543 0.         0.\n",
            "  0.         0.33454543 0.         0.         0.         0.33454543\n",
            "  0.         0.33454543 0.         0.27433204 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.33454543 0.         0.         0.\n",
            "  0.         0.         0.17139656 0.         0.         0.33454543\n",
            "  0.         0.         0.         0.33454543]]\n",
            "\n",
            "TF-IDF score for 'intelligence': 0.3345\n",
            "TF-IDF score for 'is': 0.2743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Train Word2Vec model\n",
        "# Pass the cleaned tokenized corpus (from Part 1.2) to Word2Vec\n",
        "# cleaned_corpus_nltk was generated in cell v_4FjuCqy5Kt\n",
        "model = Word2Vec(sentences=cleaned_corpus_nltk, min_count=1, vector_size=10)\n",
        "\n",
        "# Print the vector for the word \"learning\"\n",
        "word_to_experiment = \"learning\"\n",
        "if word_to_experiment in model.wv:\n",
        "    print(f\"Vector for '{word_to_experiment}':\")\n",
        "    print(model.wv[word_to_experiment])\n",
        "else:\n",
        "    print(f\"'{word_to_experiment}' not found in the vocabulary.\")"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7500e0-db2f-4e62-da01-c51bf126f6d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Vector for 'learning':\n",
            "[-0.00535678  0.00238785  0.05107836  0.09016657 -0.09301379 -0.07113771\n",
            "  0.06464887  0.08973394 -0.05023384 -0.03767424]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "# Analogy Task: Compute King - Man + Woman = ?\n",
        "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "\n",
        "print(\"King - Man + Woman = ?\")\n",
        "for word, score in result:\n",
        "    print(f\"{word}: {score:.4f}\")\n",
        "\n",
        "# Question: Does the model correctly guess \"Queen\"?\n",
        "is_queen_guessed = any(word == 'queen' for word, score in result)\n",
        "if is_queen_guessed:\n",
        "    print(\"\\nYes, the model correctly guesses 'queen' (or a close variant) in the top results.\")\n",
        "else:\n",
        "    print(\"\\nNo, the model did not correctly guess 'queen' in the top results.\")"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc6740c9-9389-4c20-87ec-52221042c8e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "King - Man + Woman = ?\n",
            "queen: 0.8524\n",
            "throne: 0.7664\n",
            "prince: 0.7592\n",
            "daughter: 0.7474\n",
            "elizabeth: 0.7460\n",
            "princess: 0.7425\n",
            "kingdom: 0.7337\n",
            "monarch: 0.7214\n",
            "eldest: 0.7185\n",
            "widow: 0.7099\n",
            "\n",
            "Yes, the model correctly guesses 'queen' (or a close variant) in the top results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# 1. Initialize the SentimentIntensityAnalyzer.\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "pizza_review = corpus[1]\n",
        "pizza_sentiment = sia.polarity_scores(pizza_review)\n",
        "print(f\"\\nPizza Review: '{pizza_review}'\")\n",
        "print(f\"Sentiment: {pizza_sentiment}\")\n",
        "\n",
        "# 3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "math_complaint = corpus[5]\n",
        "math_sentiment = sia.polarity_scores(math_complaint)\n",
        "print(f\"\\nMath Complaint: '{math_complaint}'\")\n",
        "print(f\"Sentiment: {math_sentiment}\")\n",
        "\n",
        "# Analysis: Look at the compound score for both.\n",
        "# Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19bcbd8-2eba-4ab5-db00-071fea5355de"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pizza Review: 'The pizza was absolutely delicious, but the service was terrible ... I won't go back.'\n",
            "Sentiment: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "\n",
            "Math Complaint: 'I love machine learning, but I hate the math behind it.'\n",
            "Sentiment: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}