{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text_scratch(texts):\n",
        "\n",
        "    lower_texts = []\n",
        "    for text in texts:\n",
        "        text = text.lower()\n",
        "        lower_texts.append(text)\n",
        "\n",
        "    low_punc_texts = []\n",
        "    for text in lower_texts:\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "        low_punc_texts.append(text)\n",
        "\n",
        "    words = []\n",
        "    for text in low_punc_texts:\n",
        "        tokens = text.split()\n",
        "        for token in tokens:\n",
        "            words.append(token)\n",
        "\n",
        "    words_to_remove = ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "    filtered_words = [w for w in words if w not in words_to_remove]\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "def stemming(words, suffixes):\n",
        "    stemmed_words = []\n",
        "    for word in words:\n",
        "        stemmed = False\n",
        "        for suff in suffixes:\n",
        "            if word.endswith(suff):\n",
        "                stemmed_words.append(word[:-len(suff)])\n",
        "                stemmed = True\n",
        "                break\n",
        "        if not stemmed:\n",
        "            stemmed_words.append(word)\n",
        "    return stemmed_words\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "suffixes = ['ing', 'ly', 'ed', 's']\n",
        "\n",
        "fit_words = clean_text_scratch(corpus)\n",
        "print(stemming(fit_words, suffixes))\n"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "outputId": "9a30c3d4-cd25-4923-facc-2f0662836525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain', 'pizza', 'absolute', 'deliciou', 'service', 'terrible', 'i', 'wont', 'go', 'back', 'quick', 'brown', 'fox', 'jump', 'over', 'lazy', 'dog', 'be', 'not', 'be', 'that', 'question', 'whether', 'ti', 'nobler', 'mind', 'data', 'science', 'involve', 'statistic', 'linear', 'algebra', 'machine', 'learn', 'i', 'love', 'machine', 'learn', 'i', 'hate', 'math', 'behind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "# Tokenize + lowercase\n",
        "words = []\n",
        "for sent in corpus:\n",
        "    tokens = word_tokenize(sent)\n",
        "    words.extend([token.lower() for token in tokens])\n",
        "\n",
        "# Remove stopwords & punctuation\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "fit_words = [w for w in words if w.isalpha() and w not in stop_words]\n",
        "\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# POS tagging\n",
        "tagged_words = pos_tag(fit_words)\n",
        "\n",
        "lemmatized_words = []\n",
        "\n",
        "print(\"Original filtered words:\", fit_words)\n",
        "\n",
        "print(\"\\nLemmatized words with POS:\")\n",
        "for word, tag in tagged_words:\n",
        "    wn_pos = get_wordnet_pos(tag)\n",
        "    lemma = lemmatizer.lemmatize(word, wn_pos)\n",
        "    lemmatized_words.append(lemma)\n",
        "    print(f\"{word} ({wn_pos}) -> {lemma}\")\n",
        "\n",
        "print(\"\\nFinal lemmatized corpus:\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff9f1b9-b330-4b11-bc8b-a598e8c90069",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original filtered words: ['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concerns', 'remain', 'pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'question', 'whether', 'nobler', 'mind', 'data', 'science', 'involves', 'statistics', 'linear', 'algebra', 'machine', 'learning', 'love', 'machine', 'learning', 'hate', 'math', 'behind']\n",
            "\n",
            "Lemmatized words with POS:\n",
            "artificial (a) -> artificial\n",
            "intelligence (n) -> intelligence\n",
            "transforming (v) -> transform\n",
            "world (n) -> world\n",
            "however (r) -> however\n",
            "ethical (a) -> ethical\n",
            "concerns (n) -> concern\n",
            "remain (v) -> remain\n",
            "pizza (a) -> pizza\n",
            "absolutely (r) -> absolutely\n",
            "delicious (a) -> delicious\n",
            "service (n) -> service\n",
            "terrible (a) -> terrible\n",
            "wo (n) -> wo\n",
            "go (v) -> go\n",
            "back (r) -> back\n",
            "quick (a) -> quick\n",
            "brown (n) -> brown\n",
            "fox (n) -> fox\n",
            "jumps (n) -> jump\n",
            "lazy (a) -> lazy\n",
            "dog (a) -> dog\n",
            "question (n) -> question\n",
            "whether (n) -> whether\n",
            "nobler (n) -> nobler\n",
            "mind (n) -> mind\n",
            "data (n) -> data\n",
            "science (n) -> science\n",
            "involves (v) -> involve\n",
            "statistics (n) -> statistic\n",
            "linear (a) -> linear\n",
            "algebra (a) -> algebra\n",
            "machine (n) -> machine\n",
            "learning (v) -> learn\n",
            "love (a) -> love\n",
            "machine (n) -> machine\n",
            "learning (v) -> learn\n",
            "hate (a) -> hate\n",
            "math (n) -> math\n",
            "behind (n) -> behind\n",
            "\n",
            "Final lemmatized corpus: ['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain', 'pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'question', 'whether', 'nobler', 'mind', 'data', 'science', 'involve', 'statistic', 'linear', 'algebra', 'machine', 'learn', 'love', 'machine', 'learn', 'hate', 'math', 'behind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# NLTK downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Building Vocabulary\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "\n",
        "all_words = []\n",
        "for sent in corpus:\n",
        "    all_words.extend(preprocess(sent))\n",
        "\n",
        "# Unique sorted vocabulary\n",
        "vocabulary = sorted(list(set(all_words)))\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "#  BoW vectorizer\n",
        "def bow_vector(sentence, vocab):\n",
        "    tokens = preprocess(sentence)\n",
        "    return [tokens.count(word) for word in vocab]\n",
        "\n",
        "test_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "bow = bow_vector(test_sentence, vocabulary)\n",
        "\n",
        "print(bow)\n"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8699840-918d-4af3-a6b4-963a53a1c16a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concerns', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jumps', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistics', 'terrible', 'transforming', 'whether', 'wo', 'world']\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit + transform corpus into BoW matrix\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_array)\n"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6c0c1f-3643-46a9-f053-0e92fd497e7f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n",
            " 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n",
            " 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n",
            " 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n",
            " 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n",
            " 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n",
            " 'whether' 'won' 'world']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "target = \"machine\"\n",
        "\n",
        "sentence = corpus[-1]\n",
        "\n",
        "# TF Calculation\n",
        "tokens = [w.lower() for w in sentence.split()]\n",
        "tf = tokens.count(target) / len(tokens)\n",
        "# IDF Calculation\n",
        "total_docs = len(corpus)\n",
        "docs_with_word = 0\n",
        "\n",
        "for doc in corpus:\n",
        "    if target in doc.lower().split():\n",
        "        docs_with_word += 1\n",
        "\n",
        "idf = math.log(total_docs / docs_with_word)\n",
        "\n",
        "#  TF-IDF\n",
        "tfidf = tf * idf\n",
        "\n",
        "print(\"TF:\", tf)\n",
        "print(\"IDF:\", idf)\n",
        "print(\"TF-IDF:\", tfidf)\n"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbdf5de-8a6f-4704-ea4f-00e3e1562c64"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 0.09090909090909091\n",
            "IDF: 1.0986122886681098\n",
            "TF-IDF: 0.09987384442437362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "#  Fit + transform\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "first_vector = tfidf_matrix[0].toarray()[0]\n",
        "\n",
        "print(\"Vocabulary list:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF vector for FIRST sentence:\")\n",
        "print(first_vector)\n",
        "\n",
        "print(\"\\nObservation:\")\n",
        "print('Unique words like \"intelligence\" have HIGHER TF-IDF,')\n",
        "print('Common words like \"is\", \"the\", \"in\" have LOWER TF-IDF.')\n"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c1df83-70ff-43b9-85dd-fed7dec9e4fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary list:\n",
            "['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n",
            " 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n",
            " 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n",
            " 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n",
            " 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n",
            " 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n",
            " 'whether' 'won' 'world']\n",
            "\n",
            "TF-IDF vector for FIRST sentence:\n",
            "[0.         0.         0.         0.33454543 0.         0.\n",
            " 0.         0.         0.         0.33454543 0.         0.\n",
            " 0.         0.33454543 0.         0.         0.         0.33454543\n",
            " 0.         0.33454543 0.         0.27433204 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.33454543 0.         0.         0.\n",
            " 0.         0.         0.17139656 0.         0.         0.33454543\n",
            " 0.         0.         0.         0.33454543]\n",
            "\n",
            "Observation:\n",
            "Unique words like \"intelligence\" have HIGHER TF-IDF,\n",
            "Common words like \"is\", \"the\", \"in\" have LOWER TF-IDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "cleaned_corpus = [clean_and_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "print(\"Cleaned Tokenized Corpus:\")\n",
        "print(cleaned_corpus)\n",
        "\n",
        "#  TRAIN WORD2VEC MODEL\n",
        "model = Word2Vec(\n",
        "    sentences=cleaned_corpus,\n",
        "    vector_size=10,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "print(\"\\nVector for word 'learning':\")\n",
        "print(model.wv[\"learning\"])\n"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fbcea7-e882-471a-e22c-7dc3a1d4a743"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Cleaned Tokenized Corpus:\n",
            "[['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concerns', 'remain'], ['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back'], ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog'], ['question', 'whether', 'nobler', 'mind'], ['data', 'science', 'involves', 'statistics', 'linear', 'algebra', 'machine', 'learning'], ['love', 'machine', 'learning', 'hate', 'math', 'behind']]\n",
            "\n",
            "Vector for word 'learning':\n",
            "[-0.00535678  0.00238785  0.05107836  0.09016657 -0.09301379 -0.07113771\n",
            "  0.06464887  0.08973394 -0.05023384 -0.03767424]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "result = glove_model.most_similar(\n",
        "    positive=['king', 'woman'],\n",
        "    negative=['man']\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a7a726-c162-4c9e-f47c-a43eea91037f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337412238121033), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "pizza_review = corpus[1]\n",
        "math_complaint = corpus[5]\n",
        "\n",
        "pizza_score = sia.polarity_scores(pizza_review)\n",
        "math_score = sia.polarity_scores(math_complaint)\n",
        "\n",
        "print(\"Pizza Review:\", pizza_review)\n",
        "print(\"Sentiment:\", pizza_score)\n",
        "\n",
        "print(\"\\nMath Complaint:\", math_complaint)\n",
        "print(\"Sentiment:\", math_score)\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"Compound Score Range = -1 (negative) to +1 (positive).\")\n",
        "print(\"Notice the pizza review has both 'delicious' (positive) and 'terrible' (negative).\")\n",
        "print(\"VADER usually gives it a mixed or close-to-neutral score because it balances both sentiments.\")\n"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e76aab-bfc2-4078-e9a6-9920eaa31448"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pizza Review: The pizza was absolutely delicious, but the service was terrible ... I won't go back.\n",
            "Sentiment: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "\n",
            "Math Complaint: I love machine learning, but I hate the math behind it.\n",
            "Sentiment: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n",
            "\n",
            "Interpretation:\n",
            "Compound Score Range = -1 (negative) to +1 (positive).\n",
            "Notice the pizza review has both 'delicious' (positive) and 'terrible' (negative).\n",
            "VADER usually gives it a mixed or close-to-neutral score because it balances both sentiments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}