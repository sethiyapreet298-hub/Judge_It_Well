{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1R0wJgz48mu08Q_teQL5MgFMBjbFmP3wX","timestamp":1765300987884}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Part 1: Text Preprocessing**"],"metadata":{"id":"IYACLCwsyxfa"}},{"cell_type":"markdown","source":["\n","\n","**Data-** given in the below code cell\n","\n","**1.1: Preprocessing From Scratch**\n","\n","**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n","\n","1. Lowercasing: Convert text to lowercase.\n","\n","2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n","\n","3. Tokenization: Split the string into a list of words based on whitespace.\n","\n","4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n","\n","5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n","\n","\n","Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n","\n","**Task:** Run this function on the first sentence of the corpus and print the result."],"metadata":{"id":"MTP8EqylwqDf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIRv3qS2bTFt"},"outputs":[],"source":["corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]"]},{"cell_type":"code","source":["import re\n","\n","def naive_stemmer(word):\n","    suffixes = ['ing', 'ly', 'ed', 's']\n","    for suf in suffixes:\n","        if word.endswith(suf) and len(word) > len(suf):\n","            return word[:-len(suf)]\n","    return word\n","\n","def clean_text_scratch(text):\n","    # 1. Lowercase\n","    text = text.lower()\n","\n","    # 2. Remove punctuation/special characters\n","    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n","\n","    # 3. Tokenization\n","    tokens = text.split()\n","\n","    # 4. Stopword removal\n","    stopwords = ['the','is','in','to','of','and','a','it','was','but','or']\n","    tokens = [word for word in tokens if word not in stopwords]\n","\n","    # 5. Naive stemming\n","    tokens = [naive_stemmer(word) for word in tokens]\n","\n","    return tokens\n","\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","\n","result = clean_text_scratch(corpus[0])\n","result\n","\n"],"metadata":{"id":"oR4BKqITy17z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765301725545,"user_tz":-330,"elapsed":19,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"97d395f4-108b-48c4-af03-e4670f47b6dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['artificial',\n"," 'intelligence',\n"," 'transform',\n"," 'world',\n"," 'however',\n"," 'ethical',\n"," 'concern',\n"," 'remain']"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["**1.2: Preprocessing Using Tools**\n","\n","**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n","\n","**Steps:**\n","\n","1. Use nltk.tokenize.word_tokenize.\n","2. Use nltk.corpus.stopwords.\n","3. Use nltk.stem.WordNetLemmatizer\n","\n","to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n","\n","\n","**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."],"metadata":{"id":"dN9rNq7WycqZ"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import string\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def clean_with_nltk(text):\n","    # 1. Tokenize\n","    tokens = word_tokenize(text)\n","\n","    # 2. Lowercase and remove punctuation tokens\n","    tokens = [t.lower() for t in tokens if t not in string.punctuation]\n","\n","    # 3. Stopword removal\n","    tokens = [t for t in tokens if t not in stop_words]\n","\n","    # 4. Lemmatization\n","    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n","\n","    return tokens\n","\n","pizza_review = corpus[1]\n","\n","result_nltk = clean_with_nltk(pizza_review)\n","result_nltk\n"],"metadata":{"id":"v_4FjuCqy5Kt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765301897131,"user_tz":-330,"elapsed":174,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"abf26b82-82eb-48f9-b48c-803c5eac09d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["['pizza',\n"," 'absolutely',\n"," 'delicious',\n"," 'service',\n"," 'terrible',\n"," '...',\n"," 'wo',\n"," \"n't\",\n"," 'go',\n"," 'back']"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["# **Part 2: Text Representation**"],"metadata":{"id":"hPMrwva2y1LG"}},{"cell_type":"markdown","source":["**2.1: Bag of Words (BoW)**\n","\n","**Logic:**\n","\n","**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n","\n","**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n","\n","**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""],"metadata":{"id":"cKa8NnZ5zLlm"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# NLTK downloads (run once)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import string\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def clean_with_nltk(text):\n","    tokens = word_tokenize(text)\n","    tokens = [t.lower() for t in tokens if t not in string.punctuation]\n","    tokens = [t for t in tokens if t not in stop_words]\n","    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n","    return tokens\n","\n","\n","cleaned_corpus = [clean_with_nltk(sentence) for sentence in corpus]\n","\n","vocab = sorted(list(set(word for sent in cleaned_corpus for word in sent)))\n","\n","print(\"Vocabulary:\\n\", vocab)\n","print(\"\\nVocabulary Size:\", len(vocab))\n","\n","def bow_vectorize(sentence, vocab):\n","    cleaned = clean_with_nltk(sentence)\n","    vector = [cleaned.count(word) for word in vocab]\n","    return vector\n","\n","test_sentence = \"The quick brown fox jumps over the lazy dog.\"\n","bow_vector = bow_vectorize(test_sentence, vocab)\n","\n","print(\"\\nBoW Vector:\\n\", bow_vector)\n"],"metadata":{"id":"yVUFCkm7yrg-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765302316767,"user_tz":-330,"elapsed":38,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"a862cab4-4816-45f8-9981-112bba86045a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:\n"," [\"'t\", '...', 'absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jump', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', \"n't\", 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'transforming', 'whether', 'wo', 'world']\n","\n","Vocabulary Size: 41\n","\n","BoW Vector:\n"," [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**2.2: BoW Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n","\n","**Steps:**\n","\n","1. Instantiate the vectorizer.\n","\n","2. fit_transform the raw corpus.\n","\n","3. Convert the result to an array (.toarray()) and print it."],"metadata":{"id":"UwsoZix-zUDC"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","\n","bow_matrix = vectorizer.fit_transform(corpus)\n","bow_array = bow_matrix.toarray()\n","\n","print(\"Vocabulary:\\n\", vectorizer.get_feature_names_out())\n","print(\"\\nBoW Matrix:\\n\", bow_array)\n"],"metadata":{"id":"RGs7EzLRzfGC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765303006919,"user_tz":-330,"elapsed":51,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"adcc9683-bb1f-4224-8f35-50ad7b71a52c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:\n"," ['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n"," 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n"," 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n"," 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n"," 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n"," 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n"," 'whether' 'won' 'world']\n","\n","BoW Matrix:\n"," [[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n"," [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n","  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n"," [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n","  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n","  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n"," [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n","  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["**2.3: TF-IDF From Scratch (The Math)**\n","\n","**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n","\n","\"I love machine learning, but I hate the math behind it.\"\n","\n","**Formula:**\n","\n","*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n","\n","*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n","\n","**Result:** TF * IDF.\n","\n","**Task:** Print your manual calculation result."],"metadata":{"id":"-MR6Bxgh0Gpu"}},{"cell_type":"code","source":["import math\n","\n","sentence = \"I love machine learning but I hate the math behind it\"\n","word = \"machine\"\n","\n","words = sentence.lower().split()\n","tf = words.count(word) / len(words)\n","\n","documents = [\n","    \"The quick brown fox jumps over the lazy dog\",\n","    \"I absolutely love pizza, but the service was terrible\",\n","    \"Machine learning involves transforming data into intelligence\",\n","    \"To be or not to be, that is the question\",\n","    \"Math and statistics are behind artificial intelligence\",\n","    \"I love machine learning but I hate the math behind it\"\n","]\n","\n","processed_docs = [doc.lower().split() for doc in documents]\n","\n","doc_count_with_word = sum(word in doc for doc in processed_docs)\n","\n","idf = math.log(len(documents) / doc_count_with_word)\n","\n","\n","tfidf = tf * idf\n","\n","print(\"TF:\", tf)\n","print(\"IDF:\", idf)\n","print(\"TF-IDF:\", tfidf)\n"],"metadata":{"id":"gNSo-nza0k_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765303312464,"user_tz":-330,"elapsed":7,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"7a1b310b-9e88-4791-c6f8-e03446ac2f0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TF: 0.09090909090909091\n","IDF: 1.0986122886681098\n","TF-IDF: 0.09987384442437362\n"]}]},{"cell_type":"markdown","source":["**2.4: TF-IDF Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n","\n","**Steps:** Fit it on the corpus and print the vector for the first sentence.\n","\n","**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"],"metadata":{"id":"YEYkuoSb0nDe"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","corpus = [\n","    \"The quick brown fox jumps over the lazy dog\",\n","    \"I absolutely love pizza, but the service was terrible\",\n","    \"Machine learning involves transforming data into intelligence\",\n","    \"To be or not to be, that is the question\",\n","    \"Math and statistics are behind artificial intelligence\",\n","    \"I love machine learning but I hate the math behind it\"\n","]\n","\n","vectorizer = TfidfVectorizer()\n","\n","X = vectorizer.fit_transform(corpus)\n","\n","vocab = vectorizer.get_feature_names_out()\n","print(\"Vocabulary:\\n\", vocab, \"\\n\")\n","\n","first_vector = X[0].toarray()\n","print(\"TF-IDF Vector for FIRST sentence:\\n\", first_vector, \"\\n\")\n","\n","print(\"Observation:\")\n","print(\"Higher TF-IDF = more unique or rare words in corpus.\")\n","print(\"Lower TF-IDF = common or repeated words like 'the', 'is'.\")\n"],"metadata":{"id":"Of6PfWyd0pnl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765303567541,"user_tz":-330,"elapsed":59,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"8a333595-b0c4-4c6c-93b2-37b47a6e6adb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:\n"," ['absolutely' 'and' 'are' 'artificial' 'be' 'behind' 'brown' 'but' 'data'\n"," 'dog' 'fox' 'hate' 'intelligence' 'into' 'involves' 'is' 'it' 'jumps'\n"," 'lazy' 'learning' 'love' 'machine' 'math' 'not' 'or' 'over' 'pizza'\n"," 'question' 'quick' 'service' 'statistics' 'terrible' 'that' 'the' 'to'\n"," 'transforming' 'was'] \n","\n","TF-IDF Vector for FIRST sentence:\n"," [[0.         0.         0.         0.         0.         0.\n","  0.34487217 0.         0.         0.34487217 0.34487217 0.\n","  0.         0.         0.         0.         0.         0.34487217\n","  0.34487217 0.         0.         0.         0.         0.\n","  0.         0.34487217 0.         0.         0.34487217 0.\n","  0.         0.         0.         0.40919714 0.         0.\n","  0.        ]] \n","\n","Observation:\n","Higher TF-IDF = more unique or rare words in corpus.\n","Lower TF-IDF = common or repeated words like 'the', 'is'.\n"]}]},{"cell_type":"markdown","source":["# **Part 3- Word Embeddings**"],"metadata":{"id":"YWAar8IIzp_m"}},{"cell_type":"markdown","source":["**3.1: Word2Vec Using Tools**\n","\n","**Task:** Train a model using gensim.models.Word2Vec.\n","\n","**Steps:**\n","\n","1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n","\n","2. Set min_count=1 (since our corpus is small, we want to keep all words).\n","\n","3. Set vector_size=10 (small vector size for easy viewing).\n","\n","**Experiment:** Print the vector for the word \"learning\"."],"metadata":{"id":"uY1URFxgz036"}},{"cell_type":"code","source":["!pip install gensim\n","\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words(\"english\"))\n","\n","def clean_text_scratch(text):\n","    text = text.lower()\n","    tokens = word_tokenize(text)\n","\n","    cleaned = []\n","    for t in tokens:\n","        if t.isalpha() and t not in stop_words:\n","            cleaned.append(lemmatizer.lemmatize(t))\n","    return cleaned\n","\n","corpus = [\n","    \"The quick brown fox jumps over the lazy dog\",\n","    \"I absolutely love pizza, but the service was terrible\",\n","    \"Machine learning involves transforming data into intelligence\",\n","    \"To be or not to be, that is the question\",\n","    \"Math and statistics are behind artificial intelligence\",\n","    \"I love machine learning but I hate the math behind it\"\n","]\n","\n","cleaned_corpus = [clean_text_scratch(sent) for sent in corpus]\n","\n","print(\"Cleaned Corpus:\")\n","for c in cleaned_corpus:\n","    print(c)\n","\n","model = Word2Vec(\n","    sentences=cleaned_corpus,\n","    vector_size=10,\n","    window=5,\n","    min_count=1,\n","    workers=4\n",")\n","\n","print(\"\\nVector for 'learning':\")\n","print(model.wv[\"learning\"])\n"],"metadata":{"id":"aziX2IGBzyaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765304206365,"user_tz":-330,"elapsed":19250,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"a5ace43c-3872-4d41-fa4a-94b5a0d58243"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n","Cleaned Corpus:\n","['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n","['absolutely', 'love', 'pizza', 'service', 'terrible']\n","['machine', 'learning', 'involves', 'transforming', 'data', 'intelligence']\n","['question']\n","['math', 'statistic', 'behind', 'artificial', 'intelligence']\n","['love', 'machine', 'learning', 'hate', 'math', 'behind']\n","\n","Vector for 'learning':\n","[-0.07512337 -0.0093043   0.0954107  -0.07318932 -0.02333602 -0.01938214\n","  0.08080113 -0.05932492  0.000434   -0.04756258]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**3.3: Pre-trained GloVe (Understanding Global Context)**\n","\n","**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n","\n","**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n","\n","Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n","\n","**Question:** Does the model correctly guess \"Queen\"?"],"metadata":{"id":"r3J42eQZ1fUo"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","glove_model = api.load('glove-wiki-gigaword-50')\n","\n","result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'])\n","\n","print(result[0])\n"],"metadata":{"id":"LEj5SkO81mkF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765304487213,"user_tz":-330,"elapsed":39870,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"b4fc3a34-401f-4a35-a8d3-bb0c38e1f671"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["('queen', 0.8523604273796082)\n"]}]},{"cell_type":"markdown","source":["# **Part 5- Sentiment Analysis (The Application)**\n","\n","**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n","\n","**Task:**\n","\n","1. Initialize the SentimentIntensityAnalyzer.\n","\n","2. Pass the Pizza Review (corpus[1]) into the analyzer.\n","\n","3. Pass the Math Complaint (corpus[5]) into the analyzer.\n","\n","**Analysis:** Look at the compound score for both.\n","\n","**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n","\n","Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"],"metadata":{"id":"AbI4K0UJUxy3"}},{"cell_type":"code","source":["import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","# Download VADER lexicon (run once)\n","nltk.download('vader_lexicon')\n","\n","corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]\n","\n","sia = SentimentIntensityAnalyzer()\n","\n","pizza_review = corpus[1]\n","pizza_score = sia.polarity_scores(pizza_review)\n","\n","math_complaint = corpus[5]\n","math_score = sia.polarity_scores(math_complaint)\n","\n","print(\"Pizza Review:\", pizza_review)\n","print(\"Sentiment:\", pizza_score, \"\\n\")\n","\n","print(\"Math Complaint:\", math_complaint)\n","print(\"Sentiment:\", math_score, \"\\n\")\n","\n","mixed_sentence = \"The food was delicious but the service was terrible.\"\n","mixed_score = sia.polarity_scores(mixed_sentence)\n","\n","print(\"Mixed Sentence:\", mixed_sentence)\n","print(\"Sentiment:\", mixed_score)\n"],"metadata":{"id":"_lC2c3GHUxU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765305375692,"user_tz":-330,"elapsed":43,"user":{"displayName":"Poorv Kumar","userId":"04715221369993891018"}},"outputId":"8e16fecd-dcf5-4c40-b8dc-0f32d8807781"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Pizza Review: The pizza was absolutely delicious, but the service was terrible ... I won't go back.\n","Sentiment: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926} \n","\n","Math Complaint: I love machine learning, but I hate the math behind it.\n","Sentiment: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346} \n","\n","Mixed Sentence: The food was delicious but the service was terrible.\n","Sentiment: {'neg': 0.307, 'neu': 0.519, 'pos': 0.174, 'compound': -0.4215}\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["VADER correctly detects mixed sentiment when positive and negative words appear together. In the Pizza Review, “delicious” and “terrible” produce a slightly negative compound score (–0.3926). The Math Complaint also shows mixed sentiment and results in a negative score (–0.5346). The test sentence “The food was delicious but the service was terrible” similarly yields a mixed sentiment score (–0.4215). This confirms that VADER handles opposing sentiment cues by producing balanced, near-neutral results that lean toward the stronger emotion."],"metadata":{"id":"C6H5ZFGBY_ht"}}]}