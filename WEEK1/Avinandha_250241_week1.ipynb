{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1R0wJgz48mu08Q_teQL5MgFMBjbFmP3wX","timestamp":1765178769280}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Part 1: Text Preprocessing**"],"metadata":{"id":"IYACLCwsyxfa"}},{"cell_type":"markdown","source":["\n","\n","**Data-** given in the below code cell\n","\n","**1.1: Preprocessing From Scratch**\n","\n","**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n","\n","1. Lowercasing: Convert text to lowercase.\n","\n","2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n","\n","3. Tokenization: Split the string into a list of words based on whitespace.\n","\n","4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n","\n","5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n","\n","\n","Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n","\n","**Task:** Run this function on the first sentence of the corpus and print the result."],"metadata":{"id":"MTP8EqylwqDf"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"qIRv3qS2bTFt","executionInfo":{"status":"ok","timestamp":1765268394573,"user_tz":-330,"elapsed":15,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}}},"outputs":[],"source":["corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]"]},{"cell_type":"code","source":["import re\n","##1.1\n","def clean_text_scratch(corpus):\n","  corpus_clean = [sentence.lower() for sentence in corpus] #converting text to lowercase\n","\n","  corpus_clean = [sentence.replace('!','').replace('.','').replace(',','').replace(':','').replace(';','').replace(\"'\",'') for sentence in corpus_clean] #removing puncuations\n","\n","  corpus_clean = [sentence.split(\" \") for sentence in corpus_clean] #tokenisation\n","\n","  stop_words = ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n","  corpus_clean = [word for sentence in corpus_clean for word in sentence if word not in stop_words] #removing stop words\n","\n","  return corpus_clean\n","def helper(corpus): # helper function for simple stemming\n","  corpus_clean = [word.replace('ing','').replace('ly','').replace('ed','').replace('s','') for word in clean_text_scratch(corpus)] #simple stemming\n","  print(corpus_clean)\n","helper(corpus)"],"metadata":{"id":"oR4BKqITy17z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765268400085,"user_tz":-330,"elapsed":27,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"a1535909-32ba-44e0-87c8-12f90b93867b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['artificial', 'intelligence', 'tranform', 'world', 'however', 'ethical', 'concern', 'remain', 'pizza', 'abolute', 'deliciou', 'ervice', 'terrible', '', 'i', 'wont', 'go', 'back', 'quick', 'brown', 'fox', 'jump', 'over', 'lazy', 'dog', 'be', 'not', 'be', 'that', 'quetion', 'whether', 'ti', 'nobler', 'mind', 'data', 'cience', 'involve', 'tatitic', 'linear', 'algebra', 'machine', 'learn', 'i', 'love', 'machine', 'learn', 'i', 'hate', 'math', 'behind']\n"]}]},{"cell_type":"markdown","source":["**1.2: Preprocessing Using Tools**\n","\n","**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n","\n","**Steps:**\n","\n","1. Use nltk.tokenize.word_tokenize.\n","2. Use nltk.corpus.stopwords.\n","3. Use nltk.stem.WordNetLemmatizer\n","\n","to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n","\n","\n","**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."],"metadata":{"id":"dN9rNq7WycqZ"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","def clean_sentence(sentence):\n","\n","  sentence = sentence.lower() # converting to lowercase\n","\n","  translator = str.maketrans('', '', string.punctuation)\n","  clean_text = sentence.translate(translator)  #removing puntuations\n","\n","  tokens = word_tokenize(clean_text) #tokenization\n","\n","  stop_words = set(stopwords.words('english'))\n","  tokens = [word for word in tokens if word not in stop_words] #removing stop_words\n","\n","  lemmatizer = WordNetLemmatizer()\n","  sentence = [lemmatizer.lemmatize(word,pos = 'v') for word in tokens] #lemmatising the verbs\n","  return sentence\n","\n","cleaned_corpus = [clean_sentence(sentence) for sentence in corpus]\n","print(cleaned_corpus[1])"],"metadata":{"id":"v_4FjuCqy5Kt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765283283743,"user_tz":-330,"elapsed":8,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"7f7615ab-3e54-4d57-ef17-79c5a5facc7b"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# **Part 2: Text Representation**"],"metadata":{"id":"hPMrwva2y1LG"}},{"cell_type":"markdown","source":["**2.1: Bag of Words (BoW)**\n","\n","**Logic:**\n","\n","**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n","\n","**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n","\n","**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""],"metadata":{"id":"cKa8NnZ5zLlm"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import string\n","# NLTK downloads (run once)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","given = \"The quick brown fox jumps over the lazy dog.\"\n","\n","def clean_sentence2(sentence):\n","  sentence = sentence.lower()\n","  translator = str.maketrans('', '', string.punctuation)\n","  clean_text = sentence.translate(translator)\n","  tokens = word_tokenize(clean_text)\n","  lemmatizer = WordNetLemmatizer()\n","  sentence = [lemmatizer.lemmatize(word,pos = 'v') for word in tokens]\n","  return sentence\n","\n","def clean_doc(doc):     #function to tokenize corpus\n","  vocabulary = []\n","  for sentence in doc:\n","    vocabulary.append(clean_sentence2(sentence))\n","  vocabulary = [words for sentence in vocabulary for words in sentence]\n","  vocabulary = unique_words(vocabulary)\n","  vocabulary.sort()\n","  return vocabulary\n","\n","def unique_words(text):     #function to find unique vocabulary list\n","  unique_text = []\n","  for i in range(len(text)):\n","    for j in range(len(text)):\n","      if text[i] not in unique_text :\n","        unique_text.append(text[i])\n","  return unique_text   #unique vocabulary list\n","\n","\n","given = clean_sentence2(given)\n","\n","def vectorize(words): #function to vectorise manually\n","  vector = []\n","  for w in unique_words(clean_doc(corpus)):\n","    i = 0\n","    for word in given:\n","      if w == word:\n","        i += 1\n","    vector.append(i)\n","  print(vector)       #vector\n","\n","\n","print(unique_words(clean_doc(corpus)))\n","vectorize(unique_words(clean_doc(corpus)))\n"],"metadata":{"id":"yVUFCkm7yrg-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765274621876,"user_tz":-330,"elapsed":9,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"46eb4574-dd95-4f46-a0b1-8c7f8452fe70"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["['absolutely', 'algebra', 'and', 'artificial', 'back', 'be', 'behind', 'brown', 'but', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'i', 'in', 'intelligence', 'involve', 'it', 'jump', 'lazy', 'learn', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'not', 'or', 'over', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistics', 'terrible', 'that', 'the', 'tis', 'to', 'transform', 'whether', 'wont', 'world']\n","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**2.2: BoW Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n","\n","**Steps:**\n","\n","1. Instantiate the vectorizer.\n","\n","2. fit_transform the raw corpus.\n","\n","3. Convert the result to an array (.toarray()) and print it."],"metadata":{"id":"UwsoZix-zUDC"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","vectorizer.fit_transform(corpus)\n","vocab = vectorizer.get_feature_names_out()\n","CountVectorizer(vocabulary = vocab)\n","print(vocab)\n","print(vectorizer.transform([corpus[2]]).toarray()[0].tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIwq74MnOiHW","executionInfo":{"status":"ok","timestamp":1765274527878,"user_tz":-330,"elapsed":18,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"c8420f58-8562-4369-c635-034598d9b3e6"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n"," 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n"," 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n"," 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n"," 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n"," 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n"," 'whether' 'won' 'world']\n","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"markdown","source":["**2.3: TF-IDF From Scratch (The Math)**\n","\n","**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n","\n","\"I love machine learning, but I hate the math behind it.\"\n","\n","**Formula:**\n","\n","*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n","\n","*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n","\n","**Result:** TF * IDF.\n","\n","**Task:** Print your manual calculation result."],"metadata":{"id":"-MR6Bxgh0Gpu"}},{"cell_type":"code","source":["#2.3\n","import math\n","\n","def score_of_word(word,sentence,doc):\n","  sentence = clean_sentence2(sentence)\n","  count_of_word = 0\n","  for w in sentence:\n","    if w == word:\n","      count_of_word += 1\n","\n","  TF = count_of_word/len(sentence) # term frequency\n","\n","  den = 0\n","  for s in doc:\n","    if word in s:\n","      den += 1\n","  IDF = math.log(len(doc)/den) #inverse document frequency\n","\n","  return TF*IDF\n","\n","print(score_of_word(\"machine\",corpus[5],corpus))"],"metadata":{"id":"gNSo-nza0k_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765297725863,"user_tz":-330,"elapsed":10,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"fd9cfbdd-9e95-4087-c111-a612f7979cb6"},"execution_count":138,"outputs":[{"output_type":"stream","name":"stdout","text":["0.09987384442437362\n"]}]},{"cell_type":"markdown","source":["**2.4: TF-IDF Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n","\n","**Steps:** Fit it on the corpus and print the vector for the first sentence.\n","\n","**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"],"metadata":{"id":"YEYkuoSb0nDe"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(lowercase=True)\n","tfidf_matrix = vectorizer.fit_transform(corpus)\n","\n","first_sentence_vector = tfidf_matrix[0].toarray()[0]  #for first sentence\n","vocab = vectorizer.get_feature_names_out()\n","\n","for word, score in zip(vocab, first_sentence_vector):\n","    if score > 0:\n","        print(f\"{word:<15}\\t{score:.4f}\")\n","\n","# comparing scores of word like Intelligence and Is\n","intelligence_idx = list(vocab).index('intelligence')\n","intelligence_score = first_sentence_vector[intelligence_idx]\n","is_idx = list(vocab).index('is')\n","is_score = first_sentence_vector[is_idx]\n","print(f\"intelligence score = {intelligence_score} >  is score {is_score}\")"],"metadata":{"id":"Of6PfWyd0pnl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765296121670,"user_tz":-330,"elapsed":82,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"3b2081d5-2f4b-4f25-cf2d-55c35bda29cf"},"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["artificial     \t0.3345\n","concerns       \t0.3345\n","ethical        \t0.3345\n","however        \t0.3345\n","intelligence   \t0.3345\n","is             \t0.2743\n","remain         \t0.3345\n","the            \t0.1714\n","transforming   \t0.3345\n","world          \t0.3345\n","intelligence score = 0.3345454287016015 >  is score 0.27433203727401334\n"]}]},{"cell_type":"markdown","source":["# **Part 3- Word Embeddings**"],"metadata":{"id":"YWAar8IIzp_m"}},{"cell_type":"markdown","source":["**3.1: Word2Vec Using Tools**\n","\n","**Task:** Train a model using gensim.models.Word2Vec.\n","\n","**Steps:**\n","\n","1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n","\n","2. Set min_count=1 (since our corpus is small, we want to keep all words).\n","\n","3. Set vector_size=10 (small vector size for easy viewing).\n","\n","**Experiment:** Print the vector for the word \"learning\"."],"metadata":{"id":"uY1URFxgz036"}},{"cell_type":"code","source":["!pip install gensim\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","#3.1\n","model = Word2Vec(\n","    sentences=cleaned_corpus,\n","    vector_size=10,\n","    min_count=1\n",")\n","\n","vector = model.wv['learn']   # \"learning\" does not work because we have lemmatized in Question 1.2\n","print(vector)"],"metadata":{"id":"aziX2IGBzyaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765283570708,"user_tz":-330,"elapsed":14180,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"ae39f80a-0a65-4d63-ba86-02f106345352"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n","[-0.00536899  0.00237282  0.05103846  0.0900786  -0.09300981 -0.07119522\n","  0.06463154  0.08977251 -0.0501886  -0.03764008]\n"]}]},{"cell_type":"markdown","source":["**3.3: Pre-trained GloVe (Understanding Global Context)**\n","\n","**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n","\n","**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n","\n","Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n","\n","**Question:** Does the model correctly guess \"Queen\"?"],"metadata":{"id":"r3J42eQZ1fUo"}},{"cell_type":"code","source":["import gensim.downloader as api\n","model = api.load('glove-wiki-gigaword-50')"],"metadata":{"id":"VyFOOdknm-Ll","executionInfo":{"status":"ok","timestamp":1765292183571,"user_tz":-330,"elapsed":39183,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["analogy = model.most_similar(positive=['woman', 'king'], negative=['man'])\n","print(f\"King-Man+Woman = {analogy[0][0]}\")\n","## YEP, THE MODEL IS GUESSING IT IS AS QUEEN!!!!!!!"],"metadata":{"id":"LEj5SkO81mkF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765292492940,"user_tz":-330,"elapsed":9,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"f2c220a9-5075-4838-f52e-827f0cabd525"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["King-Man+Woman = queen\n"]}]},{"cell_type":"markdown","source":["# **Part 5- Sentiment Analysis (The Application)**\n","\n","**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n","\n","**Task:**\n","\n","1. Initialize the SentimentIntensityAnalyzer.\n","\n","2. Pass the Pizza Review (corpus[1]) into the analyzer.\n","\n","3. Pass the Math Complaint (corpus[5]) into the analyzer.\n","\n","**Analysis:** Look at the compound score for both.\n","\n","**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n","\n","Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"],"metadata":{"id":"AbI4K0UJUxy3"}},{"cell_type":"code","source":["import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","# Download VADER (run once)\n","nltk.download('vader_lexicon')\n"],"metadata":{"id":"_lC2c3GHUxU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765293313258,"user_tz":-330,"elapsed":723,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"e18979d3-f218-443f-96a7-103c792b671c"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","source":["sa = SentimentIntensityAnalyzer()\n","def Sentiment_analyser(review):\n","  scores = sa.polarity_scores(review)\n","  print(scores)\n","  print(scores['compound'])\n","  i = -1\n","  j= -0.6\n","  rate = 0\n","  for k in range(5):\n","    if i< scores['compound'] and  scores['compound']< j:\n","        rate += 1\n","    i+= 0.4\n","    j+= 0.4\n","\n","  print(f\"Rate of the review in scale of 0 to 4 = {rate}\")\n","\n","Sentiment_analyser(corpus[1])      #\"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\"\n","Sentiment_analyser(corpus[5])      #\"I love machine learning, but I hate the math behind it.\"\n","\n","#COMPOUND SCORES ARE DIFFERENT AND IS HIGHER FOR CORPUS[1] THAN CORPUS[2]\n","#BOTH THE SENTENCES HAVE GOOD AND BAD REVIEW, HENCE BOTH THE SENTENCE IS GIVING A NEUTRAL SCORE."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGgtk08urgO8","executionInfo":{"status":"ok","timestamp":1765295829108,"user_tz":-330,"elapsed":93,"user":{"displayName":"Avinandha Sreedhara","userId":"15725737585025247503"}},"outputId":"ac9ebbe0-9842-4386-d802-a9c879aea4ed"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["{'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n","-0.3926\n","Rate of the review in scale of 0 to 4 = 1\n","{'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n","-0.5346\n","Rate of the review in scale of 0 to 4 = 1\n"]}]}]}