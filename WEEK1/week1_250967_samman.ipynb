{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "import re\n",
        "newcorpus=\"\"\n",
        "for t in corpus:\n",
        "  newcorpus+=t.lower()\n",
        "print(newcorpus)\n",
        "removespecialchar=r'[!.,:;\\'\"]'\n",
        "newcorpus=re.sub(removespecialchar,'',newcorpus)\n",
        "print(newcorpus)\n",
        "newcorpus=newcorpus.split();\n",
        "print(newcorpus);\n",
        "stopword=['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "newcorpus2=[]\n",
        "for t in newcorpus:\n",
        "  if t in stopword:\n",
        "    continue\n",
        "  newcorpus2.append(t)\n",
        "print(newcorpus2)\n",
        "suffixes = ['ing', 'ly', 'ed', 's']\n",
        "newcorpus3=newcorpus2\n",
        "newcorpus2=[]\n",
        "\n",
        "for t in newcorpus3:\n",
        "  found =1\n",
        "  for suffix in suffixes:\n",
        "    if(t.endswith(suffix)):\n",
        "\n",
        "      newcorpus2.append(t[:-len(suffix)])\n",
        "      found =0\n",
        "  if(found==1):\n",
        "    newcorpus2.append(t)\n",
        "print(newcorpus2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#write rest of the code here"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad69dc9-bac7-4e85-f388-3b722519360d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial intelligence is transforming the world; however, ethical concerns remain!the pizza was absolutely delicious, but the service was terrible ... i won't go back.the quick brown fox jumps over the lazy dog.to be, or not to be, that is the question: whether 'tis nobler in the mind.data science involves statistics, linear algebra, and machine learning.i love machine learning, but i hate the math behind it.\n",
            "artificial intelligence is transforming the world however ethical concerns remainthe pizza was absolutely delicious but the service was terrible  i wont go backthe quick brown fox jumps over the lazy dogto be or not to be that is the question whether tis nobler in the minddata science involves statistics linear algebra and machine learningi love machine learning but i hate the math behind it\n",
            "['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', 'however', 'ethical', 'concerns', 'remainthe', 'pizza', 'was', 'absolutely', 'delicious', 'but', 'the', 'service', 'was', 'terrible', 'i', 'wont', 'go', 'backthe', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogto', 'be', 'or', 'not', 'to', 'be', 'that', 'is', 'the', 'question', 'whether', 'tis', 'nobler', 'in', 'the', 'minddata', 'science', 'involves', 'statistics', 'linear', 'algebra', 'and', 'machine', 'learningi', 'love', 'machine', 'learning', 'but', 'i', 'hate', 'the', 'math', 'behind', 'it']\n",
            "['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concerns', 'remainthe', 'pizza', 'absolutely', 'delicious', 'service', 'terrible', 'i', 'wont', 'go', 'backthe', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dogto', 'be', 'not', 'be', 'that', 'question', 'whether', 'tis', 'nobler', 'minddata', 'science', 'involves', 'statistics', 'linear', 'algebra', 'machine', 'learningi', 'love', 'machine', 'learning', 'i', 'hate', 'math', 'behind']\n",
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remainthe', 'pizza', 'absolute', 'deliciou', 'service', 'terrible', 'i', 'wont', 'go', 'backthe', 'quick', 'brown', 'fox', 'jump', 'over', 'lazy', 'dogto', 'be', 'not', 'be', 'that', 'question', 'whether', 'ti', 'nobler', 'minddata', 'science', 'involve', 'statistic', 'linear', 'algebra', 'machine', 'learningi', 'love', 'machine', 'learn', 'i', 'hate', 'math', 'behind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#write rest of the code here\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "target_sentence_index = 1\n",
        "target_sentence = corpus[target_sentence_index]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "            cleaned_tokens.append(lemma)\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "cleaned_corpus = [process_text(sentence) for sentence in corpus]\n",
        "\n",
        "print(f\"--- Processing Results for Sentence #{target_sentence_index + 1} ---\")\n",
        "print(f\"Original Sentence:\\n'{target_sentence}'\")\n",
        "\n",
        "cleaned_second_sentence_tokens = cleaned_corpus[target_sentence_index]\n",
        "\n",
        "print(\"\\nCleaned & Lemmatized Tokens:\")\n",
        "print(cleaned_second_sentence_tokens)"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a53cd38-45a2-46a1-e5eb-46b53afb54c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing Results for Sentence #2 ---\n",
            "Original Sentence:\n",
            "'The pizza was absolutely delicious, but the service was terrible ... I won't go back.'\n",
            "\n",
            "Cleaned & Lemmatized Tokens:\n",
            "['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Define the corpus and tools (reusing setup from prior steps)\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "            cleaned_tokens.append(lemma)\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Process the corpus to get a list of lists of cleaned tokens\n",
        "cleaned_corpus_tokens = [process_text(sentence) for sentence in corpus]\n",
        "print(cleaned_corpus_tokens)\n",
        "# Flatten the list of lists into a single list of all tokens\n",
        "all_tokens = [token for sentence_tokens in cleaned_corpus_tokens for token in sentence_tokens]\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d8a659-eb53-46f4-d8a7-00e66d12de65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concern', 'remain'], ['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back'], ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog'], ['question', 'whether', 'ti', 'nobler', 'mind'], ['data', 'science', 'involves', 'statistic', 'linear', 'algebra', 'machine', 'learning'], ['love', 'machine', 'learning', 'hate', 'math', 'behind']]\n",
            "['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concern', 'remain', 'pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'question', 'whether', 'ti', 'nobler', 'mind', 'data', 'science', 'involves', 'statistic', 'linear', 'algebra', 'machine', 'learning', 'love', 'machine', 'learning', 'hate', 'math', 'behind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#rest of the code here\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "X_array = X.toarray()\n",
        "\n",
        "print(\"Unique Word List: \\n\", feature_names)\n",
        "print(\"Bag of Words Matrix: \\n\", X_array)"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d08f1c9-bf5b-4494-ccfc-00b2c1457b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Word List: \n",
            " ['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n",
            " 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n",
            " 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n",
            " 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n",
            " 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n",
            " 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n",
            " 'whether' 'won' 'world']\n",
            "Bag of Words Matrix: \n",
            " [[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write code here\n",
        "s=\"I love machine learning, but I hate the math behind it.\"\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import math as math\n",
        "s=s.split()\n",
        "f=0;\n",
        "d=0;\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "for token in s:\n",
        "  if(token==\"machine\"):\n",
        "    f+=1\n",
        "  d+=1\n",
        "print(d)\n",
        "tf=f/d\n",
        "ndoc=0\n",
        "ndocsp=0\n",
        "for sentence in corpus:\n",
        "  if\"machine\" in sentence.lower():\n",
        "    ndocsp+=1\n",
        "  ndoc+=1\n",
        "idf=math.log(ndoc/ndocsp)\n",
        "print(tf*idf)\n"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812bbed1-43a6-4617-c983-2286066d3788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "0.09987384442437362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUVGnJZkm6z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# rest of the code here\n",
        "tr_idf_model  = TfidfVectorizer()\n",
        "import pandas as pd\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "tf_idf_vector = tr_idf_model.fit_transform(corpus)\n",
        "tf_idf_array = tf_idf_vector.toarray()\n",
        "\n",
        "print(tf_idf_array)\n",
        "words_set = tr_idf_model.get_feature_names_out()\n",
        "df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set)\n",
        "\n",
        "df_tf_idf"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b4ae92b-4d4e-47a8-a393-3fcd03918b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.33454543 0.         0.\n",
            "  0.         0.         0.         0.33454543 0.         0.\n",
            "  0.         0.33454543 0.         0.         0.         0.33454543\n",
            "  0.         0.33454543 0.         0.27433204 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.33454543 0.         0.         0.\n",
            "  0.         0.         0.17139656 0.         0.         0.33454543\n",
            "  0.         0.         0.         0.33454543]\n",
            " [0.26995162 0.         0.         0.         0.26995162 0.\n",
            "  0.         0.         0.22136419 0.         0.         0.26995162\n",
            "  0.         0.         0.         0.26995162 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.26995162\n",
            "  0.         0.         0.         0.         0.26995162 0.\n",
            "  0.26995162 0.         0.27660686 0.         0.         0.\n",
            "  0.53990324 0.         0.26995162 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.35245556 0.         0.         0.         0.\n",
            "  0.35245556 0.         0.35245556 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.35245556\n",
            "  0.35245556 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.35245556 0.\n",
            "  0.         0.35245556 0.         0.         0.         0.\n",
            "  0.         0.         0.3611448  0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.4622213\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.23111065 0.         0.         0.18951404 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.23111065 0.23111065 0.23111065 0.23111065 0.         0.\n",
            "  0.23111065 0.         0.         0.         0.         0.\n",
            "  0.         0.23111065 0.23680833 0.23111065 0.4622213  0.\n",
            "  0.         0.23111065 0.         0.        ]\n",
            " [0.         0.3461711  0.3461711  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.3461711  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.3461711  0.         0.         0.\n",
            "  0.         0.28386526 0.3461711  0.         0.28386526 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.3461711  0.         0.3461711\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.37063105 0.         0.30392276 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.37063105 0.\n",
            "  0.         0.         0.         0.         0.37063105 0.\n",
            "  0.         0.30392276 0.         0.37063105 0.30392276 0.37063105\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.18988419 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   absolutely   algebra       and  artificial      back        be    behind  \\\n",
              "0    0.000000  0.000000  0.000000    0.334545  0.000000  0.000000  0.000000   \n",
              "1    0.269952  0.000000  0.000000    0.000000  0.269952  0.000000  0.000000   \n",
              "2    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000   \n",
              "3    0.000000  0.000000  0.000000    0.000000  0.000000  0.462221  0.000000   \n",
              "4    0.000000  0.346171  0.346171    0.000000  0.000000  0.000000  0.000000   \n",
              "5    0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.370631   \n",
              "\n",
              "      brown       but  concerns  ...  terrible      that       the       tis  \\\n",
              "0  0.000000  0.000000  0.334545  ...  0.000000  0.000000  0.171397  0.000000   \n",
              "1  0.000000  0.221364  0.000000  ...  0.269952  0.000000  0.276607  0.000000   \n",
              "2  0.352456  0.000000  0.000000  ...  0.000000  0.000000  0.361145  0.000000   \n",
              "3  0.000000  0.000000  0.000000  ...  0.000000  0.231111  0.236808  0.231111   \n",
              "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
              "5  0.000000  0.303923  0.000000  ...  0.000000  0.000000  0.189884  0.000000   \n",
              "\n",
              "         to  transforming       was   whether       won     world  \n",
              "0  0.000000      0.334545  0.000000  0.000000  0.000000  0.334545  \n",
              "1  0.000000      0.000000  0.539903  0.000000  0.269952  0.000000  \n",
              "2  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "3  0.462221      0.000000  0.000000  0.231111  0.000000  0.000000  \n",
              "4  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "5  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "\n",
              "[6 rows x 52 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a76d3ac0-5d3e-4d93-828e-bf3a0e58f65a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>absolutely</th>\n",
              "      <th>algebra</th>\n",
              "      <th>and</th>\n",
              "      <th>artificial</th>\n",
              "      <th>back</th>\n",
              "      <th>be</th>\n",
              "      <th>behind</th>\n",
              "      <th>brown</th>\n",
              "      <th>but</th>\n",
              "      <th>concerns</th>\n",
              "      <th>...</th>\n",
              "      <th>terrible</th>\n",
              "      <th>that</th>\n",
              "      <th>the</th>\n",
              "      <th>tis</th>\n",
              "      <th>to</th>\n",
              "      <th>transforming</th>\n",
              "      <th>was</th>\n",
              "      <th>whether</th>\n",
              "      <th>won</th>\n",
              "      <th>world</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.334545</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.334545</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.171397</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.334545</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.334545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.269952</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.269952</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.221364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269952</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.276607</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.539903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.269952</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.352456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.361145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.462221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.231111</td>\n",
              "      <td>0.236808</td>\n",
              "      <td>0.231111</td>\n",
              "      <td>0.462221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.231111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.346171</td>\n",
              "      <td>0.346171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.370631</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.303923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.189884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6 rows Ã— 52 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a76d3ac0-5d3e-4d93-828e-bf3a0e58f65a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a76d3ac0-5d3e-4d93-828e-bf3a0e58f65a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a76d3ac0-5d3e-4d93-828e-bf3a0e58f65a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a4106883-da37-4fdf-b246-85c0e9786a2b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4106883-da37-4fdf-b246-85c0e9786a2b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a4106883-da37-4fdf-b246-85c0e9786a2b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_faaeed71-135f-411e-bd38-172a583ea527\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_tf_idf')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_faaeed71-135f-411e-bd38-172a583ea527 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_tf_idf');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_tf_idf"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#rest of the code here\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "            cleaned_tokens.append(lemma)\n",
        "\n",
        "    return cleaned_tokens\n",
        "cleaned_corpus_tokens = [process_text(sentence) for sentence in corpus]\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=cleaned_corpus_tokens,\n",
        "    min_count=1,\n",
        "    vector_size=10,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "target_word = \"learning\"\n",
        "\n",
        "if target_word in model.wv:\n",
        "    vector = model.wv[target_word]\n",
        "\n",
        "\n",
        "    print(vector)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"Error: The word '{target_word}' was not found in the model's vocabulary.\")"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9907e29d-786e-4199-c118-dc7bb720819f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "--- Word2Vec Vector for 'learning' ---\n",
            "Word: 'learning'\n",
            "Vector (size 10):\n",
            "[-0.00536899  0.00237282  0.05103846  0.0900786  -0.09300981 -0.07119522\n",
            "  0.06463154  0.08977251 -0.0501886  -0.03764008]\n",
            "\n",
            "Vector Components (Rounded):\n",
            "[-0.0054  0.0024  0.051   0.0901 -0.093  -0.0712  0.0646  0.0898 -0.0502\n",
            " -0.0376]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "positive_words = ['woman', 'king']\n",
        "negative_words = ['man']\n",
        "\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "analogy_results = model.most_similar(\n",
        "    positive=positive_words,\n",
        "    negative=negative_words\n",
        ")\n",
        "for rank, (word, score) in enumerate(analogy_results, 1):\n",
        "    print(f\"{rank}. {word.ljust(10)} (Similarity Score: {score:.4f})\")\n",
        "    if word.lower() == 'queen':\n",
        "        guess_is_queen = True\n"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3456e54-65f1-4ec9-ee77-7a84d1bfa124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. queen      (Similarity Score: 0.8524)\n",
            "2. throne     (Similarity Score: 0.7664)\n",
            "3. prince     (Similarity Score: 0.7592)\n",
            "4. daughter   (Similarity Score: 0.7474)\n",
            "5. elizabeth  (Similarity Score: 0.7460)\n",
            "6. princess   (Similarity Score: 0.7425)\n",
            "7. kingdom    (Similarity Score: 0.7337)\n",
            "8. monarch    (Similarity Score: 0.7214)\n",
            "9. eldest     (Similarity Score: 0.7185)\n",
            "10. widow      (Similarity Score: 0.7099)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]\n",
        "# rest of the code here\n",
        "nltk.download('vader_lexicon')\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "target_sentence = corpus[5] # The pizza review\n",
        "scores = analyzer.polarity_scores(target_sentence)\n",
        "\n",
        "print(f\"Original: '{target_sentence}'\")\n",
        "print(f\"Compound Score: {scores['compound']:.4f}\")\n",
        "print(f\"Polarity Breakdown: {scores}\")\n",
        "target_sentence = corpus[1]\n",
        "print(f\"Original: '{target_sentence}'\")\n",
        "scores = analyzer.polarity_scores(target_sentence)\n",
        "print(f\"Compound Score: {scores['compound']:.4f}\")\n",
        "print(f\"Polarity Breakdown: {scores}\")"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df428a93-eda1-4454-f23c-531ad7d1ec22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 'I love machine learning, but I hate the math behind it.'\n",
            "Compound Score: -0.5346\n",
            "Polarity Breakdown: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n",
            "Original: 'The pizza was absolutely delicious, but the service was terrible ... I won't go back.'\n",
            "Compound Score: -0.3926\n",
            "Polarity Breakdown: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}