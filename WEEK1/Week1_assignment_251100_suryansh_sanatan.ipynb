{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#write rest of the code here\n",
        "def clean_text_scratch(text):\n",
        "\n",
        "\n",
        "  for i in range(len(text)):\n",
        "    cor1=[words.lower() for words in text ]\n",
        "    cor2=[re.sub(r'[^\\w\\s]','',lower_words) for lower_words in cor1]\n",
        "    cor3=[only_words.split() for only_words in cor2 ]\n",
        "    stopwords=['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
        "    cor4=[[main_words for main_words in sentence if main_words not in stopwords] for sentence in cor3 ]\n",
        "    suffixes=['ing','ly','ed','s']\n",
        "    for suffix in suffixes:\n",
        "      cor5 = [[\n",
        "      word[:-len(suffix)] if word.endswith(suffix) else word\n",
        "      for word in refined_sentence\n",
        "  ] for refined_sentence in cor4]\n",
        "\n",
        "    return cor5"
      ],
      "metadata": {
        "id": "oR4BKqITy17z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text_scratch(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SyWMjtUHpUR",
        "outputId": "eff5b1e4-322b-4e13-875b-5c37f871d393"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['artificial',\n",
              "  'intelligence',\n",
              "  'transforming',\n",
              "  'world',\n",
              "  'however',\n",
              "  'ethical',\n",
              "  'concern',\n",
              "  'remain'],\n",
              " ['pizza',\n",
              "  'absolutely',\n",
              "  'deliciou',\n",
              "  'service',\n",
              "  'terrible',\n",
              "  'i',\n",
              "  'wont',\n",
              "  'go',\n",
              "  'back'],\n",
              " ['quick', 'brown', 'fox', 'jump', 'over', 'lazy', 'dog'],\n",
              " ['be', 'not', 'be', 'that', 'question', 'whether', 'ti', 'nobler', 'mind'],\n",
              " ['data',\n",
              "  'science',\n",
              "  'involve',\n",
              "  'statistic',\n",
              "  'linear',\n",
              "  'algebra',\n",
              "  'machine',\n",
              "  'learning'],\n",
              " ['i', 'love', 'machine', 'learning', 'i', 'hate', 'math', 'behind']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "#write rest of the code here\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default\n",
        "\n",
        "\n",
        "tokenised_text=[word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "print(tokenised_text)\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "filtered_text = [\n",
        "    [word for word in sentence if word not in stop_words and word not in string.punctuation]\n",
        "    for sentence in tokenised_text\n",
        "]\n",
        "\n",
        "print(filtered_text)\n",
        "pos_tagged_sentences= [nltk.pos_tag(sentence) for sentence in filtered_text]\n",
        "print(pos_tagged_sentences)\n",
        "lem=WordNetLemmatizer()\n",
        "lemmatized_text = [\n",
        "    [lem.lemmatize(word, pos=get_wordnet_pos(tag)) for (word, tag) in sentence]\n",
        "    for sentence in pos_tagged_sentences\n",
        "]\n",
        "print(lemmatized_text)\n"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f46e84c-5fc7-4154-96e4-66add8d830cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', ';', 'however', ',', 'ethical', 'concerns', 'remain', '!'], ['the', 'pizza', 'was', 'absolutely', 'delicious', ',', 'but', 'the', 'service', 'was', 'terrible', '...', 'i', 'wo', \"n't\", 'go', 'back', '.'], ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'], ['to', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question', ':', 'whether', \"'t\", 'is', 'nobler', 'in', 'the', 'mind', '.'], ['data', 'science', 'involves', 'statistics', ',', 'linear', 'algebra', ',', 'and', 'machine', 'learning', '.'], ['i', 'love', 'machine', 'learning', ',', 'but', 'i', 'hate', 'the', 'math', 'behind', 'it', '.']]\n",
            "[['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concerns', 'remain'], ['pizza', 'absolutely', 'delicious', 'service', 'terrible', '...', 'wo', \"n't\", 'go', 'back'], ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog'], ['question', 'whether', \"'t\", 'nobler', 'mind'], ['data', 'science', 'involves', 'statistics', 'linear', 'algebra', 'machine', 'learning'], ['love', 'machine', 'learning', 'hate', 'math', 'behind']]\n",
            "[[('artificial', 'JJ'), ('intelligence', 'NN'), ('transforming', 'VBG'), ('world', 'NN'), ('however', 'RB'), ('ethical', 'JJ'), ('concerns', 'NNS'), ('remain', 'VBP')], [('pizza', 'NN'), ('absolutely', 'RB'), ('delicious', 'JJ'), ('service', 'NN'), ('terrible', 'JJ'), ('...', ':'), ('wo', 'MD'), (\"n't\", 'RB'), ('go', 'VB'), ('back', 'RB')], [('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('lazy', 'JJ'), ('dog', 'NN')], [('question', 'NN'), ('whether', 'IN'), (\"'t\", 'JJ'), ('nobler', 'NN'), ('mind', 'NN')], [('data', 'NNS'), ('science', 'NN'), ('involves', 'VBZ'), ('statistics', 'NNS'), ('linear', 'JJ'), ('algebra', 'JJ'), ('machine', 'NN'), ('learning', 'NN')], [('love', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('hate', 'JJ'), ('math', 'NN'), ('behind', 'IN')]]\n",
            "[['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain'], ['pizza', 'absolutely', 'delicious', 'service', 'terrible', '...', 'wo', \"n't\", 'go', 'back'], ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog'], ['question', 'whether', \"'t\", 'nobler', 'mind'], ['data', 'science', 'involve', 'statistic', 'linear', 'algebra', 'machine', 'learning'], ['love', 'machine', 'learn', 'hate', 'math', 'behind']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# rest of the code here\n",
        "all_words=[words  for sentence in lemmatized_text for words in sentence ]\n",
        "sorted_all_words=sorted(all_words)\n",
        "unique_words= set(words for sentence in lemmatized_text for words in sentence )\n",
        "sorted_words = sorted(unique_words)\n",
        "count_array=[]\n",
        "print(sorted_all_words)\n",
        "def vectorise(text):\n",
        "  i=0\n",
        "  while i < len(sorted_all_words):\n",
        "          count = 1\n",
        "          while i + 1 < len(sorted_all_words) and sorted_all_words[i] == sorted_all_words[i+1]:\n",
        "              count += 1\n",
        "              i += 1\n",
        "          count_array.append((sorted_all_words[i], count))\n",
        "          i += 1\n",
        "\n",
        "  return count_array\n",
        "print(vectorise(sorted_all_words))"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddc1698-c556-42c0-9960-5ee4be7d73f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'t\", '...', 'absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involve', 'jump', 'lazy', 'learn', 'learning', 'linear', 'love', 'machine', 'machine', 'math', 'mind', \"n't\", 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'transform', 'whether', 'wo', 'world']\n",
            "[(\"'t\", 1), ('...', 1), ('absolutely', 1), ('algebra', 1), ('artificial', 1), ('back', 1), ('behind', 1), ('brown', 1), ('concern', 1), ('data', 1), ('delicious', 1), ('dog', 1), ('ethical', 1), ('fox', 1), ('go', 1), ('hate', 1), ('however', 1), ('intelligence', 1), ('involve', 1), ('jump', 1), ('lazy', 1), ('learn', 1), ('learning', 1), ('linear', 1), ('love', 1), ('machine', 2), ('math', 1), ('mind', 1), (\"n't\", 1), ('nobler', 1), ('pizza', 1), ('question', 1), ('quick', 1), ('remain', 1), ('science', 1), ('service', 1), ('statistic', 1), ('terrible', 1), ('transform', 1), ('whether', 1), ('wo', 1), ('world', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ccAkbBhV_ssm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#rest of the code here\n",
        "v= CountVectorizer()\n",
        "print(v.fit_transform(corpus).toarray())\n"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4858c454-faa0-4ec5-b171-e6c6f510b254"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "tf_sentence=\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "total_words=tf_sentence.split()\n",
        "no_total_words=len(total_words)\n",
        "machine_count=total_words.count('machine')\n",
        "#print(machine_count)\n",
        "tf=machine_count/no_total_words\n",
        "i=0\n",
        "count=1\n",
        "for i in range(5):\n",
        "  if 'machine' in corpus[i]:\n",
        "    count+=1\n",
        "\n",
        "idf=math.log(len(corpus)/count)\n",
        "print(tf*idf)\n"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c155ca9d-7dde-487b-95b4-499991c9613c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09987384442437362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# rest of the code here\n",
        "vect= TfidfVectorizer()\n",
        "transformed_output=vect.fit_transform(corpus)\n",
        "print(vect.vocabulary_)\n",
        "print(transformed_output[3])\n"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e145ef-f8c1-40c3-a97b-cfa05a89e4c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'artificial': 3, 'intelligence': 19, 'is': 21, 'transforming': 47, 'the': 44, 'world': 51, 'however': 17, 'ethical': 13, 'concerns': 9, 'remain': 38, 'pizza': 35, 'was': 48, 'absolutely': 0, 'delicious': 11, 'but': 8, 'service': 40, 'terrible': 42, 'won': 50, 'go': 15, 'back': 4, 'quick': 37, 'brown': 7, 'fox': 14, 'jumps': 23, 'over': 34, 'lazy': 24, 'dog': 12, 'to': 46, 'be': 5, 'or': 33, 'not': 32, 'that': 43, 'question': 36, 'whether': 49, 'tis': 45, 'nobler': 31, 'in': 18, 'mind': 30, 'data': 10, 'science': 39, 'involves': 20, 'statistics': 41, 'linear': 26, 'algebra': 1, 'and': 2, 'machine': 28, 'learning': 25, 'love': 27, 'hate': 16, 'math': 29, 'behind': 6, 'it': 22}\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 13 stored elements and shape (1, 52)>\n",
            "  Coords\tValues\n",
            "  (0, 21)\t0.18951403836835215\n",
            "  (0, 44)\t0.23680832518670944\n",
            "  (0, 46)\t0.4622212982553308\n",
            "  (0, 5)\t0.4622212982553308\n",
            "  (0, 33)\t0.2311106491276654\n",
            "  (0, 32)\t0.2311106491276654\n",
            "  (0, 43)\t0.2311106491276654\n",
            "  (0, 36)\t0.2311106491276654\n",
            "  (0, 49)\t0.2311106491276654\n",
            "  (0, 45)\t0.2311106491276654\n",
            "  (0, 31)\t0.2311106491276654\n",
            "  (0, 18)\t0.2311106491276654\n",
            "  (0, 30)\t0.2311106491276654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "model=Word2Vec(\n",
        "    window=10,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "model.build_vocab(lemmatized_text, progress_per=10)\n",
        "print(model.epochs)\n",
        "model.corpus_count\n",
        "model.train(lemmatized_text,total_examples=model.corpus_count,epochs=model.epochs)\n",
        "model.wv[\"learning\"]\n"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c218483-fb50-405c-e796-369bf8d9edec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8.13227147e-03, -4.45733406e-03, -1.06835726e-03,  1.00636482e-03,\n",
              "       -1.91113955e-04,  1.14817743e-03,  6.11386076e-03, -2.02715401e-05,\n",
              "       -3.24596534e-03, -1.51072862e-03,  5.89729892e-03,  1.51410222e-03,\n",
              "       -7.24261976e-04,  9.33324732e-03, -4.92128357e-03, -8.38409644e-04,\n",
              "        9.17541143e-03,  6.74942741e-03,  1.50285603e-03, -8.88256077e-03,\n",
              "        1.14874600e-03, -2.28825561e-03,  9.36823711e-03,  1.20992784e-03,\n",
              "        1.49006362e-03,  2.40640994e-03, -1.83600665e-03, -4.99963388e-03,\n",
              "        2.32429506e-04, -2.01418041e-03,  6.60093315e-03,  8.94012302e-03,\n",
              "       -6.74754381e-04,  2.97701475e-03, -6.10765442e-03,  1.69932481e-03,\n",
              "       -6.92623248e-03, -8.69402662e-03, -5.90020278e-03, -8.95647518e-03,\n",
              "        7.27759488e-03, -5.77203138e-03,  8.27635173e-03, -7.24354526e-03,\n",
              "        3.42167495e-03,  9.67499893e-03, -7.78544787e-03, -9.94505733e-03,\n",
              "       -4.32914635e-03, -2.68313056e-03, -2.71289347e-04, -8.83155130e-03,\n",
              "       -8.61755759e-03,  2.80021061e-03, -8.20640661e-03, -9.06933658e-03,\n",
              "       -2.34046578e-03, -8.63180775e-03, -7.05664977e-03, -8.40115082e-03,\n",
              "       -3.01328895e-04, -4.56429832e-03,  6.62717456e-03,  1.52716041e-03,\n",
              "       -3.34147573e-03,  6.10897178e-03, -6.01328490e-03, -4.65616956e-03,\n",
              "       -7.20750913e-03, -4.33658017e-03, -1.80932996e-03,  6.48964290e-03,\n",
              "       -2.77039292e-03,  4.91896737e-03,  6.90444233e-03, -7.46370573e-03,\n",
              "        4.56485013e-03,  6.12697843e-03, -2.95447465e-03,  6.62502181e-03,\n",
              "        6.12587947e-03, -6.44348515e-03, -6.76455162e-03,  2.53895880e-03,\n",
              "       -1.62381888e-03, -6.06512791e-03,  9.49920900e-03, -5.13014663e-03,\n",
              "       -6.55409694e-03, -1.19885204e-04, -2.70142802e-03,  4.44400299e-04,\n",
              "       -3.53745813e-03, -4.19330609e-04, -7.08615757e-04,  8.22820642e-04,\n",
              "        8.19481723e-03, -5.73670724e-03, -1.65952800e-03,  5.57160750e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "print(glove_model.most_similar(positive=['woman', 'king'], negative=['man'])[:5])\n",
        "#rest of the code here"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb8b0b9-42d5-4940-c07c-93b64809a6e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "# rest of the code here\n",
        "analyser=SentimentIntensityAnalyzer()\n",
        "score_pizza=analyser.polarity_scores(corpus[1])\n",
        "print(score_pizza)\n",
        "math_complaint_score=analyser.polarity_scores(corpus[5])\n",
        "\n"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e9474d-2f2a-48f8-829b-f9f8d469db63"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "# rest of the code here\n",
        "analyser=SentimentIntensityAnalyzer()\n",
        "score_pizza=analyser.polarity_scores(corpus[1])\n",
        "print(score_pizza)\n",
        "math_complaint_score=analyser.polarity_scores(corpus[5])\n",
        "print(math_complaint_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433bee03-785f-4057-cc8d-8df05a3089c7",
        "id": "9vAiKDnp8JM5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "{'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}