{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYACLCwsyxfa"
      },
      "source": [
        "# **Part 1: Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTP8EqylwqDf"
      },
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR4BKqITy17z",
        "outputId": "e85a0d8a-dbb3-4ea6-88a7-c4fcb9028287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "#write rest of the code here\n",
        "\n",
        "def clean_text_scratch(text):\n",
        "  text = text.lower()\n",
        "  for i in \"!,.,,,:,;,...,'\":\n",
        "    text = text.replace(i,\"\")\n",
        "  tockens = text.split()\n",
        "  for i in ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']:\n",
        "    for j in range(tockens.count(i)):\n",
        "      tockens.remove(i)\n",
        "  for i in range(len(tockens)):\n",
        "    if tockens[i][-1]==\"s\":\n",
        "      tockens[i] = tockens[i][:-1]\n",
        "    if tockens[i][-3:]==\"ing\":\n",
        "      tockens[i] = tockens[i][:-3]\n",
        "    if tockens[i][-2:]==\"ly\" or tockens[i][-2:]==\"ed\":\n",
        "      tockens[i] = tockens[i][:-2]\n",
        "  print(tockens)\n",
        "clean_text_scratch(corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN9rNq7WycqZ"
      },
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_4FjuCqy5Kt",
        "outputId": "abbc48a3-9e2a-4661-b80c-90d70051f2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['artificial', 'intelligence', 'transforming', 'world', 'however', 'ethical', 'concern', 'remain', 'pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'question', 'whether', 'nobler', 'mind', 'data', 'science', 'involves', 'statistic', 'linear', 'algebra', 'machine', 'learning', 'love', 'machine', 'learning', 'hate', 'math', 'behind']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#write rest of the code here\n",
        "def clean_text(txt):\n",
        "  lis = nltk.tokenize.word_tokenize(txt.lower())\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "  wordsFiltered = []\n",
        "  lis = [i for i in lis if i.isalpha()]\n",
        "  for w in lis:\n",
        "     if w not in stopWords:\n",
        "          wordsFiltered.append(w.lower())\n",
        "  lis = wordsFiltered.copy()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for i in range(len(lis)):\n",
        "    lis[i] = lemmatizer.lemmatize(lis[i])\n",
        "  return lis\n",
        "\n",
        "lis = clean_text(' '.join(corpus))\n",
        "print(lis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPMrwva2y1LG"
      },
      "source": [
        "# **Part 2: Text Representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      },
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVUFCkm7yrg-",
        "outputId": "be32856d-54be-4f9f-b285-73fc8ae72d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jump', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'transforming', 'whether', 'wo', 'world']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# rest of the code here\n",
        "def countwords(sentence):\n",
        "  lik = clean_text(sentence)\n",
        "  fik = list(set(lik))\n",
        "  fik.sort()\n",
        "  a = []\n",
        "  for i in fik:\n",
        "    a.append(lik.count(i))\n",
        "  return a\n",
        "\n",
        "def get_words(i):\n",
        "  return clean_text(i)\n",
        "\n",
        "full_text = ' '.join(corpus)\n",
        "words = []\n",
        "for i in corpus:\n",
        "  words+=get_words(i)\n",
        "words = list(set(words))\n",
        "words.sort()\n",
        "print(words)\n",
        "print(countwords(full_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwsoZix-zUDC"
      },
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGs7EzLRzfGC",
        "outputId": "752fac41-171b-49b4-add3-7b56a63cbb5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#rest of the code here\n",
        "\n",
        "v = CountVectorizer()\n",
        "abc = v.fit_transform(corpus)\n",
        "print(abc.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      },
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNSo-nza0k_c",
        "outputId": "b682658e-5083-472c-d753-8b211e8d535b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09090909090909091\n",
            "0.0\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "# write code here\n",
        "sentence = corpus[-1]\n",
        "words = sentence.split()\n",
        "tf = 1/len(words)\n",
        "idf = math.log(1/1)\n",
        "print(tf)\n",
        "print(idf)\n",
        "print(tf*idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEYkuoSb0nDe"
      },
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of6PfWyd0pnl",
        "outputId": "d97a97b5-382d-4aee-8c23-946b6547fc27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "behind : 0.0\n",
            "but : 0.0\n",
            "hate : 0.0\n",
            "it : 0.0\n",
            "learning : 0.0\n",
            "love : 0.0\n",
            "machine : 0.0\n",
            "math : 0.0\n",
            "the : 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "first_vector = X[0].toarray()\n",
        "w = vectorizer.get_feature_names_out()\n",
        "\n",
        "print('\\n'.join([\"{} : {}\".format(w[i],first_vector[0][i]) for i in range(len(first_vector[0]))]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWAar8IIzp_m"
      },
      "source": [
        "# **Part 3- Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY1URFxgz036"
      },
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aziX2IGBzyaa",
        "outputId": "d031cdee-5518-419a-8cb0-1b8b53520870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "[-0.08167583  0.04493266 -0.04121889  0.00810245  0.08528175 -0.0447215\n",
            "  0.04544761 -0.06766256 -0.03566954  0.09391536]\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#rest of the code here\n",
        "sens = [word_tokenize(i.lower()) for i in corpus]\n",
        "model = Word2Vec(sentences=sens, vector_size=10, min_count=1)\n",
        "print(model.wv[\"learning\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3J42eQZ1fUo"
      },
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEj5SkO81mkF",
        "outputId": "90315d2b-7358-4c7d-8561-a80552a33b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337412238121033), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "print(glove_model.most_similar(positive=['woman', 'king'], negative=['man'])[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbI4K0UJUxy3"
      },
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lC2c3GHUxU-",
        "outputId": "6e208e66-3498-4bb3-9a4a-cdfab83107b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "{'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "nltk.download('vader_lexicon')\n",
        "# rest of the code here\n",
        "ar = SentimentIntensityAnalyzer()\n",
        "print(ar.polarity_scores(corpus[1]))\n",
        "print(ar.polarity_scores(corpus[5]))\n",
        "#Yes, it does"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
