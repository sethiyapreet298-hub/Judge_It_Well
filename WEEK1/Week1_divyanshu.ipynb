{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYACLCwsyxfa"
   },
   "source": [
    "# **Part 1: Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTP8EqylwqDf"
   },
   "source": [
    "\n",
    "\n",
    "**Data-** given in the below code cell\n",
    "\n",
    "**1.1: Preprocessing From Scratch**\n",
    "\n",
    "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
    "\n",
    "1. Lowercasing: Convert text to lowercase.\n",
    "\n",
    "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
    "\n",
    "3. Tokenization: Split the string into a list of words based on whitespace.\n",
    "\n",
    "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
    "\n",
    "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
    "\n",
    "\n",
    "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
    "\n",
    "**Task:** Run this function on the first sentence of the corpus and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qIRv3qS2bTFt"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
    "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
    "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
    "    \"I love machine learning, but I hate the math behind it.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oR4BKqITy17z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'i', 'transform', 'the', 'world', 'however', 'ethical', 'concern', 'remain']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#write rest of the code here\n",
    "\n",
    "def clean_text_scratch(text):\n",
    "    text=text.lower()          #lowercasing\n",
    "    text = re.sub(r\"[!.,:;…']\", \"\", text)       #punctuation removal\n",
    "    l=text.split()                      #tekenization\n",
    "\n",
    "    a=['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n",
    "    for i in text:\n",
    "        if i in l:\n",
    "            l.remove(i)                      #stopword removal\n",
    "\n",
    "    return l\n",
    "def helper(text):\n",
    "    l=clean_text_scratch(text)\n",
    "    r=[]\n",
    "    ending = (\"ing\", \"ly\", \"ed\", \"s\")\n",
    "    for i in l:\n",
    "        for a in ending:\n",
    "            if i.endswith(a):\n",
    "                i = i[:-len(a)]\n",
    "                break\n",
    "        r.append(i)\n",
    "    print(r) \n",
    "\n",
    "helper(\"Artificial Intelligence is transforming the world; however, ethical concerns remain!\")\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN9rNq7WycqZ"
   },
   "source": [
    "**1.2: Preprocessing Using Tools**\n",
    "\n",
    "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use nltk.tokenize.word_tokenize.\n",
    "2. Use nltk.corpus.stopwords.\n",
    "3. Use nltk.stem.WordNetLemmatizer\n",
    "\n",
    "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
    "\n",
    "\n",
    "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_4FjuCqy5Kt",
    "outputId": "f55f4c3f-8831-4498-c567-c28b38ebdd40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and lemmatized sentence: ['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wont', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "lemmat = WordNetLemmatizer()\n",
    "a=corpus[1].lower()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "a = a.translate(translator)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "tokens = word_tokenize(a)\n",
    "\n",
    "cleaned_lemmatized = [lemmat.lemmatize(word) for word in tokens if  word not in stop]\n",
    "\n",
    "print(f\"Cleaned and lemmatized sentence: {cleaned_lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPMrwva2y1LG"
   },
   "source": [
    "# **Part 2: Text Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKa8NnZ5zLlm"
   },
   "source": [
    "**2.1: Bag of Words (BoW)**\n",
    "\n",
    "**Logic:**\n",
    "\n",
    "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
    "\n",
    "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
    "\n",
    "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yVUFCkm7yrg-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Vocabulary (39 words):\n",
      "['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jump', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'ti', 'transforming', 'whether', 'wont', 'world']\n",
      "\n",
      "Test sentence: 'The quick brown fox jumps over the lazy dog.'\n",
      "BoW vector: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NLTK downloads (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def cleanl(t):\n",
    "\n",
    "    import string\n",
    "    \n",
    "    \n",
    "    lemmat = WordNetLemmatizer()\n",
    "    a=t.lower()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    a = a.translate(translator)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    \n",
    "    tokens = word_tokenize(a)\n",
    "    cleaned_lemmatized=[]\n",
    "    for word in tokens:\n",
    "        if word not in stop:\n",
    "            cleaned_lemmatized.append(lemmat.lemmatize(word))\n",
    "    return cleaned_lemmatized\n",
    "vocab=set()\n",
    "for i in corpus:\n",
    "    a=list(cleanl(i))\n",
    "    for j in a:\n",
    "        vocab.add(j)\n",
    "\n",
    "# Sort vocabulary alphabetically\n",
    "vocabulary = sorted(list(vocab))\n",
    "print(f\"Unique Vocabulary ({len(vocabulary)} words):\")\n",
    "print(vocabulary)\n",
    "\n",
    "# Function to create BoW vector for a sentence\n",
    "def bow_vector(sentence, vocab):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    vector = [0] * len(vocab)\n",
    "    for word in tokens:\n",
    "        if word.isalpha() and word not in stop:\n",
    "            lemmatized = lemmat.lemmatize(word)\n",
    "            if lemmatized in vocab:\n",
    "                idx = vocab.index(lemmatized)\n",
    "                vector[idx] += 1\n",
    "    return vector\n",
    "\n",
    "# Test with the third sentence\n",
    "test_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "vector = bow_vector(test_sentence, vocabulary)\n",
    "print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "print(f\"BoW vector: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MR6Bxgh0Gpu"
   },
   "source": [
    "**2.3: TF-IDF From Scratch (The Math)**\n",
    "\n",
    "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
    "\n",
    "\"I love machine learning, but I hate the math behind it.\"\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
    "\n",
    "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
    "\n",
    "**Result:** TF * IDF.\n",
    "\n",
    "**Task:** Print your manual calculation result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwsoZix-zUDC"
   },
   "source": [
    "**2.2: BoW Using Tools**\n",
    "\n",
    "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Instantiate the vectorizer.\n",
    "\n",
    "2. fit_transform the raw corpus.\n",
    "\n",
    "3. Convert the result to an array (.toarray()) and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix (from CountVectorizer):\n",
      "[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n",
      " [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
      "  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "Shape: (6, 52) (documents x vocabulary size)\n",
      "\n",
      "Feature names (vocabulary): ['absolutely' 'algebra' 'and' 'artificial' 'back' 'be' 'behind' 'brown'\n",
      " 'but' 'concerns' 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate'\n",
      " 'however' 'in' 'intelligence' 'involves' 'is' 'it' 'jumps' 'lazy'\n",
      " 'learning' 'linear' 'love' 'machine' 'math' 'mind' 'nobler' 'not' 'or'\n",
      " 'over' 'pizza' 'question' 'quick' 'remain' 'science' 'service'\n",
      " 'statistics' 'terrible' 'that' 'the' 'tis' 'to' 'transforming' 'was'\n",
      " 'whether' 'won' 'world'][:20]...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "bow_array = bow_matrix.toarray()\n",
    "print(\"Bag of Words Matrix (from CountVectorizer):\")\n",
    "print(bow_array)\n",
    "print(f\"\\nShape: {bow_array.shape} (documents x vocabulary size)\")\n",
    "print(f\"\\nFeature names (vocabulary): {vectorizer.get_feature_names_out()}[:20]...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gNSo-nza0k_c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Score:\n",
      "  TF-IDF = TF × IDF = 0.0909 × 1.0986 = 0.099874\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "word = \"machine\"\n",
    "last_sentence = corpus[5]  # \"I love machine learning, but I hate the math behind it.\"\n",
    "\n",
    "\n",
    "# Calculate TF (Term Frequency)\n",
    "words_in_sentence = last_sentence.lower().split()\n",
    "total_words = len(words_in_sentence)\n",
    "count_word = sum(1 for w in words_in_sentence if word in w.lower())\n",
    "tf = count_word / total_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_documents = len(corpus)\n",
    "docs_with_word = sum(1 for doc in corpus if word.lower() in doc.lower())                 # Calculate IDF (Inverse Document Frequency)\n",
    "idf = math.log(total_documents / docs_with_word)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = tf * idf\n",
    "print(\"TF-IDF Score:\")\n",
    "print(f\"  TF-IDF = TF × IDF = {tf:.4f} × {idf:.4f} = {tf_idf:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEYkuoSb0nDe"
   },
   "source": [
    "**2.4: TF-IDF Using Tools**\n",
    "\n",
    "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
    "\n",
    "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
    "\n",
    "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of6PfWyd0pnl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector for the first sentence:\n",
      "Sentence: Artificial Intelligence is transforming the world; however, ethical concerns remain!\n",
      "\n",
      "Non-zero TF-IDF scores:\n",
      "  artificial: 0.3345\n",
      "  concerns: 0.3345\n",
      "  ethical: 0.3345\n",
      "  however: 0.3345\n",
      "  intelligence: 0.3345\n",
      "  is: 0.2743\n",
      "  remain: 0.3345\n",
      "  the: 0.1714\n",
      "  transforming: 0.3345\n",
      "  world: 0.3345\n",
      "\n",
      "Observation:\n",
      "Unique words (like 'intelligence', 'artificial', 'transforming') have higher scores\n",
      "Common words (like 'is', 'the') would have lower or zero scores due to high document frequency.\n",
      "\n",
      "Shape of TF-IDF matrix: (6, 52)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "\n",
    "first_tfidf_vector = tfidf_array[0]\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF Vector for the first sentence:\")\n",
    "print(f\"Sentence: {corpus[0]}\\n\")\n",
    "\n",
    "print(\"Non-zero TF-IDF scores:\")\n",
    "for idx, score in enumerate(first_tfidf_vector):\n",
    "    if score > 0:\n",
    "        print(f\"  {feature_names[idx]}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"Unique words (like 'intelligence', 'artificial', 'transforming') have higher scores\")\n",
    "print(\"Common words (like 'is', 'the') would have lower or zero scores due to high document frequency.\")\n",
    "print(f\"\\nShape of TF-IDF matrix: {tfidf_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWAar8IIzp_m"
   },
   "source": [
    "# **Part 3- Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY1URFxgz036"
   },
   "source": [
    "**3.1: Word2Vec Using Tools**\n",
    "\n",
    "**Task:** Train a model using gensim.models.Word2Vec.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
    "\n",
    "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
    "\n",
    "3. Set vector_size=10 (small vector size for easy viewing).\n",
    "\n",
    "**Experiment:** Print the vector for the word \"learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aziX2IGBzyaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08167583  0.04493266 -0.04121889  0.00810245  0.08528175 -0.0447215\n",
      "  0.04544761 -0.06766256 -0.03566954  0.09391536]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "a = [word_tokenize(i.lower()) for i in corpus]\n",
    "\n",
    "vect = Word2Vec(a, vector_size=10, min_count=1)\n",
    "\n",
    "print(vect.wv[\"learning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3J42eQZ1fUo"
   },
   "source": [
    "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
    "\n",
    "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
    "\n",
    "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
    "\n",
    "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
    "\n",
    "**Question:** Does the model correctly guess \"Queen\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LEj5SkO81mkF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman  equals queen\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe model\n",
    "glove = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "predict = glove.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print('King - Man + Woman  equals',predict[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbI4K0UJUxy3"
   },
   "source": [
    "# **Part 5- Sentiment Analysis (The Application)**\n",
    "\n",
    "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Initialize the SentimentIntensityAnalyzer.\n",
    "\n",
    "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
    "\n",
    "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
    "\n",
    "**Analysis:** Look at the compound score for both.\n",
    "\n",
    "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
    "\n",
    "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_lC2c3GHUxU-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
      "{'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# Download VADER (run once)\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "print(sia.polarity_scores(corpus[1]))\n",
    "print(sia.polarity_scores(corpus[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
