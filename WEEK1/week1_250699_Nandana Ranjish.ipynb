{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Text Preprocessing**"
      ],
      "metadata": {
        "id": "IYACLCwsyxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Data-** given in the below code cell\n",
        "\n",
        "**1.1: Preprocessing From Scratch**\n",
        "\n",
        "**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n",
        "\n",
        "1. Lowercasing: Convert text to lowercase.\n",
        "\n",
        "2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n",
        "\n",
        "3. Tokenization: Split the string into a list of words based on whitespace.\n",
        "\n",
        "4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n",
        "\n",
        "5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n",
        "\n",
        "\n",
        "Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n",
        "\n",
        "**Task:** Run this function on the first sentence of the corpus and print the result."
      ],
      "metadata": {
        "id": "MTP8EqylwqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRv3qS2bTFt"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text_scratch(text):\n",
        "  text=text.lower()\n",
        "  text=re.sub(r\"[!.,:;']\",\"\",text)\n",
        "  words=text.split()\n",
        "  i=0\n",
        "  while i<len(words):\n",
        "    if words[i] in  ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']:\n",
        "      del words[i]\n",
        "    else:\n",
        "      i+=1\n",
        "  for i in range(len(words)):\n",
        "    if words[i].endswith('ing'):\n",
        "      words[i]=words[i][:-3]\n",
        "    if words[i].endswith(('ly','ed')):\n",
        "      words[i]=words[i][:-2]\n",
        "    if words[i].endswith('s'):\n",
        "      words[i]=words[i][:-1]\n",
        "  return ' '.join(words)\n",
        "print(clean_text_scratch(corpus[0]))\n",
        "\n",
        "\n",
        "#write rest of the code here"
      ],
      "metadata": {
        "id": "oR4BKqITy17z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d28654b-4782-4000-9c72-5a2c37a55497"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial intelligence transform world however ethical concern remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Preprocessing Using Tools**\n",
        "\n",
        "**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use nltk.tokenize.word_tokenize.\n",
        "2. Use nltk.corpus.stopwords.\n",
        "3. Use nltk.stem.WordNetLemmatizer\n",
        "\n",
        "to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n",
        "\n",
        "\n",
        "**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."
      ],
      "metadata": {
        "id": "dN9rNq7WycqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def clean_text(text):\n",
        "  lemm=WordNetLemmatizer()\n",
        "  text=text.split()\n",
        "  text=[lemm.lemmatize(word) for word in text if not word in set(['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'])]\n",
        "  text=' '.join(text)\n",
        "  text=re.sub(r\"[!.,:;']\",\"\",text)\n",
        "  text=text.lower().strip()\n",
        "  return text\n",
        "clean_corpus=[0]*len(corpus)\n",
        "for i in range(len(corpus)):\n",
        "  clean_corpus[i]=clean_text(corpus[i])\n",
        "print(clean_corpus[1])\n",
        "\n",
        "#write rest of the code here"
      ],
      "metadata": {
        "id": "v_4FjuCqy5Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3710f34-e117-4a88-940d-35f289a4704f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pizza absolutely delicious service terrible wont go back\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Text Representation**"
      ],
      "metadata": {
        "id": "hPMrwva2y1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Bag of Words (BoW)**\n",
        "\n",
        "**Logic:**\n",
        "\n",
        "**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n",
        "\n",
        "**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n",
        "\n",
        "**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "cKa8NnZ5zLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NLTK downloads (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "  lemm=WordNetLemmatizer()\n",
        "  text=text.split()\n",
        "  text=[lemm.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
        "  text=' '.join(text)\n",
        "  text=re.sub(r\"[!.,:;']\",\"\",text)\n",
        "  text=text.lower().strip()\n",
        "  return text\n",
        "clean_corpus=[0]*len(corpus)\n",
        "for i in range(len(corpus)):\n",
        "  clean_corpus[i]=clean_text(corpus[i])\n",
        "print(\"corpus after cleaning:\",clean_corpus)\n",
        "un_words=[]\n",
        "for i in clean_corpus:\n",
        "  sent=i.split()\n",
        "  for j in sent:\n",
        "    if j not in un_words:\n",
        "      un_words.append(j)\n",
        "un_words.sort()\n",
        "print(\"unique words sorted alphabetically:\",un_words)\n",
        "\n",
        "def vectorize(sent):\n",
        "  words_sent=sent.split()\n",
        "  count=[0]*len(un_words)\n",
        "  for i in range(len(un_words)):\n",
        "    for j in words_sent:\n",
        "      if un_words[i]==j:\n",
        "        count[i]+=1\n",
        "  return count\n",
        "print(vectorize(clean_corpus[2]))\n",
        "\n",
        "# rest of the code here"
      ],
      "metadata": {
        "id": "yVUFCkm7yrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11f19a1-66dd-40c9-f5d7-d3b55e4be53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus after cleaning: ['artificial intelligence transforming world however ethical concern remain', 'pizza absolutely delicious service terrible wont go back', 'quick brown fox jump lazy dog', 'question whether ti nobler mind', 'data science involves statistic linear algebra machine learning', 'love machine learning hate math behind']\n",
            "unique words sorted alphabetically: ['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jump', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'ti', 'transforming', 'whether', 'wont', 'world']\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['artificial intelligence transforming world however ethical concern remain', 'pizza absolutely delicious service terrible wont go back', 'quick brown fox jump lazy dog', 'question whether ti nobler mind', 'data science involves statistic linear algebra machine learning', 'love machine learning hate math behind']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: BoW Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Instantiate the vectorizer.\n",
        "\n",
        "2. fit_transform the raw corpus.\n",
        "\n",
        "3. Convert the result to an array (.toarray()) and print it."
      ],
      "metadata": {
        "id": "UwsoZix-zUDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorize=CountVectorizer()\n",
        "x=vectorize.fit_transform(corpus)\n",
        "feature_names=vectorize.get_feature_names_out()\n",
        "x_array=x.toarray()\n",
        "print(\"unique word list:\",feature_names)\n",
        "print(\"Bag of words:\",x_array)\n",
        "#rest of the code here"
      ],
      "metadata": {
        "id": "RGs7EzLRzfGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef30724-64b5-457a-ae9b-165f39e3aae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique word list: ['absolutely' 'algebra' 'artificial' 'back' 'behind' 'brown' 'concern'\n",
            " 'data' 'delicious' 'dog' 'ethical' 'fox' 'go' 'hate' 'however'\n",
            " 'intelligence' 'involves' 'jump' 'lazy' 'learning' 'linear' 'love'\n",
            " 'machine' 'math' 'mind' 'nobler' 'pizza' 'question' 'quick' 'remain'\n",
            " 'science' 'service' 'statistic' 'terrible' 'ti' 'transforming' 'whether'\n",
            " 'wont' 'world']\n",
            "Bag of words: [[0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
            "  0 0 1]\n",
            " [1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            "  0 1 0]\n",
            " [0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0\n",
            "  1 0 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            "  0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: TF-IDF From Scratch (The Math)**\n",
        "\n",
        "**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n",
        "\n",
        "\"I love machine learning, but I hate the math behind it.\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n",
        "\n",
        "*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n",
        "\n",
        "**Result:** TF * IDF.\n",
        "\n",
        "**Task:** Print your manual calculation result."
      ],
      "metadata": {
        "id": "-MR6Bxgh0Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write code here\n",
        "import math\n",
        "def tf(sent,word):\n",
        "  words_sent=sent.split()\n",
        "  count=0\n",
        "  if len(words_sent)==0:\n",
        "    return 0\n",
        "  for i in words_sent:\n",
        "    if i==word:\n",
        "      count+=1\n",
        "  return count/len(words_sent)\n",
        "def idf(word):\n",
        "  count_docx=0\n",
        "  for i in corpus:\n",
        "    words_sent=i.split()\n",
        "    for j in words_sent:\n",
        "      if j==word:\n",
        "        count_docx+=1\n",
        "        break\n",
        "  return math.log10(len(corpus)/count_docx)\n",
        "def tfidf_for_word(sent,word):\n",
        "  print(\"TF-IDF of\",word,\"=\",tf(sent,word)*idf(word))\n",
        "tfidf_for_word(corpus[5],'machine')\n"
      ],
      "metadata": {
        "id": "gNSo-nza0k_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45292181-3df1-47db-e15a-eaf444c9c066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF of machine = 0.043374659519969314\n",
            "['artificial intelligence transforming world however ethical concern remain', 'pizza absolutely delicious service terrible wont go back', 'quick brown fox jump lazy dog', 'question whether ti nobler mind', 'data science involves statistic linear algebra machine learning', 'love machine learning hate math behind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: TF-IDF Using Tools**\n",
        "\n",
        "**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n",
        "\n",
        "**Steps:** Fit it on the corpus and print the vector for the first sentence.\n",
        "\n",
        "**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"
      ],
      "metadata": {
        "id": "YEYkuoSb0nDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n",
        "    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n",
        "    \"Data science involves statistics, linear algebra, and machine learning.\",\n",
        "    \"I love machine learning, but I hate the math behind it.\"\n",
        "]"
      ],
      "metadata": {
        "id": "DmVtSmJtPjEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# rest of the code here\n",
        "tf_idf_=TfidfVectorizer()\n",
        "tf_idf_vector=tf_idf_.fit_transform(corpus)\n",
        "#print(type(tf_idf_vector),tf_idf_vector.shape)\n",
        "tf_idf_array=tf_idf_vector.toarray()\n",
        "print(tf_idf_array[0])\n",
        "feature_names = tf_idf_.get_feature_names_out()\n",
        "index1=list(feature_names).index('intelligence')\n",
        "index2=list(feature_names).index('is')\n",
        "if(tf_idf_array[0,index1]>tf_idf_array[0,index2]):\n",
        "  print(\"tf-idf score for rare words are larger than common words\\nintelligence:\",tf_idf_array[0,index1],' ',\"is:\",tf_idf_array[0,index2])\n",
        "else:\n",
        "  print(\"tf-idf score for rare words are smaller than common words\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Of6PfWyd0pnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff7115a-4e9e-4d54-914b-ad8f96c6bcd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.         0.         0.33454543 0.         0.\n",
            " 0.         0.         0.         0.33454543 0.         0.\n",
            " 0.         0.33454543 0.         0.         0.         0.33454543\n",
            " 0.         0.33454543 0.         0.27433204 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.33454543 0.         0.         0.\n",
            " 0.         0.         0.17139656 0.         0.         0.33454543\n",
            " 0.         0.         0.         0.33454543]\n",
            "tf-idf score for rare words are larger than common words\n",
            "intelligence: 0.3345454287016015   is: 0.27433203727401334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3- Word Embeddings**"
      ],
      "metadata": {
        "id": "YWAar8IIzp_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Word2Vec Using Tools**\n",
        "\n",
        "**Task:** Train a model using gensim.models.Word2Vec.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n",
        "\n",
        "2. Set min_count=1 (since our corpus is small, we want to keep all words).\n",
        "\n",
        "3. Set vector_size=10 (small vector size for easy viewing).\n",
        "\n",
        "**Experiment:** Print the vector for the word \"learning\"."
      ],
      "metadata": {
        "id": "uY1URFxgz036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "model=Word2Vec(sentences=clean_corpus,vector_size=10,min_count=1)\n",
        "print(model.wv.get_vector(\"learning\"))\n",
        "\n",
        "\n",
        "#rest of the code here"
      ],
      "metadata": {
        "id": "aziX2IGBzyaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1f95af-a17e-4330-fef5-9b19c9cd761c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "[-0.00536899  0.00237282  0.05103846  0.0900786  -0.09300981 -0.07119522\n",
            "  0.06463154  0.08977251 -0.0501886  -0.03764008]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Pre-trained GloVe (Understanding Global Context)**\n",
        "\n",
        "**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n",
        "\n",
        "**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n",
        "\n",
        "Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n",
        "\n",
        "**Question:** Does the model correctly guess \"Queen\"?"
      ],
      "metadata": {
        "id": "r3J42eQZ1fUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "print(glove_model.most_similar(positive=['king','woman'],negative=['man']))\n",
        "#rest of the code here"
      ],
      "metadata": {
        "id": "LEj5SkO81mkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b05f234-8479-4dd7-e5c3-e483e9aaedf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337412238121033), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5- Sentiment Analysis (The Application)**\n",
        "\n",
        "**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "1. Initialize the SentimentIntensityAnalyzer.\n",
        "\n",
        "2. Pass the Pizza Review (corpus[1]) into the analyzer.\n",
        "\n",
        "3. Pass the Math Complaint (corpus[5]) into the analyzer.\n",
        "\n",
        "**Analysis:** Look at the compound score for both.\n",
        "\n",
        "**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n",
        "\n",
        "Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"
      ],
      "metadata": {
        "id": "AbI4K0UJUxy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "# Download VADER (run once)\n",
        "#nltk.download('vader_lexicon')\n",
        "# rest of the code here\n",
        "sia=SentimentIntensityAnalyzer()\n",
        "scores1=sia.polarity_scores(corpus[1])\n",
        "scores5=sia.polarity_scores(corpus[5])\n",
        "print(\"polarity scores for pizza:\",scores1)\n",
        "print(\"polarity scores for math:\",scores5)\n",
        "print(\"delicious and terrible in the same sentence results in a compound score of\",scores1['compound'],\"indicating the pizza review is moderately negative\")\n"
      ],
      "metadata": {
        "id": "_lC2c3GHUxU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bca0dff-501a-44b1-daf1-873a94f4f45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polarity scores for pizza: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n",
            "polarity scores for math: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n",
            "delicious and terrible in the same sentence results in a compound score of -0.3926 indicating the pizza review is moderately negative\n"
          ]
        }
      ]
    }
  ]
}